{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# former version\n",
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Dict, List, Tuple\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score,\n",
    "    precision_recall_curve, precision_score, recall_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Configure Polars for memory usage\n",
    "pl.Config.set_streaming_chunk_size(1_000_000)\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "\n",
    "class LazyDatasetLoader:\n",
    "    \"\"\"Memory-efficient dataset loader with checkpoint-based FID filtering\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, checkpoint_dir: str, debug_mode: bool = True, sample_size: int = 700_000, fids_to_ensure: List[int] = None):\n",
    "        self.data_path = data_path\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self._cached_dataset = None\n",
    "        self._cached_name = None\n",
    "        self.debug_mode = debug_mode\n",
    "        self.sample_size = sample_size\n",
    "        self.base_fids = None\n",
    "        self.fids_to_ensure = fids_to_ensure\n",
    "\n",
    "\n",
    "    def set_base_fids(self, fids):\n",
    "        \"\"\"Set base FIDs to maintain consistent filtering\"\"\"\n",
    "        self.base_fids = fids\n",
    "        print(f\"Set base FIDs: {len(fids)} records\")\n",
    "        \n",
    "        \n",
    "    def get_checkpoint_fids(self):\n",
    "        \"\"\"Get base FIDs from profile checkpoint if it exists\"\"\"\n",
    "        profile_checkpoint = f\"{self.checkpoint_dir}/profile_features.parquet\"\n",
    "        if os.path.exists(profile_checkpoint):\n",
    "            df = pl.read_parquet(profile_checkpoint)\n",
    "            if 'fid' in df.columns:\n",
    "                self.base_fids = df['fid']\n",
    "                print(f\"Loaded base FIDs from checkpoint: {len(self.base_fids)} records\")\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def get_dataset(self, name: str, columns: List[str] = None, source=\"farcaster\") -> pl.DataFrame:\n",
    "        \"\"\"Get dataset with checkpoint-based FID filtering\"\"\"\n",
    "        if self._cached_dataset is not None:\n",
    "            self._cached_dataset = None\n",
    "            \n",
    "        if source == \"farcaster\":\n",
    "            path = f\"{self.data_path}/farcaster-{name}-0-1733162400.parquet\"\n",
    "        elif source == \"nindexer\":\n",
    "            path = f\"{self.data_path}/nindexer-{name}-0-1733508243.parquet\"\n",
    "        try:\n",
    "            scan_query = pl.scan_parquet(path)\n",
    "            if columns:\n",
    "                scan_query = scan_query.select(columns)\n",
    "                \n",
    "            if self.debug_mode:\n",
    "                if self.base_fids is None:\n",
    "                    # Try to get FIDs from checkpoint first\n",
    "                    if not self.get_checkpoint_fids():\n",
    "                        if name == 'profile_with_addresses':\n",
    "                            self._cached_dataset = scan_query.limit(self.sample_size).collect()\n",
    "                            dataset_with_fids = scan_query.filter(pl.col('fid').is_in(self.fids_to_ensure)).collect()\n",
    "                            if len(dataset_with_fids) > 0:\n",
    "                                self._cached_dataset = pl.concat([self._cached_dataset, dataset_with_fids], how='diagonal').unique(subset='fid')\n",
    "\n",
    "                            self.base_fids = self._cached_dataset['fid']\n",
    "                            print(f\"Established new base FIDs from {name}: {len(self.base_fids)} records\")\n",
    "                        else:\n",
    "                            print(f\"Warning: No base FIDs available for {name}\")\n",
    "                            self._cached_dataset = scan_query.limit(self.sample_size).collect()\n",
    "                else:\n",
    "                    print(f\"Filtering {name} by {len(self.base_fids)} base FIDs\")\n",
    "                    self._cached_dataset = (scan_query\n",
    "                        .filter(pl.col('fid').is_in(self.base_fids))\n",
    "                        .collect())\n",
    "            else:\n",
    "                self._cached_dataset = scan_query.collect()\n",
    "                    \n",
    "            print(f\"Loaded {name}: {len(self._cached_dataset)} records\")\n",
    "            return self._cached_dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {name}: {str(e)}\")\n",
    "            raise\n",
    "            return pl.DataFrame()\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the cached dataset\"\"\"\n",
    "        self._cached_dataset = None\n",
    "        self._cached_name = None        \n",
    "\n",
    "class FeatureSet:\n",
    "    \"\"\"Track feature dependencies and versioning\"\"\"\n",
    "    def __init__(self, name: str, version: str, dependencies: List[str] = None):\n",
    "        self.name = name\n",
    "        self.version = version  # Version of feature calculation logic\n",
    "        self.dependencies = dependencies or []\n",
    "        self.checkpoint_path = None\n",
    "        self.last_modified = None\n",
    "\n",
    "class FeatureEngineering:\n",
    "    \"\"\"Enhanced bot detection system\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, checkpoint_dir: str, fids_to_ensure: List[int] = None):\n",
    "        self.data_path = data_path\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.loader = LazyDatasetLoader(data_path, checkpoint_dir, fids_to_ensure=fids_to_ensure)\n",
    "        self.fids_to_ensure = fids_to_ensure\n",
    "        \n",
    "        # Define comprehensive feature dependencies and versions\n",
    "        self.feature_sets = {\n",
    "            # Base features\n",
    "            'profile': FeatureSet('profile', '1.0'),\n",
    "            'network': FeatureSet('network', '1.0'),\n",
    "            'temporal': FeatureSet('temporal', '1.0', ['network']),\n",
    "            \n",
    "            # Activity features\n",
    "            'cast': FeatureSet('cast', '1.0'),\n",
    "            'reaction': FeatureSet('reaction', '1.0'),\n",
    "            'channel': FeatureSet('channel', '1.0'),\n",
    "            'verification': FeatureSet('verification', '1.0'),\n",
    "            \n",
    "            # Account features\n",
    "            'user_data': FeatureSet('user_data', '1.0'),\n",
    "            'storage': FeatureSet('storage', '1.0'),\n",
    "            'signers': FeatureSet('signers', '1.0'),\n",
    "            \n",
    "            # Interaction patterns\n",
    "            'engagement': FeatureSet('engagement', '1.0', \n",
    "                ['cast', 'reaction', 'channel']),\n",
    "            'mentions': FeatureSet('mentions', '1.0', \n",
    "                ['cast', 'network']),\n",
    "            'reply_patterns': FeatureSet('reply_patterns', '1.0', \n",
    "                ['cast', 'temporal']),\n",
    "            \n",
    "            # Network quality\n",
    "            'network_quality': FeatureSet('network_quality', '1.0', \n",
    "                ['network', 'engagement']),\n",
    "            'power_user_interaction': FeatureSet('power_user_interaction', '1.0', \n",
    "                ['network', 'temporal']),\n",
    "            'cluster_analysis': FeatureSet('cluster_analysis', '1.0', \n",
    "                ['network', 'engagement']),\n",
    "            \n",
    "            # Behavioral patterns\n",
    "            'activity_patterns': FeatureSet('activity_patterns', '1.0', \n",
    "                ['temporal', 'cast', 'reaction']),\n",
    "            'update_behavior': FeatureSet('update_behavior', '1.0', \n",
    "                ['user_data', 'profile']),\n",
    "            'verification_patterns': FeatureSet('verification_patterns', '1.0', \n",
    "                ['verification', 'temporal']),\n",
    "            \n",
    "            # Meta features\n",
    "            'authenticity': FeatureSet('authenticity', '2.0', [\n",
    "                'profile', 'network', 'channel', 'verification',\n",
    "                'engagement', 'network_quality', 'activity_patterns'\n",
    "            ]),\n",
    "            'influence': FeatureSet('influence', '1.0', [\n",
    "                'network', 'engagement', 'power_user_interaction'\n",
    "            ]),\n",
    "            \n",
    "            # Final derived features\n",
    "            'derived': FeatureSet('derived', '2.0', [\n",
    "                'network', 'temporal', 'authenticity',\n",
    "                'engagement', 'network_quality', 'influence'\n",
    "            ]),\n",
    "\n",
    "            # nindexer features\n",
    "            'enhanced_network': FeatureSet('enhanced_network', '1.0', \n",
    "                ['network']),\n",
    "            'enhanced_profile': FeatureSet('enhanced_profile', '1.0', \n",
    "                ['profile']),\n",
    "            'neynar_score': FeatureSet('neynar_score', '1.0'),\n",
    "\n",
    "            'name_patterns': FeatureSet('name_patterns', '1.0', ['profile']),\n",
    "            'content_patterns': FeatureSet('content_patterns', '1.0', ['cast']),\n",
    "            'advanced_temporal': FeatureSet('advanced_temporal', '1.0', ['temporal', 'cast', 'reaction']),\n",
    "            'reward_gaming': FeatureSet('reward_gaming', '1.0', ['cast', 'reaction', 'temporal']),\n",
    "            'engagement_authenticity': FeatureSet('engagement_authenticity', '1.0', ['network', 'cast', 'reaction'])\n",
    "\n",
    "        }\n",
    "        \n",
    "        # Initialize checkpoint tracking\n",
    "        self._init_checkpoints()\n",
    "\n",
    "    def _analyze_name_patterns(self, text: str) -> Dict[str, int]:\n",
    "        \"\"\"Analyze username/display name patterns\"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'random_numbers': 0,\n",
    "                'wallet_pattern': 0,\n",
    "                'excessive_symbols': 0,\n",
    "                'airdrop_terms': 0,\n",
    "                'has_year': 0\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'random_numbers': int(bool(re.findall(r'\\d{4,}', text))),\n",
    "            'wallet_pattern': int(bool(re.findall(r'0x[a-fA-F0-9]{40}', text))),\n",
    "            'excessive_symbols': int(bool(re.findall(r'[_.\\-]{2,}', text))),\n",
    "            'airdrop_terms': int(any(term in text.lower() for term in ['airdrop', 'farm', 'degen', 'wojak'])),\n",
    "            'has_year': int(bool(re.findall(r'20[12]\\d', text)))\n",
    "        }\n",
    "\n",
    "    def _analyze_content_patterns(self, text: str) -> Dict[str, int]:\n",
    "        \"\"\"Analyze content for spam/bot patterns\"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'template_structure': 0,\n",
    "                'multiple_cta': 0,\n",
    "                'urgency_terms': 0,\n",
    "                'excessive_emojis': 0,\n",
    "                'price_mentions': 0\n",
    "            }\n",
    "        \n",
    "        text = text.lower()\n",
    "        return {\n",
    "            'template_structure': int(bool(re.findall(r'\\[.*?\\]|\\{.*?\\}|\\<.*?\\>', text))),\n",
    "            'multiple_cta': int(len(re.findall(r'click|join|follow|claim|grab', text)) > 2),\n",
    "            'urgency_terms': int(bool(re.findall(r'hurry|limited|fast|quick|soon|ending', text))),\n",
    "            'excessive_emojis': int(len(re.findall(r'[\\U0001F300-\\U0001F9FF]', text)) > 5),\n",
    "            'price_mentions': int(bool(re.findall(r'\\$\\d+|\\d+\\$', text))),\n",
    "            'excessive_symbols': int(bool(re.findall(r'[_.\\-]{2,}', text))),\n",
    "            'airdrop_terms': int(any(term in text.lower() for term in ['airdrop', 'farm', 'degen', 'wojak'])),\n",
    "        }\n",
    "        \n",
    "    def validate_dimensions(func):\n",
    "        \"\"\"Decorator to validate DataFrame dimensions\"\"\"\n",
    "        def wrapper(self, df: pl.DataFrame, *args, **kwargs):\n",
    "            input_shape = len(df)\n",
    "            try:\n",
    "                result = func(self, df, *args, **kwargs)\n",
    "                if len(result) != input_shape:\n",
    "                    print(f\"Warning: Shape mismatch in {func.__name__}. Input: {input_shape}, Output: {len(result)}\")\n",
    "                    # Don't force join or filtering here. Just warn.\n",
    "                return result.fill_null(0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {func.__name__}: {str(e)}\")\n",
    "                raise\n",
    "        return wrapper\n",
    "\n",
    "        \n",
    "    def get_dataset_columns(self, name: str) -> List[str]:\n",
    "        \"\"\"Get the list of columns from the dataset without loading data\"\"\"\n",
    "        path = f\"{self.data_path}/farcaster-{name}-0-1733162400.parquet\"\n",
    "        ds = pl.scan_parquet(path)\n",
    "        return ds.columns\n",
    "        \n",
    "    def _init_checkpoints(self):\n",
    "        \"\"\"Initialize checkpoint paths and check existing files\"\"\"\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        for name, feature_set in self.feature_sets.items():\n",
    "            path = f\"{self.checkpoint_dir}/{name}_features.parquet\"\n",
    "            feature_set.checkpoint_path = path\n",
    "            \n",
    "            if os.path.exists(path):\n",
    "                feature_set.last_modified = os.path.getmtime(path)\n",
    "    \n",
    "    def _needs_rebuild(self, feature_set: FeatureSet) -> bool:\n",
    "        \"\"\"Check if feature set needs to be rebuilt\"\"\"\n",
    "        # Always rebuild if no checkpoint exists\n",
    "        if not os.path.exists(feature_set.checkpoint_path):\n",
    "            return True\n",
    "                \n",
    "        return False\n",
    "\n",
    "\n",
    "    def extract_profile_features(self) -> pl.DataFrame:\n",
    "        \"\"\"Extract comprehensive profile features\"\"\"\n",
    "        profiles = self.loader.get_dataset('profile_with_addresses', \n",
    "            ['fid', 'fname', 'bio', 'avatar_url', 'verified_addresses', 'display_name'])\n",
    "        \n",
    "        # Filter valid profiles and cast fid type immediately\n",
    "        profiles = (profiles\n",
    "            .filter(pl.col('fname').is_not_null() & (pl.col('fname') != \"\"))\n",
    "            .with_columns(pl.col('fid').cast(pl.Int64)))\n",
    "        \n",
    "        df = profiles.with_columns([\n",
    "            pl.col('fname').str.contains(r'\\.eth$').cast(pl.Int32).alias('has_ens'),\n",
    "            (pl.col('bio').is_not_null() & (pl.col('bio') != \"\")).cast(pl.Int32).alias('has_bio'),\n",
    "            pl.col('avatar_url').is_not_null().cast(pl.Int32).alias('has_avatar'),\n",
    "            pl.when(pl.col('verified_addresses').str.contains(','))\n",
    "            .then(pl.col('verified_addresses').str.contains(',').cast(pl.Int32) + 1)\n",
    "            .otherwise(pl.when(pl.col('verified_addresses') != '[]')\n",
    "                        .then(1)\n",
    "                        .otherwise(0))\n",
    "            .alias('verification_count'),\n",
    "            (pl.col('display_name').is_not_null()).cast(pl.Int32).alias('has_display_name')\n",
    "        ])\n",
    "        \n",
    "        self.loader.clear_cache()\n",
    "        return df\n",
    "    def add_blocking_behavior(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add memory-efficient blocking behavior features\"\"\"\n",
    "        blocks = self.loader.get_dataset('blocks', ['blocker_fid', 'blocked_fid'])\n",
    "        \n",
    "        blocking_features = (\n",
    "            blocks.group_by('blocker_fid')\n",
    "            .agg([\n",
    "                pl.count().alias('blocks_made'),\n",
    "                pl.n_unique('blocked_fid').alias('unique_blocks')\n",
    "            ])\n",
    "            .with_columns([\n",
    "                (pl.col('blocks_made') / (pl.col('unique_blocks') + 1)).alias('block_repeat_ratio')\n",
    "            ])\n",
    "            .rename({'blocker_fid': 'fid'})\n",
    "        )\n",
    "        \n",
    "        self.loader.clear_cache()\n",
    "        return df.join(blocking_features, on='fid', how='left').fill_null(0)\n",
    "\n",
    "    def add_enhanced_verification_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add verification features with proper string handling\"\"\"\n",
    "        try:\n",
    "            # Initialize result with defaults\n",
    "            result = df.with_columns([\n",
    "                pl.lit(0).alias('total_verifications'),\n",
    "                pl.lit(0).alias('eth_verifications'),\n",
    "                pl.lit(0.0).alias('verification_timing_std'),\n",
    "                pl.lit(0).alias('platforms_verified'),\n",
    "                pl.lit(None).alias('first_platform_verification'),\n",
    "                pl.lit(None).alias('last_platform_verification'),\n",
    "                pl.lit(0).alias('verification_span_days')\n",
    "            ])\n",
    "            \n",
    "            # Process on-chain verifications\n",
    "            verifications = self.loader.get_dataset('verifications', \n",
    "                ['fid', 'claim', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            if verifications is not None and len(verifications) > 0:\n",
    "                verif_features = (\n",
    "                    verifications\n",
    "                    .filter(pl.col('deleted_at').is_null())\n",
    "                    .with_columns([\n",
    "                        pl.col('timestamp').cast(pl.Datetime)\n",
    "                    ])\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        pl.len().alias('total_verifications'),\n",
    "                        pl.col('claim').str.contains('ethSignature').sum().alias('eth_verifications'),\n",
    "                        # Convert durations to floats and fill nulls before std()\n",
    "                        pl.col('timestamp')\n",
    "                            .diff()\n",
    "                            .dt.total_seconds()\n",
    "                            .cast(pl.Float64)\n",
    "                            .fill_null(0)\n",
    "                            .std()\n",
    "                            .fill_null(0)\n",
    "                            .alias('verification_timing_std')\n",
    "                    ])\n",
    "                )\n",
    "                verif_features = verif_features.unique(subset=['fid']) \n",
    "                result = result.join(verif_features, on='fid', how='left')\n",
    "            \n",
    "            # Process platform verifications\n",
    "            acc_verifications = self.loader.get_dataset('account_verifications', \n",
    "                ['fid', 'platform', 'platform_username', 'verified_at'])\n",
    "            \n",
    "            if acc_verifications is not None and len(acc_verifications) > 0:\n",
    "                platform_features = (\n",
    "                    acc_verifications\n",
    "                    .with_columns([\n",
    "                        pl.col('platform_username').map_elements(lambda x: len(str(x)) if x else 0, return_dtype=pl.Int64),\n",
    "                        pl.col('verified_at').cast(pl.Datetime)\n",
    "                    ])\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        pl.n_unique('platform').alias('platforms_verified'),\n",
    "                        pl.col('verified_at').min().alias('first_platform_verification'),\n",
    "                        pl.col('verified_at').max().alias('last_platform_verification')\n",
    "                    ])\n",
    "                )\n",
    "                platform_features = platform_features.unique(subset=['fid']) \n",
    "                result = result.join(platform_features, on='fid', how='left')\n",
    "\n",
    "                result = result.with_columns([\n",
    "                    # First ensure both columns are Datetime\n",
    "                    pl.col('last_platform_verification').cast(pl.Datetime),\n",
    "                    pl.col('first_platform_verification').cast(pl.Datetime)\n",
    "                ])\n",
    "\n",
    "                # Compute duration safely in a separate step\n",
    "                result = result.with_columns([\n",
    "                    (pl.col('last_platform_verification') - pl.col('first_platform_verification'))\n",
    "                        .alias('verification_duration')\n",
    "                ])\n",
    "\n",
    "                # Now handle the null durations and convert to days\n",
    "                result = result.with_columns([\n",
    "                    pl.when(pl.col('verification_duration').is_not_null())\n",
    "                    .then(\n",
    "                        pl.col('verification_duration')\n",
    "                        .dt.total_days()  # This should return Float64 if duration is valid\n",
    "                        .fill_null(0.0)   # fill null if any appear\n",
    "                    )\n",
    "                    .otherwise(0.0)\n",
    "                    .alias('verification_span_days')\n",
    "                ])\n",
    "\n",
    "                # Drop the intermediate column if not needed\n",
    "                result = result.drop('verification_duration')\n",
    "\n",
    "            self.loader.clear_cache()\n",
    "            return result.fill_null(0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in verification features: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "    def add_cast_behavior_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add cast behavior features including link and media analysis\"\"\"\n",
    "        try:\n",
    "            base_fids = df['fid']\n",
    "            print(f\"Processing casts for {len(base_fids)} FIDs\")\n",
    "            \n",
    "            # Get casts with all needed fields\n",
    "            casts_df = self.loader.get_dataset('casts', columns=[\n",
    "                'fid', 'text', 'parent_hash', 'mentions', 'deleted_at', \n",
    "                'timestamp', 'embeds'  # Adding embeds for media detection\n",
    "            ])\n",
    "            \n",
    "            # Calculate features safely\n",
    "            valid_casts = casts_df.filter(pl.col('deleted_at').is_null())\n",
    "            def analyze_spam_patterns(text: str) -> Dict[str, int]:\n",
    "                if not text:\n",
    "                    return {'airdrop': 0, 'money': 0, 'rewards': 0, 'claim': 0, 'moxie': 0}\n",
    "                    \n",
    "                text = text.lower()\n",
    "                spam_keywords = ['airdrop', 'money', 'rewards', 'claim', 'moxie', 'nft', 'drop']\n",
    "                return {\n",
    "                    word: text.count(word) \n",
    "                    for word in spam_keywords\n",
    "                }\n",
    "                \n",
    "            def get_symbol_ratios(text: str) -> Dict[str, float]:\n",
    "                if not text:\n",
    "                    return {'at_symbol_ratio': 0, 'dollar_symbol_ratio': 0, 'link_ratio': 0}\n",
    "                    \n",
    "                total_length = len(text)\n",
    "                return {\n",
    "                    'at_symbol_ratio': text.count('@') / total_length if total_length > 0 else 0,\n",
    "                    'dollar_symbol_ratio': text.count('$') / total_length if total_length > 0 else 0,\n",
    "                    'link_ratio': len(re.findall(r'http[s]?://', text)) / total_length if total_length > 0 else 0\n",
    "                }\n",
    "            # Helper function to count links in text\n",
    "            def count_links(text):\n",
    "                if not text:\n",
    "                    return 0\n",
    "                # Look for common URL patterns\n",
    "                url_patterns = ['http://', 'https://', 'www.']\n",
    "                return sum(1 for pattern in url_patterns if pattern in text.lower())\n",
    "            \n",
    "            # Helper function to count media items in embeds\n",
    "            def count_media(embeds):\n",
    "                if not embeds or embeds == '[]':\n",
    "                    return 0\n",
    "                try:\n",
    "                    # Count image URLs in embeds\n",
    "                    return embeds.lower().count('image')\n",
    "                except:\n",
    "                    return 0\n",
    "            \n",
    "            # Add link and media detection\n",
    "            cast_features = (valid_casts\n",
    "                .with_columns([\n",
    "                    # Existing features\n",
    "                    pl.when(pl.col('text').is_not_null())\n",
    "                    .then(pl.col('text').map_elements(lambda x: len(x) if x else 0, return_dtype=pl.Int64))\n",
    "                    .otherwise(0)\n",
    "                    .alias('cast_length'),\n",
    "                    pl.col('parent_hash').is_not_null().cast(pl.Int32).alias('is_reply'),\n",
    "                    (pl.col('mentions').is_not_null() & \n",
    "                    (pl.col('mentions') != '') & \n",
    "                    (pl.col('mentions') != '[]')).cast(pl.Int32).alias('has_mentions'),\n",
    "                    \n",
    "                    # New features for links and media\n",
    "                    pl.when(pl.col('text').is_not_null())\n",
    "                    .then(pl.col('text').map_elements(count_links, return_dtype=pl.Int32))\n",
    "                    .otherwise(0)\n",
    "                    .alias('link_count'),\n",
    "                    \n",
    "                    pl.when(pl.col('embeds').is_not_null())\n",
    "                    .then(pl.col('embeds').map_elements(count_media, return_dtype=pl.Int32))\n",
    "                    .otherwise(0)\n",
    "                    .alias('media_count'),\n",
    "                    \n",
    "                    # Flag for casts containing both link and media\n",
    "                    (pl.when(pl.col('text').is_not_null())\n",
    "                    .then(pl.col('text').map_elements(count_links, return_dtype=pl.Int32))\n",
    "                    .otherwise(0) > 0 &\n",
    "                    pl.when(pl.col('embeds').is_not_null())\n",
    "                    .then(pl.col('embeds').map_elements(count_media, return_dtype=pl.Int32))\n",
    "                    .otherwise(0) > 0)\n",
    "                    .cast(pl.Int32)\n",
    "                    .alias('has_link_and_media'),\n",
    "\n",
    "                    pl.col('text').map_elements(analyze_spam_patterns, return_dtype=pl.Utf8).alias('spam_counts'),\n",
    "                    pl.col('text').map_elements(get_symbol_ratios, return_dtype=pl.Utf8).alias('symbol_ratios'),\n",
    "                    pl.col('text').map_elements(self._analyze_content_patterns, return_dtype=pl.Utf8).alias('content_patterns')\n",
    "\n",
    "                ])\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    # Existing metrics\n",
    "                    pl.len().alias('cast_count'),\n",
    "                    pl.col('cast_length').mean().alias('avg_cast_length'),\n",
    "                    pl.col('is_reply').sum().alias('reply_count'),\n",
    "                    pl.col('has_mentions').sum().alias('mentions_count'),\n",
    "                    \n",
    "                    # New metrics for links\n",
    "                    pl.col('link_count').sum().alias('total_links'),\n",
    "                    (pl.col('link_count') > 0).sum().alias('casts_with_links'),\n",
    "                    (pl.col('link_count') / pl.len()).alias('link_ratio'),\n",
    "                    \n",
    "                    # New metrics for media\n",
    "                    pl.col('media_count').sum().alias('total_media'),\n",
    "                    (pl.col('media_count') > 0).sum().alias('casts_with_media'),\n",
    "                    (pl.col('media_count') / pl.len()).alias('media_ratio'),\n",
    "                    \n",
    "                    # Spam metrics\n",
    "                    (pl.col('spam_counts').map_elements(lambda x: x['airdrop']).sum() / pl.len())\n",
    "                        .alias('airdrop_mention_ratio'),\n",
    "                    (pl.col('spam_counts').map_elements(lambda x: sum(x.values())).sum() / pl.len())\n",
    "                        .alias('spam_keyword_ratio'),\n",
    "                        \n",
    "                    # Symbol usage metrics\n",
    "                    pl.col('symbol_ratios').map_elements(lambda x: x['at_symbol_ratio']).mean()\n",
    "                        .alias('avg_at_symbol_ratio'),\n",
    "                    pl.col('symbol_ratios').map_elements(lambda x: x['dollar_symbol_ratio']).mean()\n",
    "                        .alias('avg_dollar_symbol_ratio'),\n",
    "                    pl.col('symbol_ratios').map_elements(lambda x: x['link_ratio']).mean()\n",
    "                        .alias('avg_link_ratio'),\n",
    "                        \n",
    "                    # Combined metrics\n",
    "                    pl.col('has_link_and_media').sum().alias('casts_with_both'),\n",
    "                    (pl.col('has_link_and_media').sum() / pl.len()).alias('multimedia_ratio')\n",
    "\n",
    "                    # Content pattern metrics\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['template_structure'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('template_usage_ratio'),\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['multiple_cta'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('cta_heavy_ratio'),\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['urgency_terms'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('urgency_ratio'),\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['excessive_emojis'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('emoji_spam_ratio'),\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['price_mentions'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('price_mention_ratio'),\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['excessive_symbols'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('symbol_spam_ratio'),\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['airdrop_terms'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('airdrop_term_ratio')\n",
    "                ]))\n",
    "            \n",
    "            cast_features = cast_features.unique(subset=['fid']) \n",
    "            # Join and handle nulls\n",
    "            result = df.join(cast_features, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "            # Add derived ratios\n",
    "            result = result.with_columns([\n",
    "                # Percentage of casts that contain links\n",
    "                (pl.col('casts_with_links') / pl.col('cast_count')).alias('link_usage_rate'),\n",
    "                # Percentage of casts that contain media\n",
    "                (pl.col('casts_with_media') / pl.col('cast_count')).alias('media_usage_rate'),\n",
    "                # Average number of links per cast with links\n",
    "                (pl.col('total_links') / (pl.col('casts_with_links') + 1)).alias('avg_links_per_link_cast'),\n",
    "                # Average number of media items per cast with media\n",
    "                (pl.col('total_media') / (pl.col('casts_with_media') + 1)).alias('avg_media_per_media_cast')\n",
    "            ])\n",
    "            \n",
    "            return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in cast behavior: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "\n",
    "    def add_influence_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add influence features with proper error handling\"\"\"\n",
    "        try:\n",
    "            # Ensure required columns exist and are properly initialized\n",
    "            required_cols = ['follower_count', 'following_count', 'total_reactions', 'cast_count']\n",
    "            for col in required_cols:\n",
    "                if col not in df.columns:\n",
    "                    df = df.with_columns(pl.lit(0).alias(col))\n",
    "            \n",
    "            # Calculate time span if possible\n",
    "            if 'first_follow' in df.columns and 'last_follow' in df.columns:\n",
    "                df = df.with_columns([\n",
    "                    pl.when(pl.col('last_follow').is_not_null() & pl.col('first_follow').is_not_null())\n",
    "                    .then((pl.col('last_follow') - pl.col('first_follow')).dt.total_hours())\n",
    "                    .otherwise(0)\n",
    "                    .alias('follow_time_span_hours')\n",
    "                ])\n",
    "            else:\n",
    "                df = df.with_columns(pl.lit(0).alias('follow_time_span_hours'))\n",
    "\n",
    "            # Calculate influence metrics safely\n",
    "            df = df.with_columns([\n",
    "                # Normalize influence metrics\n",
    "                ((pl.col('follower_count').fill_null(0) * 0.4 +\n",
    "                pl.col('total_reactions').fill_null(0) * 0.3 +\n",
    "                pl.col('cast_count').fill_null(0) * 0.3) / \n",
    "                (pl.col('following_count').fill_null(0) + 1)\n",
    "                ).alias('influence_score'),\n",
    "                \n",
    "                # Safe engagement rate calculation\n",
    "                (pl.when(pl.col('cast_count') > 0)\n",
    "                .then(pl.col('total_reactions') / pl.col('cast_count'))\n",
    "                .otherwise(0)\n",
    "                ).alias('engagement_rate'),\n",
    "                \n",
    "                # Safe follower growth rate calculation\n",
    "                (pl.when(pl.col('follow_time_span_hours') > 0)\n",
    "                .then(pl.col('follower_count') / pl.col('follow_time_span_hours'))\n",
    "                .otherwise(0)\n",
    "                ).alias('follower_growth_rate')\n",
    "            ])\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in influence features: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "    def add_storage_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add storage features with updated functions\"\"\"\n",
    "        storage = self.loader.get_dataset('storage', ['fid', 'units', 'deleted_at'])\n",
    "        \n",
    "        storage_features = (\n",
    "            storage.filter(pl.col('deleted_at').is_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.col('units').mean().alias('avg_storage_units'),\n",
    "                pl.col('units').max().alias('max_storage_units'),\n",
    "                pl.len().alias('storage_update_count')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        self.loader.clear_cache()\n",
    "        storage_features = storage_features.unique(subset=['fid']) \n",
    "        return df.join(storage_features, on='fid', how='left').fill_null(0)\n",
    "    def add_user_data_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Extract features from user_data with better error handling\"\"\"\n",
    "        try:\n",
    "            user_data = self.loader.get_dataset('user_data', \n",
    "                ['fid', 'type', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            if user_data is None or len(user_data) == 0:\n",
    "                return df.with_columns([\n",
    "                    pl.lit(0).alias('total_user_data_updates'),\n",
    "                    pl.lit(0.0).alias('avg_update_interval')\n",
    "                ])\n",
    "                \n",
    "            update_features = (\n",
    "                user_data.filter(pl.col('deleted_at').is_null())\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    pl.len().alias('total_user_data_updates'),\n",
    "                    pl.col('timestamp').diff().mean().dt.total_hours().fill_null(0)\n",
    "                        .alias('avg_update_interval')\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            self.loader.clear_cache()\n",
    "            update_features = update_features.unique(subset=['fid']) \n",
    "            return df.join(update_features, on='fid', how='left').fill_null(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in user_data features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0).alias('total_user_data_updates'),\n",
    "                pl.lit(0.0).alias('avg_update_interval')\n",
    "            ])\n",
    "    def add_signer_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Extract features from signer behavior\"\"\"\n",
    "        signers = self.loader.get_dataset('signers', \n",
    "            ['fid', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        signer_features = (\n",
    "            signers.filter(pl.col('deleted_at').is_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.count().alias('signer_count'),\n",
    "                pl.col('timestamp').diff().mean().dt.total_hours().alias('avg_hours_between_signers'),\n",
    "                pl.col('timestamp').diff().std().dt.total_hours().alias('std_hours_between_signers')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        self.loader.clear_cache()\n",
    "        signer_features = signer_features.unique(subset=['fid']) \n",
    "        return df.join(signer_features, on='fid', how='left').fill_null(0)\n",
    "        \n",
    "    def add_reaction_patterns(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add reaction pattern features with dimension validation\"\"\"\n",
    "        try:\n",
    "            base_fids = df['fid']\n",
    "            print(f\"Processing reactions for {len(base_fids)} FIDs\")\n",
    "            \n",
    "            reactions = self.loader.get_dataset('reactions', \n",
    "                ['fid', 'reaction_type', 'target_fid', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            # First filter by base FIDs\n",
    "            reactions = reactions.filter(pl.col('fid').is_in(base_fids))\n",
    "            reaction_features = (\n",
    "                reactions.filter(pl.col('deleted_at').is_null())\n",
    "                .with_columns([\n",
    "                    pl.col('timestamp').cast(pl.Datetime)\n",
    "                ])\n",
    "                .sort('timestamp')\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    pl.len().alias('total_reactions'),\n",
    "                    (pl.col('reaction_type') == 1).sum().alias('like_count'),\n",
    "                    (pl.col('reaction_type') == 2).sum().alias('recast_count'),\n",
    "                    pl.n_unique('target_fid').alias('unique_users_reacted_to'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().mean().alias('avg_hours_between_reactions'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().std().alias('std_hours_between_reactions')\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            # Calculate ratios only after joining back to maintain dimensions\n",
    "            reaction_features = reaction_features.unique(subset=['fid']) \n",
    "            result = df.join(reaction_features, on='fid', how='left', coalesce=True).fill_null(0)\n",
    "            \n",
    "            result = result.with_columns([\n",
    "                (pl.col('like_count') / (pl.col('total_reactions') + 1)).alias('like_ratio'),\n",
    "                (pl.col('recast_count') / (pl.col('total_reactions') + 1)).alias('recast_ratio'),\n",
    "                (pl.col('unique_users_reacted_to') / (pl.col('total_reactions') + 1)).alias('reaction_diversity'),\n",
    "                (pl.col('like_count') / (pl.col('recast_count') + 1)).alias('likes_to_recasts_ratio'),\n",
    "            ])\n",
    "            \n",
    "            # Verify dimensions\n",
    "            if len(result) != len(df):\n",
    "                print(f\"Warning: Reaction features shape mismatch. Expected {len(df)}, got {len(result)}\")\n",
    "                result = result.filter(pl.col('fid').is_in(base_fids))\n",
    "                \n",
    "            self.loader.clear_cache()\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in reaction patterns: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "    def build_network_quality_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Build network quality features with safer dependency handling\"\"\"\n",
    "        try:\n",
    "            # Ensure required base metrics exist\n",
    "            base_metrics = {\n",
    "                'engagement_score': 0.0,\n",
    "                'following_count': 0,\n",
    "                'follower_count': 0\n",
    "            }\n",
    "            \n",
    "            result = self._validate_and_ensure_features(df, base_metrics)\n",
    "            \n",
    "            # Load power users\n",
    "            power_users = self.loader.get_dataset('power_users', ['fid'])\n",
    "            if power_users is None or len(power_users) == 0:\n",
    "                return result.with_columns([\n",
    "                    pl.lit(0).alias('power_reply_count'),\n",
    "                    pl.lit(0).alias('power_mentions_count')\n",
    "                ])\n",
    "            \n",
    "            # Calculate power user metrics\n",
    "            power_fids = power_users['fid'].cast(pl.Int64).unique()\n",
    "            casts = self.loader.get_dataset('casts', \n",
    "                ['fid', 'parent_fid', 'mentions', 'deleted_at'])\n",
    "                \n",
    "            if casts is not None and len(casts) > 0:\n",
    "                power_fid_str = str(power_fids[0])\n",
    "\n",
    "                power_metrics = (\n",
    "                    casts.filter(pl.col('deleted_at').is_null())\n",
    "                    .with_columns([\n",
    "                        pl.col('parent_fid').cast(pl.Int64).is_in(power_fids)\n",
    "                            .alias('is_power_reply'),\n",
    "                        pl.when(pl.col('mentions').is_not_null() & pl.col('mentions').str.contains(power_fid_str))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias('has_power_mention')\n",
    "                    ])\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        pl.sum('is_power_reply').alias('power_reply_count'),\n",
    "                        pl.sum('has_power_mention').alias('power_mentions_count')\n",
    "                    ])\n",
    "                )\n",
    "                \n",
    "                result = result.join(power_metrics, on='fid', how='left').fill_null(0)\n",
    "                \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in network quality features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0).alias('power_reply_count'),\n",
    "                pl.lit(0).alias('power_mentions_count')\n",
    "            ])\n",
    "\n",
    "    def add_network_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add network features with proper error handling and null safety\"\"\"\n",
    "        try:\n",
    "            links = self.loader.get_dataset('links', \n",
    "                ['fid', 'target_fid', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            # Filter valid links first\n",
    "            valid_links = links.filter(pl.col('deleted_at').is_null())\n",
    "            \n",
    "            # Calculate following patterns safely\n",
    "            following = (valid_links\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    pl.len().alias('following_count'),\n",
    "                    pl.n_unique('target_fid').alias('unique_following_count'),\n",
    "                    pl.col('timestamp').min().alias('first_follow'),\n",
    "                    pl.col('timestamp').max().alias('last_follow')\n",
    "                ])\n",
    "                .fill_null(0))\n",
    "            \n",
    "            # Calculate follower patterns separately\n",
    "            followers = (valid_links\n",
    "                .group_by('target_fid')\n",
    "                .agg([\n",
    "                    pl.len().alias('follower_count'),\n",
    "                    pl.n_unique('fid').alias('unique_follower_count')\n",
    "                ])\n",
    "                .rename({'target_fid': 'fid'})\n",
    "                .fill_null(0))\n",
    "            \n",
    "            # Join both patterns\n",
    "            result = df.join(following, on='fid', how='left').fill_null(0)\n",
    "            result = result.join(followers, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "            # Calculate ratios safely with null handling\n",
    "            result = result.with_columns([\n",
    "                (pl.col('follower_count') / (pl.col('following_count') + 1))\n",
    "                    .alias('follower_ratio'),\n",
    "                (pl.col('unique_follower_count') / (pl.col('unique_following_count') + 1))\n",
    "                    .alias('unique_follower_ratio'),\n",
    "                \n",
    "                # Add log transformations\n",
    "                (pl.col('follower_count') / (pl.col('following_count') + 1))\n",
    "                    .log1p()\n",
    "                    .alias('follower_ratio_log'),\n",
    "                (pl.col('unique_follower_count') / (pl.col('unique_following_count') + 1))\n",
    "                    .log1p()\n",
    "                    .alias('unique_follower_ratio_log')\n",
    "            ])\n",
    "            \n",
    "            return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in network features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0).alias('following_count'),\n",
    "                pl.lit(0).alias('unique_following_count'),\n",
    "                pl.lit(0).alias('follower_count'),\n",
    "                pl.lit(0).alias('unique_follower_count'),\n",
    "                pl.lit(0.0).alias('follower_ratio'),\n",
    "                pl.lit(0.0).alias('unique_follower_ratio'),\n",
    "                pl.lit(0.0).alias('follower_ratio_log'),\n",
    "                pl.lit(0.0).alias('unique_follower_ratio_log')\n",
    "            ])\n",
    "    def add_temporal_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add enhanced temporal features with burst detection\"\"\"\n",
    "        try:\n",
    "            links = self.loader.get_dataset('links', ['fid', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            # Ensure timestamp is datetime type\n",
    "            valid_links = (links\n",
    "                .filter(pl.col('deleted_at').is_null())\n",
    "                .filter(pl.col('timestamp').is_not_null())\n",
    "                .with_columns([\n",
    "                    pl.col('timestamp').cast(pl.Datetime).alias('timestamp')\n",
    "                ]))\n",
    "            \n",
    "            temporal_features = (valid_links\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    # Basic temporal features\n",
    "                    pl.len().alias('total_activity'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().mean().alias('avg_hours_between_actions'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().std().alias('std_hours_between_actions'),\n",
    "                    pl.col('timestamp').dt.weekday().std().alias('weekday_variance'),\n",
    "                    (pl.col('timestamp').diff().dt.total_hours() < 1).sum().alias('rapid_actions'),\n",
    "                    (pl.col('timestamp').diff().dt.total_hours() > 24).sum().alias('long_gaps'),\n",
    "                    \n",
    "                    # New temporal features\n",
    "                    pl.col('timestamp').diff().dt.total_hours().quantile(0.9).alias('p90_time_between_actions'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().quantile(0.1).alias('p10_time_between_actions'),\n",
    "                    \n",
    "                    # Calculate burst ratio (actions within 1 hour of each other)\n",
    "                    (pl.col('timestamp').diff().dt.total_hours() < 1).sum().alias('actions_in_bursts'),\n",
    "                    \n",
    "                    # Calculate velocity\n",
    "                    (pl.col('timestamp').max() - pl.col('timestamp').min()).dt.total_hours().alias('time_span')\n",
    "                ]))\n",
    "            \n",
    "            # Add derived temporal metrics\n",
    "            result = df.join(temporal_features, on='fid', how='left').fill_null(0)\n",
    "            result = result.with_columns([\n",
    "                # Burst activity ratio\n",
    "                (pl.col('actions_in_bursts') / (pl.col('total_activity') + 1)).alias('burst_activity_ratio'),\n",
    "                \n",
    "                # Activity spread (ratio of actual timespan to expected even distribution)\n",
    "                (pl.col('time_span') / ((pl.col('total_activity') + 1) * pl.col('avg_hours_between_actions'))).alias('activity_spread'),\n",
    "                \n",
    "                # Temporal irregularity (variation in action timing)\n",
    "                (pl.col('std_hours_between_actions') / (pl.col('avg_hours_between_actions') + 1)).alias('temporal_irregularity'),\n",
    "                \n",
    "                # Follow velocity (follows per hour)\n",
    "                (pl.col('total_activity') / (pl.col('time_span') + 1)).alias('follow_velocity')\n",
    "            ])\n",
    "            \n",
    "            return result.fill_null(0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in temporal features: {str(e)}\")\n",
    "            raise\n",
    "            return df.fill_null(0)\n",
    "# \n",
    "    # def add_advanced_temporal_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add advanced temporal features for bot detection\"\"\"\n",
    "        try:\n",
    "            activities = []\n",
    "            \n",
    "            # Collect cast timestamps\n",
    "            casts = self.loader.get_dataset('casts', ['fid', 'timestamp', 'deleted_at'])\n",
    "            if casts is not None:\n",
    "                valid_casts = casts.filter(pl.col('deleted_at').is_null())\n",
    "                activities.append(valid_casts.select(['fid', 'timestamp']))\n",
    "            \n",
    "            # Collect reaction timestamps\n",
    "            reactions = self.loader.get_dataset('reactions', ['fid', 'timestamp', 'deleted_at'])\n",
    "            if reactions is not None:\n",
    "                valid_reactions = reactions.filter(pl.col('deleted_at').is_null())\n",
    "                activities.append(valid_reactions.select(['fid', 'timestamp']))\n",
    "            \n",
    "            if not activities:\n",
    "                return df\n",
    "            \n",
    "            # Combine all activities\n",
    "            all_activities = pl.concat(activities)\n",
    "            \n",
    "            temporal_features = (all_activities\n",
    "                .sort(['fid', 'timestamp'])\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    # Robotic timing detection\n",
    "                    (pl.col('timestamp').diff().dt.total_seconds().std() < 1)\n",
    "                        .cast(pl.Int32)\n",
    "                        .alias('has_robotic_timing'),\n",
    "                    \n",
    "                    # Rapid actions\n",
    "                    (pl.col('timestamp').diff().dt.total_seconds() < 2)\n",
    "                        .sum()\n",
    "                        .alias('rapid_action_count'),\n",
    "                    \n",
    "                    # Activity bursts\n",
    "                    (pl.col('timestamp').diff().dt.total_hours().gt(24).sum())\n",
    "                        .alias('long_dormancy_periods'),\n",
    "                        \n",
    "                    # Time between bursts\n",
    "                    pl.col('timestamp').diff().dt.total_hours().mean().alias('avg_burst_interval')\n",
    "                ]))\n",
    "            \n",
    "            return df.join(temporal_features, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in advanced temporal features: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def add_power_user_interaction_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Enhanced power user interaction analysis\"\"\"\n",
    "        try:\n",
    "            # Load power users\n",
    "            power_users = self.loader.get_dataset('warpcast_power_users', ['fid'])\n",
    "            if power_users is None or len(power_users) == 0:\n",
    "                print(\"Warning: No power users found\")\n",
    "                return df.with_columns([\n",
    "                    pl.lit(0).alias('power_user_replies'),\n",
    "                    pl.lit(0).alias('power_user_mentions'),\n",
    "                    pl.lit(0).alias('power_user_reactions'),\n",
    "                    pl.lit(0).alias('power_user_interaction_ratio')\n",
    "                ])\n",
    "            \n",
    "            # Ensure power_fids are Int64\n",
    "            power_fids = power_users['fid'].cast(pl.Int64).unique()\n",
    "            \n",
    "            # Get interactions with power users\n",
    "            casts = self.loader.get_dataset('casts', \n",
    "                ['fid', 'parent_fid', 'mentions', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            # Process cast interactions\n",
    "            power_fid_str = str(power_fids[0])\n",
    "            power_cast_features = (\n",
    "                casts.filter(pl.col('deleted_at').is_null())\n",
    "                .with_columns([\n",
    "                    pl.col('parent_fid').cast(pl.Int64).is_in(power_fids).alias('is_power_reply'),\n",
    "               pl.when(pl.col('mentions').is_not_null() & pl.col('mentions').str.contains(power_fid_str))\n",
    "        .then(1)\n",
    "        .otherwise(0)\n",
    "        .alias('has_power_mention')\n",
    "\n",
    "                ])\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    pl.sum('is_power_reply').alias('power_user_replies'),\n",
    "                    pl.sum('has_power_mention').alias('power_user_mentions'),\n",
    "                    pl.len().alias('total_casts')\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            # Get reaction data\n",
    "            reactions = self.loader.get_dataset('reactions', \n",
    "                ['fid', 'target_fid', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            power_reaction_features = (\n",
    "                reactions.filter(pl.col('deleted_at').is_null())\n",
    "                .with_columns([\n",
    "                    pl.col('target_fid').cast(pl.Int64).is_in(power_fids).alias('is_power_reaction')\n",
    "                ])\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    pl.sum('is_power_reaction').alias('power_user_reactions'),\n",
    "                    pl.len().alias('total_reactions')\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            # Join features\n",
    "            result = df.join(power_cast_features, on='fid', how='left')\n",
    "            result = result.join(power_reaction_features, on='fid', how='left')\n",
    "            \n",
    "            # Calculate interaction ratios\n",
    "            result = result.with_columns([\n",
    "                pl.col('power_user_replies').fill_null(0),\n",
    "                pl.col('power_user_mentions').fill_null(0),\n",
    "                pl.col('power_user_reactions').fill_null(0),\n",
    "                pl.col('total_casts').fill_null(0),\n",
    "                pl.col('total_reactions').fill_null(0)\n",
    "            ])\n",
    "            \n",
    "            # Calculate overall interaction ratio\n",
    "            result = result.with_columns([\n",
    "                ((pl.col('power_user_replies') + \n",
    "                pl.col('power_user_mentions') + \n",
    "                pl.col('power_user_reactions')) / \n",
    "                (pl.col('total_casts') + pl.col('total_reactions') + 1)\n",
    "                ).alias('power_user_interaction_ratio')\n",
    "            ])\n",
    "            \n",
    "            return result.fill_null(0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in power user interaction features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0).alias('power_user_replies'),\n",
    "                pl.lit(0).alias('power_user_mentions'),\n",
    "                pl.lit(0).alias('power_user_reactions'),\n",
    "                pl.lit(0).alias('power_user_interaction_ratio')\n",
    "            ])\n",
    "\n",
    "    def add_activity_patterns_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add activity patterns with fully safe calculations\"\"\"\n",
    "        try:\n",
    "            print(\"Processing activity patterns...\")\n",
    "            \n",
    "            # Get activity data\n",
    "            casts = self.loader.get_dataset('casts', ['fid', 'timestamp', 'deleted_at'])\n",
    "            reactions = self.loader.get_dataset('reactions', ['fid', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            # Initialize result with default values\n",
    "            result = df.with_columns([\n",
    "                pl.lit(0.0).alias('hour_diversity'),\n",
    "                pl.lit(0.0).alias('weekday_diversity'),\n",
    "                pl.lit(0.0).alias('total_activities')\n",
    "            ])\n",
    "            \n",
    "            # Process activities if data exists\n",
    "            if casts is not None and reactions is not None:\n",
    "                # Combine valid activities\n",
    "                activities = pl.concat([\n",
    "                    casts.filter(pl.col('deleted_at').is_null())\n",
    "                        .select(['fid', 'timestamp']),\n",
    "                    reactions.filter(pl.col('deleted_at').is_null())\n",
    "                        .select(['fid', 'timestamp'])\n",
    "                ])\n",
    "                \n",
    "                if len(activities) > 0:\n",
    "                    # Calculate activity metrics\n",
    "                    activity_features = (activities\n",
    "                        .with_columns([\n",
    "                            pl.col('timestamp').cast(pl.Datetime).dt.hour().alias('hour'),\n",
    "                            pl.col('timestamp').cast(pl.Datetime).dt.weekday().alias('weekday')\n",
    "                        ])\n",
    "                        .group_by('fid')\n",
    "                        .agg([\n",
    "                            pl.col('hour').value_counts()\n",
    "                                .std().fill_null(0).alias('hour_diversity'),\n",
    "                            pl.col('weekday').value_counts()\n",
    "                                .std().fill_null(0).alias('weekday_diversity'),\n",
    "                            pl.len().alias('total_activities')\n",
    "                        ])\n",
    "                    )\n",
    "                    \n",
    "                    # Update result with calculated features\n",
    "                    result = df.join(activity_features, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "            print(\"Activity patterns calculated successfully\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in activity patterns: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0.0).alias('hour_diversity'),\n",
    "                pl.lit(0.0).alias('weekday_diversity'),\n",
    "                pl.lit(0.0).alias('total_activities')\n",
    "            ])\n",
    "    def verify_matrix(self, df: pl.DataFrame):\n",
    "        \"\"\"Verify the final feature matrix has no list columns\"\"\"\n",
    "        for col in df.columns:\n",
    "            dtype = df[col].dtype\n",
    "            if str(dtype).startswith('List'):\n",
    "                raise ValueError(f\"Column {col} is still a list type: {dtype}\")\n",
    "            if dtype not in [pl.Float64, pl.Int64]:\n",
    "                raise ValueError(f\"Column {col} is not numeric: {dtype}\")\n",
    "                \n",
    "    def add_mentions_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Analyze mention patterns with proper null handling\"\"\"\n",
    "        try:\n",
    "            base_fids = df['fid']\n",
    "            print(f\"Processing mentions for {len(base_fids)} FIDs\")\n",
    "            \n",
    "            casts = self.loader.get_dataset('casts', ['fid', 'mentions', 'deleted_at'])\n",
    "            \n",
    "            # Filter by base FIDs first\n",
    "            casts = casts.filter(pl.col('fid').is_in(base_fids))\n",
    "            \n",
    "            # Parse mentions as JSON and handle counts\n",
    "            mention_features = (\n",
    "                casts.filter(pl.col('deleted_at').is_null())\n",
    "                .with_columns([\n",
    "                    # Parse JSON string to array and count elements\n",
    "                    pl.when(\n",
    "                        pl.col('mentions').is_not_null() & \n",
    "                        (pl.col('mentions') != '') & \n",
    "                        (pl.col('mentions') != '[]')\n",
    "                    )\n",
    "                    .then(pl.col('mentions').str.json_decode().list.len())\n",
    "                    .otherwise(0)\n",
    "                    .alias('mention_count'),\n",
    "                    \n",
    "                    # Flag for casts with mentions\n",
    "                    (pl.col('mentions').is_not_null() & \n",
    "                    (pl.col('mentions') != '') & \n",
    "                    (pl.col('mentions') != '[]')\n",
    "                    ).cast(pl.Int32).alias('has_mentions')\n",
    "                ])\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    # Count total casts with mentions\n",
    "                    pl.col('has_mentions').sum().alias('casts_with_mentions'),\n",
    "                    # Total mentions\n",
    "                    pl.col('mention_count').sum().alias('total_mentions'),\n",
    "                    # Average mentions per cast\n",
    "                    pl.col('mention_count').mean().alias('avg_mentions_per_cast')\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            # Join and add ratios\n",
    "            result = df.join(mention_features, on='fid', how='left', coalesce=True).fill_null(0)\n",
    "            \n",
    "            # Add derived metrics\n",
    "            result = result.with_columns([\n",
    "                (pl.col('casts_with_mentions') / (pl.col('cast_count') + 1)).alias('mention_frequency'),\n",
    "                (pl.col('avg_mentions_per_cast') / (pl.col('cast_count') + 1)).alias('mention_ratio')\n",
    "            ])\n",
    "            \n",
    "            print(f\"Mentions features complete. Shape: {result.shape}\")\n",
    "            self.loader.clear_cache()\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in mentions features: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "    def add_reply_patterns_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add reply features with updated functions\"\"\"\n",
    "        casts = self.loader.get_dataset('casts', \n",
    "            ['fid', 'parent_hash', 'parent_fid', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        reply_features = (\n",
    "            casts.filter(pl.col('deleted_at').is_null())\n",
    "            .filter(pl.col('parent_hash').is_not_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.len().alias('total_replies'),\n",
    "                pl.n_unique('parent_fid').alias('unique_users_replied_to'),\n",
    "                pl.col('timestamp').diff().mean().dt.total_seconds()\n",
    "                    .alias('avg_seconds_between_replies'),\n",
    "                pl.col('timestamp').diff().std().dt.total_seconds()\n",
    "                    .alias('std_seconds_between_replies')\n",
    "            ])\n",
    "            .with_columns([\n",
    "                (pl.col('unique_users_replied_to') / pl.col('total_replies'))\n",
    "                    .alias('reply_diversity'),\n",
    "                (pl.col('std_seconds_between_replies') / \n",
    "                pl.col('avg_seconds_between_replies')).alias('reply_timing_variability')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        self.loader.clear_cache()\n",
    "        return df.join(reply_features, on='fid', how='left').fill_null(0)\n",
    "\n",
    "    # def add_cluster_analysis_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "    #     \"\"\"Analyze network clustering with updated functions\"\"\"\n",
    "    #     try:\n",
    "    #         links = self.loader.get_dataset('links', \n",
    "    #             ['fid', 'target_fid', 'deleted_at'])\n",
    "            \n",
    "    #         valid_links = links.filter(pl.col('deleted_at').is_null())\n",
    "            \n",
    "    #         # Calculate clustering features\n",
    "    #         cluster_features = (\n",
    "    #             valid_links.join(\n",
    "    #                 valid_links.rename({'fid': 'mutual_fid', 'target_fid': 'mutual_target'}),\n",
    "    #                 left_on='target_fid',\n",
    "    #                 right_on='mutual_fid'\n",
    "    #             )\n",
    "    #             .group_by('fid')\n",
    "    #             .agg([\n",
    "    #                 pl.n_unique('mutual_target').alias('mutual_connections'),\n",
    "    #                 pl.len().alias('potential_triangles')\n",
    "    #             ])\n",
    "    #             .with_columns([\n",
    "    #                 (pl.col('mutual_connections') / (pl.col('potential_triangles') + 1))\n",
    "    #                 .alias('clustering_coefficient')\n",
    "    #             ])\n",
    "    #         )\n",
    "            \n",
    "    #         self.loader.clear_cache()\n",
    "    #         return df.join(cluster_features, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error in cluster analysis: {str(e)}\")\n",
    "    #         raise\n",
    "    #         return df.with_columns([\n",
    "    #             pl.lit(0).alias('mutual_connections'),\n",
    "    #             pl.lit(0).alias('potential_triangles'),\n",
    "    #             pl.lit(0.0).alias('clustering_coefficient')\n",
    "    #         ])\n",
    "\n",
    "    def add_authenticity_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add authenticity features with comprehensive null handling\"\"\"\n",
    "        try:\n",
    "            print(\"Building authenticity features...\")\n",
    "            \n",
    "            # Initialize with safe default values\n",
    "            result = df.clone()\n",
    "            required_cols = {\n",
    "                'has_bio': 0,\n",
    "                'has_avatar': 0,\n",
    "                'verification_count': 0,\n",
    "                'has_ens': 0,\n",
    "                'following_count': 0.0,\n",
    "                'follower_count': 0.0,\n",
    "                'total_updates': 0,\n",
    "                'avg_update_interval': 0.0,\n",
    "                'profile_update_consistency': 0.0\n",
    "            }\n",
    "            \n",
    "            # Ensure all required columns exist with proper types\n",
    "            for col, default in required_cols.items():\n",
    "                if col not in result.columns:\n",
    "                    print(f\"Adding missing column {col} with default {default}\")\n",
    "                    result = result.with_columns(pl.lit(default).alias(col))\n",
    "                \n",
    "                # Fill nulls with defaults\n",
    "                result = result.with_columns(\n",
    "                    pl.col(col).fill_null(default).alias(col)\n",
    "                )\n",
    "            \n",
    "            result = result.with_columns(\n",
    "                pl.col('total_updates').cast(pl.Float64).fill_null(0),\n",
    "                pl.col('profile_update_consistency').cast(pl.Float64).fill_null(0)\n",
    "            )\n",
    "\n",
    "            # Safe calculations with explicit null handling\n",
    "            result = result.with_columns([\n",
    "                # Profile completeness (0-1) with safe operations\n",
    "                ((pl.col('has_bio').fill_null(0) + \n",
    "                pl.col('has_avatar').fill_null(0) + \n",
    "                pl.col('has_ens').fill_null(0) + \n",
    "                (pl.col('verification_count').fill_null(0) > 0).cast(pl.Int64)) / 4.0\n",
    "                ).alias('profile_completeness'),\n",
    "                \n",
    "                # Network balance (0-1) with safe division\n",
    "                (pl.when(pl.col('following_count').fill_null(0) + pl.col('follower_count').fill_null(0) > 0)\n",
    "                .then(1.0 - (pl.col('following_count').fill_null(0) - pl.col('follower_count').fill_null(0)).abs() /\n",
    "                    (pl.col('following_count').fill_null(0) + pl.col('follower_count').fill_null(0)))\n",
    "                .otherwise(0.0)\n",
    "                ).alias('network_balance'),\n",
    "                \n",
    "                # Update naturalness (0-1) with safe comparisons\n",
    "                    (pl.when(pl.col('total_updates') > 0)\n",
    "                .then(1.0 - pl.col('profile_update_consistency').clip(0.0, 1.0))\n",
    "                .otherwise(0.0))\n",
    "                .alias('update_naturalness')\n",
    "            ])\n",
    "            \n",
    "            # Calculate final authenticity score with weights\n",
    "            result = result.with_columns([\n",
    "                (pl.col('profile_completeness').fill_null(0.0) * 0.4 +\n",
    "                pl.col('network_balance').fill_null(0.0) * 0.3 +\n",
    "                pl.col('update_naturalness').fill_null(0.0) * 0.3\n",
    "                ).alias('authenticity_score')\n",
    "            ])\n",
    "            \n",
    "            print(\"Authenticity features completed successfully\")\n",
    "            return result.drop(['profile_completeness', 'network_balance', 'update_naturalness'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in authenticity features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns(pl.lit(0.0).alias('authenticity_score'))\n",
    "    def add_update_behavior_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add update behavior features with comprehensive null handling\"\"\"\n",
    "        try:\n",
    "            print(\"Building update behavior features...\")\n",
    "            \n",
    "            # Initialize result with default values\n",
    "            result = df.clone().with_columns([\n",
    "                pl.lit(0.0).alias('profile_update_consistency'),\n",
    "                pl.lit(0).alias('total_updates'),\n",
    "                pl.lit(0.0).alias('avg_update_interval'),\n",
    "                pl.lit(0.0).alias('update_time_std')\n",
    "            ])\n",
    "            \n",
    "            # Get user data\n",
    "            user_data = self.loader.get_dataset('user_data', ['fid', 'timestamp', 'deleted_at'])\n",
    "            if user_data is None or len(user_data) == 0:\n",
    "                return result\n",
    "                \n",
    "            # Process updates with strict null handling\n",
    "            valid_updates = (user_data\n",
    "                .filter(pl.col('deleted_at').is_null())\n",
    "                .filter(pl.col('timestamp').is_not_null())\n",
    "                .with_columns([\n",
    "                    pl.col('timestamp').cast(pl.Datetime).alias('timestamp')\n",
    "                ]))\n",
    "            \n",
    "            if len(valid_updates) == 0:\n",
    "                return result\n",
    "                            \n",
    "            update_metrics = (valid_updates\n",
    "                .sort(['fid', 'timestamp'])\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    pl.len().alias('total_updates'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().mean().alias('avg_update_interval'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().std().alias('update_time_std')\n",
    "                ]))\n",
    "\n",
    "            # Ensure all columns are numeric and nulls are handled\n",
    "            update_metrics = update_metrics.with_columns([\n",
    "                pl.col('total_updates').cast(pl.Float64).fill_null(0),\n",
    "                pl.col('avg_update_interval').cast(pl.Float64).fill_null(0),\n",
    "                pl.col('update_time_std').cast(pl.Float64).fill_null(0)\n",
    "            ])\n",
    "\n",
    "            update_metrics = update_metrics.with_columns([\n",
    "                pl.when(pl.col('avg_update_interval') > 0)\n",
    "                .then(pl.col('update_time_std') / pl.col('avg_update_interval'))\n",
    "                .otherwise(0.0)\n",
    "                .alias('profile_update_consistency')\n",
    "            ])\n",
    "\n",
    "            # Join new features safely\n",
    "            update_metrics = update_metrics.unique(subset=['fid']) \n",
    "            result = result.join(update_metrics, on='fid', how='left')\n",
    "            \n",
    "            # Fill any remaining nulls\n",
    "            result = result.with_columns([\n",
    "                pl.col('total_updates').cast(pl.Float64).fill_null(0),\n",
    "                pl.col('avg_update_interval').cast(pl.Float64).fill_null(0),\n",
    "                pl.col('update_time_std').cast(pl.Float64).fill_null(0),\n",
    "                pl.col('profile_update_consistency').cast(pl.Float64).fill_null(0)\n",
    "            ])\n",
    "            \n",
    "            self.loader.clear_cache()\n",
    "            print(\"Update behavior features completed successfully\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in update behavior features: {str(e)}\")\n",
    "            raise\n",
    "            print(f\"Returning dataframe with default values\")\n",
    "            return df.with_columns([\n",
    "                pl.lit(0.0).alias('profile_update_consistency'),\n",
    "                pl.lit(0).alias('total_updates'),\n",
    "                pl.lit(0.0).alias('avg_update_interval'),\n",
    "                pl.lit(0.0).alias('update_time_std')\n",
    "            ])\n",
    "\n",
    "    def add_verification_patterns_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add verification patterns with safe calculations\"\"\"\n",
    "        try:\n",
    "            # Initialize with default columns\n",
    "            result = df.clone()\n",
    "            default_cols = {\n",
    "                'avg_hours_between_verifications': 0.0,\n",
    "                'std_hours_between_verifications': 0.0,\n",
    "                'rapid_verifications': 0,\n",
    "                'avg_hours_between_platform_verifs': 0.0,\n",
    "                'std_hours_between_platform_verifs': 0.0\n",
    "            }\n",
    "            \n",
    "            # Add on-chain verification patterns\n",
    "            verifications = self.loader.get_dataset('verifications', \n",
    "                ['fid', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            if verifications is not None and len(verifications) > 0:\n",
    "                valid_verifs = verifications.filter(pl.col('deleted_at').is_null())\n",
    "                \n",
    "                if len(valid_verifs) > 0:\n",
    "                    verif_patterns = (\n",
    "                        valid_verifs\n",
    "                        .with_columns(pl.col('timestamp').cast(pl.Datetime))\n",
    "                        .group_by('fid')\n",
    "                        .agg([\n",
    "                            pl.col('timestamp').diff().dt.total_hours().mean().fill_null(0)\n",
    "                                .alias('avg_hours_between_verifications'),\n",
    "                            pl.col('timestamp').diff().dt.total_hours().std().fill_null(0)\n",
    "                                .alias('std_hours_between_verifications'),\n",
    "                            (pl.col('timestamp').diff().dt.total_hours() < 1).sum().fill_null(0)\n",
    "                                .alias('rapid_verifications')\n",
    "                        ])\n",
    "                    )\n",
    "                    result = result.join(verif_patterns, on='fid', how='left')\n",
    "            \n",
    "            # Add platform verification patterns\n",
    "            acc_verifications = self.loader.get_dataset('account_verifications', \n",
    "                ['fid', 'verified_at'])\n",
    "            \n",
    "            if acc_verifications is not None and len(acc_verifications) > 0:\n",
    "                platform_patterns = (\n",
    "                    acc_verifications\n",
    "                    .with_columns(pl.col('verified_at').cast(pl.Datetime))\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        pl.col('verified_at').diff().dt.total_hours().mean().fill_null(0)\n",
    "                            .alias('avg_hours_between_platform_verifs'),\n",
    "                        pl.col('verified_at').diff().dt.total_hours().std().fill_null(0)\n",
    "                            .alias('std_hours_between_platform_verifs')\n",
    "                    ])\n",
    "                )\n",
    "                platform_patterns = platform_patterns.unique(subset=['fid']) \n",
    "                result = result.join(platform_patterns, on='fid', how='left')\n",
    "            \n",
    "            # Add any missing columns with defaults\n",
    "            for col, default in default_cols.items():\n",
    "                if col not in result.columns:\n",
    "                    result = result.with_columns(pl.lit(default).alias(col))\n",
    "                else:\n",
    "                    result = result.with_columns(pl.col(col).fill_null(default))\n",
    "            \n",
    "            self.loader.clear_cache()\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in verification patterns: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([pl.lit(v).alias(k) for k, v in default_cols.items()])\n",
    "    def _validate_required_columns(self, df: pl.DataFrame, required_cols: List[str]):\n",
    "        \"\"\"Validate required columns exist\"\"\"\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "            \n",
    "    def _get_feature_build_order(self):\n",
    "        \"\"\"Get correct feature build order based on dependencies\"\"\"\n",
    "        visited = set()\n",
    "        order = []\n",
    "        \n",
    "        def visit(name):\n",
    "            if name in visited:\n",
    "                return\n",
    "            visited.add(name)\n",
    "            feature_set = self.feature_sets[name]\n",
    "            for dep in feature_set.dependencies:\n",
    "                visit(dep)\n",
    "            order.append(name)\n",
    "        \n",
    "        for name in self.feature_sets:\n",
    "            visit(name)\n",
    "        return order\n",
    "\n",
    "\n",
    "    def _validate_feature_addition(self, original_df: pl.DataFrame, \n",
    "                                new_df: pl.DataFrame,\n",
    "                                base_fids: pl.Series,\n",
    "                                feature_name: str) -> pl.DataFrame:\n",
    "        \"\"\"Validate and fix feature addition results\"\"\"\n",
    "        if new_df is None:\n",
    "            print(f\"Error: {feature_name} returned None\")\n",
    "            raise\n",
    "            return original_df\n",
    "            \n",
    "        if len(new_df) != len(original_df):\n",
    "            print(f\"Warning: Shape mismatch in {feature_name}. Expected {len(original_df)}, got {len(new_df)}\")\n",
    "            new_df = new_df.filter(pl.col('fid').is_in(base_fids))\n",
    "            if len(new_df) != len(original_df):\n",
    "                return original_df\n",
    "                \n",
    "        # Cast numeric columns and handle nulls\n",
    "        new_cols = [c for c in new_df.columns if c not in original_df.columns]\n",
    "        if new_cols:\n",
    "            try:\n",
    "                new_df = new_df.with_columns([\n",
    "                    pl.col(c).cast(pl.Float64).fill_null(0) \n",
    "                    for c in new_cols \n",
    "                    if self._is_numeric_dtype(new_df[c].dtype)\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                print(f\"Error casting columns in {feature_name}: {str(e)}\")\n",
    "                raise\n",
    "                return original_df\n",
    "                \n",
    "        return new_df\n",
    "\n",
    "\n",
    "    def add_enhanced_channel_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add channel features with improved error handling\"\"\"\n",
    "        try:\n",
    "            # Prepare result DataFrame with default values\n",
    "            result = df.with_columns([\n",
    "                pl.lit(0).alias('unique_channels_followed'),\n",
    "                pl.lit(0).alias('rapid_channel_follows'),\n",
    "                pl.lit(0.0).alias('channel_follow_hour_std'),\n",
    "                pl.lit(0).alias('channel_memberships'),\n",
    "                pl.lit(0).alias('unique_channel_memberships'),\n",
    "                pl.lit(0.0).alias('channel_follow_burst_ratio'),\n",
    "                pl.lit(0.0).alias('channel_engagement_ratio')\n",
    "            ])\n",
    "            \n",
    "            # Process channel follows if available\n",
    "            channel_follows = self.loader.get_dataset('channel_follows', \n",
    "                ['fid', 'channel_id', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            if channel_follows is not None and len(channel_follows) > 0:\n",
    "                follow_features = (\n",
    "                    channel_follows.filter(pl.col('deleted_at').is_null())\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        pl.n_unique('channel_id').alias('unique_channels_followed'),\n",
    "                        (pl.col('timestamp').diff().dt.total_seconds() < 60)\n",
    "                            .sum().alias('rapid_channel_follows'),\n",
    "                        pl.col('timestamp').dt.hour().value_counts()\n",
    "                            .std().alias('channel_follow_hour_std')\n",
    "                    ])\n",
    "                )\n",
    "                # Join follow features safely\n",
    "                if len(follow_features) > 0:\n",
    "                    follow_features = follow_features.unique(subset=['fid']) \n",
    "                    result = result.join(follow_features, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "            # Process channel memberships if available\n",
    "            channel_members = self.loader.get_dataset('channel_members', \n",
    "                ['fid', 'channel_id', 'deleted_at'])\n",
    "            \n",
    "            if channel_members is not None and len(channel_members) > 0:\n",
    "                member_features = (\n",
    "                    channel_members.filter(pl.col('deleted_at').is_null())\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        pl.len().alias('channel_memberships'),\n",
    "                        pl.n_unique('channel_id').alias('unique_channel_memberships')\n",
    "                    ])\n",
    "                )\n",
    "                # Join member features safely\n",
    "                if len(member_features) > 0:\n",
    "                    member_features = member_features.unique(subset=['fid']) \n",
    "                    result = result.join(member_features, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "            # Calculate derived metrics safely\n",
    "            result = result.with_columns([\n",
    "                (pl.col('rapid_channel_follows') / pl.col('unique_channels_followed').add(1))\n",
    "                    .alias('channel_follow_burst_ratio'),\n",
    "                (pl.col('channel_memberships') / pl.col('unique_channel_memberships').add(1))\n",
    "                    .alias('channel_engagement_ratio')\n",
    "            ])\n",
    "            \n",
    "            self.loader.clear_cache()\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in channel features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0).alias('unique_channels_followed'),\n",
    "                pl.lit(0).alias('rapid_channel_follows'),\n",
    "                pl.lit(0.0).alias('channel_follow_hour_std'),\n",
    "                pl.lit(0).alias('channel_memberships'),\n",
    "                pl.lit(0).alias('unique_channel_memberships'),\n",
    "                pl.lit(0.0).alias('channel_follow_burst_ratio'),\n",
    "                pl.lit(0.0).alias('channel_engagement_ratio')\n",
    "            ])\n",
    "\n",
    "    def add_engagement_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add engagement features with improved dependency handling\"\"\"\n",
    "        try:\n",
    "            print(\"Processing engagement features...\")\n",
    "            \n",
    "            # Initialize required columns with defaults\n",
    "            required_cols = {\n",
    "                'cast_count': 0,\n",
    "                'total_reactions': 0,\n",
    "                'channel_memberships': 0\n",
    "            }\n",
    "            \n",
    "            # Ensure base columns exist\n",
    "            result = df.clone()\n",
    "            for col, default in required_cols.items():\n",
    "                if col not in result.columns:\n",
    "                    result = result.with_columns(pl.lit(default).alias(col))\n",
    "                else:\n",
    "                    result = result.with_columns(pl.col(col).fill_null(default))\n",
    "            \n",
    "            # Calculate engagement metrics safely\n",
    "            result = result.with_columns([\n",
    "                # Overall engagement score\n",
    "                ((pl.col('cast_count') + \n",
    "                pl.col('total_reactions') + \n",
    "                pl.col('channel_memberships')) / 3.0\n",
    "                ).alias('engagement_score'),\n",
    "                \n",
    "                # Activity balance\n",
    "                (pl.col('cast_count') / pl.col('total_reactions').add(1))\n",
    "                    .alias('creation_consumption_ratio')\n",
    "            ])\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in engagement features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0.0).alias('engagement_score'),\n",
    "                pl.lit(0.0).alias('creation_consumption_ratio')\n",
    "            ])\n",
    "    def _validate_and_ensure_features(self, df: pl.DataFrame, \n",
    "                                required_features: Dict[str, float]) -> pl.DataFrame:\n",
    "        \"\"\"Enhanced feature validation with null handling\"\"\"\n",
    "        result = df.clone()\n",
    "        \n",
    "        for feature, default_value in required_features.items():\n",
    "            if feature not in result.columns:\n",
    "                print(f\"Adding missing feature {feature} with default value {default_value}\")\n",
    "                result = result.with_columns(pl.lit(default_value).alias(feature))\n",
    "            else:\n",
    "                result = result.with_columns(\n",
    "                    pl.when(pl.col(feature).is_null())\n",
    "                    .then(pl.lit(default_value))\n",
    "                    .otherwise(pl.col(feature))\n",
    "                    .alias(feature)\n",
    "                )\n",
    "        \n",
    "        return result\n",
    "    def _load_checkpoint(self, feature_set: FeatureSet, base_fids: pl.Series) -> pl.DataFrame:\n",
    "        \"\"\"Enhanced checkpoint loading with proper list type handling\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading checkpoint: {feature_set.checkpoint_path}\")\n",
    "            checkpoint_df = pl.read_parquet(feature_set.checkpoint_path)\n",
    "            checkpoint_df = checkpoint_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "\n",
    "            # Handle each column based on its type\n",
    "            for col in checkpoint_df.columns:\n",
    "                if col == 'fid':\n",
    "                    continue\n",
    "                    \n",
    "                dtype_str = str(checkpoint_df[col].dtype).lower()\n",
    "                \n",
    "                # Skip list type columns\n",
    "                if 'list' in dtype_str:\n",
    "                    continue\n",
    "                    \n",
    "                # Handle numeric columns\n",
    "                if any(num_type in dtype_str for num_type in ['int', 'float', 'decimal']):\n",
    "                    checkpoint_df = checkpoint_df.with_columns([\n",
    "                        pl.col(col).cast(pl.Float64).fill_null(0)\n",
    "                    ])\n",
    "\n",
    "            # Debug info\n",
    "            print(f\"Checkpoint fid type: {checkpoint_df['fid'].dtype}\")\n",
    "            print(f\"Base fids type: {base_fids.dtype}\")\n",
    "            \n",
    "            # Force consistent FID types\n",
    "            checkpoint_df = checkpoint_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "            base_fids = base_fids.cast(pl.Int64)\n",
    "            \n",
    "            # Filter to base_fids\n",
    "            filtered_df = checkpoint_df.filter(pl.col('fid').is_in(base_fids))\n",
    "            print(f\"Filtered checkpoint from {len(checkpoint_df)} to {len(filtered_df)} rows\")\n",
    "            \n",
    "            # Special handling for certain feature sets\n",
    "            if feature_set.name in ['authenticity', 'update_behavior']:\n",
    "                filtered_df = self._validate_sensitive_checkpoint(filtered_df, feature_set.name)\n",
    "                \n",
    "            return filtered_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _validate_checkpoint_compatibility(self, checkpoint_df: pl.DataFrame, \n",
    "                                        base_fids: pl.Series) -> bool:\n",
    "        \"\"\"Validate checkpoint compatibility with list type handling\"\"\"\n",
    "        try:\n",
    "            if checkpoint_df is None or len(checkpoint_df) == 0:\n",
    "                return False\n",
    "                \n",
    "            # Verify FID column exists and is correct type\n",
    "            if 'fid' not in checkpoint_df.columns:\n",
    "                return False\n",
    "                \n",
    "            checkpoint_fids = checkpoint_df['fid'].cast(pl.Int64)\n",
    "            base_fids = base_fids.cast(pl.Int64)\n",
    "            \n",
    "            # Verify all base FIDs are present\n",
    "            missing_fids = pl.Series(np.setdiff1d(base_fids, checkpoint_fids))\n",
    "            if len(missing_fids) > 0:\n",
    "                print(f\"Missing FIDs in checkpoint: {missing_fids}\")\n",
    "                return False\n",
    "                \n",
    "            # Verify column types\n",
    "            for col in checkpoint_df.columns:\n",
    "                if col == 'fid':\n",
    "                    continue\n",
    "                    \n",
    "                dtype_str = str(checkpoint_df[col].dtype).lower()\n",
    "                # Skip validation for list type columns\n",
    "                if 'list' in dtype_str:\n",
    "                    continue\n",
    "                    \n",
    "                # Validate numeric columns\n",
    "                if any(num_type in dtype_str for num_type in ['int', 'float', 'decimal']):\n",
    "                    try:\n",
    "                        # Test if we can cast to Float64\n",
    "                        checkpoint_df.select(pl.col(col).cast(pl.Float64))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Column {col} failed type validation: {str(e)}\")\n",
    "                        return False\n",
    "                        \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error validating checkpoint compatibility: {str(e)}\")\n",
    "            return False\n",
    "    def add_nindexer_enhanced_network_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add enhanced network features with scalar aggregations\"\"\"\n",
    "        try:\n",
    "            follows = self.loader.get_dataset('follows', \n",
    "                ['fid', 'target_fid', 'timestamp', 'created_at', 'deleted_at'], \n",
    "                source=\"nindexer\")\n",
    "            follow_counts = self.loader.get_dataset('follow_counts',\n",
    "                ['fid', 'follower_count', 'following_count', 'created_at'], \n",
    "                source=\"nindexer\")\n",
    "            \n",
    "            if follows is not None and len(follows) > 0:\n",
    "                valid_follows = follows.filter(pl.col('deleted_at').is_null())\n",
    "                \n",
    "                follow_metrics = (valid_follows\n",
    "                    .with_columns([\n",
    "                        pl.col('timestamp').cast(pl.Datetime),\n",
    "                        pl.col('created_at').cast(pl.Datetime)\n",
    "                    ])\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        (pl.col('timestamp').max() - pl.col('timestamp').min())\n",
    "                            .dt.total_hours()\n",
    "                            .cast(pl.Float64)\n",
    "                            .alias('network_age_hours'),\n",
    "                        pl.len().alias('total_follows'),\n",
    "                        (pl.col('created_at') - pl.col('timestamp'))\n",
    "                            .dt.total_seconds()\n",
    "                            .mean()\n",
    "                            .cast(pl.Float64)\n",
    "                            .alias('avg_follow_latency_seconds')\n",
    "                    ])\n",
    "                    .with_columns([\n",
    "                        (pl.col('total_follows') / \n",
    "                        (pl.col('network_age_hours') + 1))\n",
    "                        .alias('follow_rate_per_hour')\n",
    "                    ]))\n",
    "                \n",
    "                if follow_counts is not None and len(follow_counts) > 0:\n",
    "                    count_metrics = (follow_counts\n",
    "                        .sort('created_at')  # Sort to ensure last() gets most recent\n",
    "                        .group_by('fid')\n",
    "                        .agg([\n",
    "                            pl.col('follower_count')\n",
    "                                .last()\n",
    "                                .cast(pl.Float64)\n",
    "                                .alias('latest_follower_count'),\n",
    "                            pl.col('following_count')\n",
    "                                .last()\n",
    "                                .cast(pl.Float64)\n",
    "                                .alias('latest_following_count')\n",
    "                        ])\n",
    "                        .with_columns([\n",
    "                            (pl.col('latest_follower_count') / \n",
    "                            (pl.col('latest_following_count') + 1))\n",
    "                            .alias('latest_follow_ratio')\n",
    "                        ]))\n",
    "                    \n",
    "                    result = df.join(follow_metrics, on='fid', how='left')\n",
    "                    result = result.join(count_metrics, on='fid', how='left')\n",
    "                else:\n",
    "                    result = df.join(follow_metrics, on='fid', how='left')\n",
    "                \n",
    "                return result.fill_null(0)\n",
    "                \n",
    "            return df.with_columns([\n",
    "                pl.lit(0.0).alias('network_age_hours'),\n",
    "                pl.lit(0.0).alias('total_follows'),\n",
    "                pl.lit(0.0).alias('follow_rate_per_hour'),\n",
    "                pl.lit(0.0).alias('avg_follow_latency_seconds'),\n",
    "                pl.lit(0.0).alias('latest_follower_count'),\n",
    "                pl.lit(0.0).alias('latest_following_count'),\n",
    "                pl.lit(0.0).alias('latest_follow_ratio')\n",
    "            ])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in enhanced network features: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def add_nindexer_enhanced_profile_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add enhanced profile features with scalar aggregations\"\"\"\n",
    "        try:\n",
    "            profiles = self.loader.get_dataset('profiles', \n",
    "                ['fid', 'bio', 'pfp_url', 'url', 'username', \n",
    "                'location', 'created_at', 'updated_at'], \n",
    "                source=\"nindexer\")\n",
    "            \n",
    "            if profiles is not None and len(profiles) > 0:\n",
    "                profile_metrics = (profiles\n",
    "                    .with_columns([\n",
    "                        pl.col('created_at').cast(pl.Datetime),\n",
    "                        pl.col('updated_at').cast(pl.Datetime),\n",
    "                        pl.col('url').is_not_null().cast(pl.Int32).alias('has_url'),\n",
    "                        pl.col('location').is_not_null().cast(pl.Int32).alias('has_location')\n",
    "                    ])\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        # Ensure scalar sum\n",
    "                        (pl.col('has_url') + pl.col('has_location'))\n",
    "                            .cast(pl.Float64)\n",
    "                            .alias('additional_profile_fields'),\n",
    "                        (pl.col('updated_at').max() - pl.col('created_at').min())\n",
    "                            .dt.total_hours()\n",
    "                            .cast(pl.Float64)\n",
    "                            .alias('profile_age_hours'),\n",
    "                        pl.col('location')\n",
    "                            .first()\n",
    "                            .alias('location')\n",
    "                    ]))\n",
    "                \n",
    "                result = df.join(profile_metrics, on='fid', how='left')\n",
    "                \n",
    "                if result.select(pl.col('location').is_not_null().sum()).item() > 0:\n",
    "                    result = result.with_columns([\n",
    "                        pl.col('location')\n",
    "                            .is_not_null()\n",
    "                            .cast(pl.Int32)\n",
    "                            .alias('has_location_info')\n",
    "                    ])\n",
    "                \n",
    "                return result.fill_null(0)\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in enhanced profile features: {str(e)}\")\n",
    "            raise\n",
    "    def add_nindexer_neynar_score_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add Neynar score features and correlations with proper type handling\"\"\"\n",
    "        try:\n",
    "            scores = self.loader.get_dataset('neynar_user_scores', \n",
    "                ['fid', 'score', 'created_at'], source=\"nindexer\")\n",
    "            \n",
    "            if scores is not None and len(scores) > 0:\n",
    "                # Get latest scores per user and ensure we're dealing with scalar values\n",
    "                score_features = (scores\n",
    "                    .with_columns([\n",
    "                        pl.col('created_at').cast(pl.Datetime),\n",
    "                        # Ensure score is handled as a scalar\n",
    "                        pl.when(pl.col('score').is_null())\n",
    "                        .then(0.0)\n",
    "                        .otherwise(pl.col('score'))\n",
    "                        .alias('score')\n",
    "                    ])\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        # Latest score\n",
    "                        pl.col('score').last().alias('neynar_score'),\n",
    "                        # Average score over time\n",
    "                        pl.col('score').mean().alias('avg_neynar_score'),\n",
    "                        # Score stability\n",
    "                        pl.col('score').std().alias('neynar_score_std'),\n",
    "                        # Score trend (positive or negative)\n",
    "                        (pl.col('score').last() - pl.col('score').first()).alias('score_trend')\n",
    "                    ]))\n",
    "                \n",
    "                result = df.join(score_features, on='fid', how='left')\n",
    "                \n",
    "                # Calculate correlation with authenticity score if it exists\n",
    "                if 'authenticity_score' in result.columns:\n",
    "                    result = result.with_columns([\n",
    "                        # Safely calculate score difference\n",
    "                        (pl.col('neynar_score').cast(pl.Float64) - \n",
    "                        pl.col('authenticity_score').cast(pl.Float64))\n",
    "                        .abs()\n",
    "                        .alias('score_divergence'),\n",
    "                        \n",
    "                        # Calculate relative score difference\n",
    "                        ((pl.col('neynar_score').cast(pl.Float64) - \n",
    "                        pl.col('authenticity_score').cast(pl.Float64)) /\n",
    "                        (pl.col('authenticity_score').cast(pl.Float64) + 1e-6))\n",
    "                        .alias('relative_score_diff')\n",
    "                    ])\n",
    "                \n",
    "                # Fill any remaining nulls with 0\n",
    "                result = result.with_columns([\n",
    "                    pl.col('neynar_score').fill_null(0.0),\n",
    "                    pl.col('avg_neynar_score').fill_null(0.0),\n",
    "                    pl.col('neynar_score_std').fill_null(0.0),\n",
    "                    pl.col('score_trend').fill_null(0.0)\n",
    "                ])\n",
    "                \n",
    "                if 'score_divergence' in result.columns:\n",
    "                    result = result.with_columns([\n",
    "                        pl.col('score_divergence').fill_null(0.0),\n",
    "                        pl.col('relative_score_diff').fill_null(0.0)\n",
    "                    ])\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in neynar score features: {str(e)}\")\n",
    "            # Return original dataframe with default columns if error occurs\n",
    "            return df.with_columns([\n",
    "                pl.lit(0.0).alias('neynar_score'),\n",
    "                pl.lit(0.0).alias('avg_neynar_score'),\n",
    "                pl.lit(0.0).alias('neynar_score_std'),\n",
    "                pl.lit(0.0).alias('score_trend')\n",
    "            ])\n",
    "    def add_name_pattern_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add name pattern features with improved error handling\"\"\"\n",
    "        try:\n",
    "            print(\"Building name pattern features...\")\n",
    "            \n",
    "            # Initialize with default values\n",
    "            result = df.clone().with_columns([\n",
    "                pl.lit(0).alias('random_numbers'),\n",
    "                pl.lit(0).alias('wallet_pattern'),\n",
    "                pl.lit(0).alias('excessive_symbols'),\n",
    "                pl.lit(0).alias('airdrop_terms'),\n",
    "                pl.lit(0).alias('has_year')\n",
    "            ])\n",
    "\n",
    "            result = result.with_columns([\n",
    "                    pl.col('fname').map_elements(self._analyze_name_patterns, return_dtype=pl.Utf8).alias('fname_content_patterns'),\n",
    "                    pl.col('bio').map_elements(self._analyze_name_patterns, return_dtype=pl.Utf8).alias('bio_content_patterns'),\n",
    "                ])\n",
    "\n",
    "            result = result.with_columns([\n",
    "                    (pl.col('fname_content_patterns').map_elements(lambda x: x['fname_random_numbers'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('fname_random_numbers'),\n",
    "                    (pl.col('fname_content_patterns').map_elements(lambda x: x['fname_wallet_pattern'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('fname_wallet_pattern'),\n",
    "                    (pl.col('fname_content_patterns').map_elements(lambda x: x['fname_excessive_symbols'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('fname_excessive_symbols'),\n",
    "                    (pl.col('fname_content_patterns').map_elements(lambda x: x['fname_airdrop_terms'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('fname_airdrop_terms'),\n",
    "                    (pl.col('fname_content_patterns').map_elements(lambda x: x['fname_has_year'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('fname_has_year'),\n",
    "                    (pl.col('bio_content_patterns').map_elements(lambda x: x['bio_random_numbers'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('bio_random_numbers'),\n",
    "                    (pl.col('bio_content_patterns').map_elements(lambda x: x['bio_wallet_pattern'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('bio_wallet_pattern'),\n",
    "                    (pl.col('bio_content_patterns').map_elements(lambda x: x['bio_excessive_symbols'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('bio_excessive_symbols'),\n",
    "                    (pl.col('bio_content_patterns').map_elements(lambda x: x['bio_airdrop_terms'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('bio_airdrop_terms'),\n",
    "                    (pl.col('bio_content_patterns').map_elements(lambda x: x['bio_has_year'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('bio_has_year'),\n",
    "            ])\n",
    "\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in name pattern features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0).alias('random_numbers'),\n",
    "                pl.lit(0).alias('wallet_pattern'),\n",
    "                pl.lit(0).alias('excessive_symbols'),\n",
    "                pl.lit(0).alias('airdrop_terms'),\n",
    "                pl.lit(0).alias('has_year')\n",
    "            ])\n",
    "\n",
    "    def build_feature_matrix(self) -> pl.DataFrame:\n",
    "        \"\"\"Build feature matrix with enhanced safety checks while maintaining existing functionality\"\"\"\n",
    "        print(\"Starting feature extraction...\")\n",
    "        \n",
    "        try:\n",
    "            # Load or build profile features\n",
    "            if self._needs_rebuild(self.feature_sets['profile']):\n",
    "                print(\"Building profile features...\")\n",
    "                df = self.extract_profile_features()\n",
    "            else:\n",
    "                print(\"Loading profile features from checkpoint...\")\n",
    "                df = pl.read_parquet(self.feature_sets['profile'].checkpoint_path)\n",
    "            \n",
    "            # Setup base configuration\n",
    "            df = df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "            base_fids = df['fid'].cast(pl.Int64).unique()\n",
    "            df = df.filter(pl.col('fid').is_in(base_fids))\n",
    "            self.loader.set_base_fids(base_fids)\n",
    "            initial_cols = df.columns\n",
    "            print(f\"Base shape: {df.shape}\")\n",
    "\n",
    "            # Define dependencies between features\n",
    "            dependencies = {\n",
    "                'engagement': ['cast', 'reaction', 'channel'],\n",
    "                'network_quality': ['network', 'engagement'],\n",
    "                'activity_patterns': ['temporal', 'cast', 'reaction'],\n",
    "                'mentions': ['cast'],\n",
    "                'reply_patterns': ['cast'],\n",
    "                'update_behavior': ['user_data'],\n",
    "                'verification_patterns': ['verification'],\n",
    "                'authenticity': ['profile', 'network', 'verification', 'engagement']\n",
    "            }\n",
    "\n",
    "            # Track successfully built features\n",
    "            built_features = {'profile'}\n",
    "            \n",
    "            feature_sequence = [\n",
    "                ('network', self.add_network_features),\n",
    "                ('temporal', self.add_temporal_features),\n",
    "                ('cast', self.add_cast_behavior_features),\n",
    "                ('reaction', self.add_reaction_patterns),\n",
    "                ('channel', self.add_enhanced_channel_features),\n",
    "                ('user_data', self.add_user_data_features),\n",
    "                ('verification', self.add_enhanced_verification_features),\n",
    "                ('engagement', self.add_engagement_features),\n",
    "                ('network_quality', self.build_network_quality_features),\n",
    "                ('activity_patterns', self.add_activity_patterns_features),\n",
    "                ('influence', self.add_influence_features),\n",
    "                ('mentions', self.add_mentions_features),\n",
    "                ('reply_patterns', self.add_reply_patterns_features),\n",
    "                ('power_user_interaction', self.add_power_user_interaction_features),\n",
    "                # ('cluster_analysis', self.add_cluster_analysis_features),\n",
    "                ('update_behavior', self.add_update_behavior_features),\n",
    "                ('verification_patterns', self.add_verification_patterns_features),\n",
    "                ('authenticity', self.add_authenticity_features),\n",
    "                ('storage', self.add_storage_features),\n",
    "                ('derived', self._add_derived_features),\n",
    "                ('enhanced_network', self.add_nindexer_enhanced_network_features),\n",
    "                ('enhanced_profile', self.add_nindexer_enhanced_profile_features),\n",
    "                ('neynar_score', self.add_nindexer_neynar_score_features),\n",
    "                ('name_patterns', self.add_name_pattern_features),\n",
    "                # ('content_patterns', self.add_cast_behavior_features),  # Modified version\n",
    "                # ('advanced_temporal', self.add_advanced_temporal_features),\n",
    "                # ('reward_gaming', self.add_reward_gaming_features),\n",
    "                # ('engagement_authenticity', self.add_engagement_authenticity_features)\n",
    "            ]\n",
    "\n",
    "            for feature_name, feature_func in feature_sequence:\n",
    "                feature_set = self.feature_sets[feature_name]\n",
    "                current_cols = set(df.columns)\n",
    "                \n",
    "                try:\n",
    "                    # Check if dependencies are met\n",
    "                    should_rebuild = self._needs_rebuild(feature_set)\n",
    "                    if feature_name in dependencies:\n",
    "                        deps = dependencies[feature_name]\n",
    "                        missing_deps = [dep for dep in deps if dep not in built_features]\n",
    "                        if missing_deps:\n",
    "                            print(f\"Missing dependencies for {feature_name}: {missing_deps}\")\n",
    "                            print(f\"Currently built features: {built_features}\")\n",
    "                            should_rebuild = True\n",
    "\n",
    "                    if should_rebuild:\n",
    "                        print(f\"Building {feature_name} features...\")\n",
    "                        new_df = feature_func(df)\n",
    "                        \n",
    "                        if new_df is not None:\n",
    "                            # Validate and safely join new features\n",
    "                            new_df = self._validate_checkpoint(new_df, feature_name)\n",
    "                            new_df = new_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "                            \n",
    "                            # Only save and update if validation passes\n",
    "                            if self._validate_checkpoint_compatibility(new_df, base_fids):\n",
    "                                self._save_checkpoint(new_df, feature_set)\n",
    "                                df = self._safe_join_features(df, new_df, feature_name)\n",
    "                                built_features.add(feature_name)\n",
    "                                print(f\"Successfully built and saved {feature_name}\")\n",
    "                    else:\n",
    "                        print(f\"Loading {feature_name} features from checkpoint...\")\n",
    "                        checkpoint_df = self._load_checkpoint(feature_set, base_fids)\n",
    "                        \n",
    "                        if checkpoint_df is not None:\n",
    "                            new_cols = [c for c in checkpoint_df.columns if c not in current_cols]\n",
    "                            if new_cols:\n",
    "                                print(f\"Adding {len(new_cols)} new columns from {feature_name}\")\n",
    "                                # Use safe join for checkpoint data too\n",
    "                                df = self._safe_join_features(\n",
    "                                    df,\n",
    "                                    checkpoint_df.select(['fid'] + new_cols),\n",
    "                                    feature_name\n",
    "                                )\n",
    "                                built_features.add(feature_name)\n",
    "                        else:\n",
    "                            print(f\"Failed to load {feature_name} checkpoint, forcing rebuild...\")\n",
    "                            new_df = feature_func(df)\n",
    "                            if new_df is not None:\n",
    "                                new_df = self._validate_checkpoint(new_df, feature_name)\n",
    "                                new_df = new_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "                                self._save_checkpoint(new_df, feature_set)\n",
    "                                df = self._safe_join_features(df, new_df, feature_name)\n",
    "                                built_features.add(feature_name)\n",
    "                    \n",
    "                    print(f\"Shape after {feature_name}: {df.shape}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in {feature_name}: {str(e)}\")\n",
    "                    raise\n",
    "                    continue\n",
    "\n",
    "            # Final validation\n",
    "            df = df.fill_null(0)\n",
    "            df = df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "            \n",
    "            # self.verify_matrix(df)\n",
    "\n",
    "\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Critical error: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def _validate_checkpoint(self, df: pl.DataFrame, name: str) -> pl.DataFrame:\n",
    "        \"\"\"Validate checkpoint data types and ensure type consistency with list handling\"\"\"\n",
    "        try:\n",
    "            # Always ensure fid is Int64 first\n",
    "            if 'fid' in df.columns:\n",
    "                df = df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "                \n",
    "            # Cast numeric columns and handle nulls, excluding list types\n",
    "            numeric_cols = []\n",
    "            for col in df.columns:\n",
    "                if col != 'fid':\n",
    "                    dtype_str = str(df[col].dtype).lower()\n",
    "                    # Check if it's a list type\n",
    "                    if 'list' in dtype_str:\n",
    "                        continue\n",
    "                    # Check if it's a numeric type\n",
    "                    if any(num_type in dtype_str for num_type in ['int', 'float', 'decimal']):\n",
    "                        numeric_cols.append(col)\n",
    "            \n",
    "            if numeric_cols:\n",
    "                df = df.with_columns([\n",
    "                    pl.col(col).cast(pl.Float64).fill_null(0) \n",
    "                    for col in numeric_cols\n",
    "                ])\n",
    "            \n",
    "            return df\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error validating checkpoint {name}: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "    def _is_numeric_dtype(self, dtype) -> bool:\n",
    "        \"\"\"Check if a Polars dtype is numeric, excluding list types\"\"\"\n",
    "        # Convert dtype to string for comparison\n",
    "        dtype_str = str(dtype).lower()\n",
    "        # Exclude list types\n",
    "        if 'list' in dtype_str:\n",
    "            return False\n",
    "        return any(num_type in dtype_str \n",
    "                for num_type in ['int', 'float', 'decimal'])\n",
    "\n",
    "    def _safe_join_features(self, df: pl.DataFrame, \n",
    "                        new_features: pl.DataFrame,\n",
    "                        feature_name: str) -> pl.DataFrame:\n",
    "        \"\"\"Enhanced safe join features with comprehensive null and list handling\"\"\"\n",
    "        try:\n",
    "            if new_features is None or len(new_features) == 0:\n",
    "                print(f\"No valid features to join for {feature_name}\")\n",
    "                return df\n",
    "\n",
    "            # Get new columns\n",
    "            existing_cols = set(df.columns)\n",
    "            new_cols = [c for c in new_features.columns \n",
    "                    if c != 'fid' and c not in existing_cols]\n",
    "                    \n",
    "            if not new_cols:\n",
    "                print(f\"No new columns to add from {feature_name}\")\n",
    "                return df\n",
    "                \n",
    "            # Handle nulls in new features before join\n",
    "            safe_features = new_features.clone()\n",
    "            for col in new_cols:\n",
    "                dtype_str = str(new_features[col].dtype).lower()\n",
    "                if 'list' in dtype_str:\n",
    "                    # For list columns, replace null with empty list\n",
    "                    safe_features = safe_features.with_columns(\n",
    "                        pl.col(col).fill_null([])\n",
    "                    )\n",
    "                elif self._is_numeric_dtype(new_features[col].dtype):\n",
    "                    # For numeric columns, fill null with 0\n",
    "                    safe_features = safe_features.with_columns(\n",
    "                        pl.col(col).fill_null(0.0)\n",
    "                    )\n",
    "            \n",
    "            # Join with guaranteed FID type consistency\n",
    "            safe_features = safe_features.unique(subset=['fid']) \n",
    "            result = df.join(\n",
    "                safe_features.select(['fid'] + new_cols)\n",
    "                .with_columns(pl.col('fid').cast(pl.Int64)),\n",
    "                on='fid',\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # Handle any new nulls that appeared after join\n",
    "            for col in new_cols:\n",
    "                dtype_str = str(result[col].dtype).lower()\n",
    "                if 'list' in dtype_str:\n",
    "                    result = result.with_columns(\n",
    "                        pl.col(col).fill_null([])\n",
    "                    )\n",
    "                elif self._is_numeric_dtype(result[col].dtype):\n",
    "                    result = result.with_columns(\n",
    "                        pl.col(col).fill_null(0.0)\n",
    "                    )\n",
    "                            \n",
    "            return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error joining {feature_name}: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "    def _validate_feature_dependencies(self, feature_name: str, \n",
    "                                built_features: set) -> bool:\n",
    "        \"\"\"Validate feature dependencies are met\"\"\"\n",
    "        if feature_name not in self.feature_sets:\n",
    "            return False\n",
    "            \n",
    "        feature_set = self.feature_sets[feature_name]\n",
    "        for dep in feature_set.dependencies:\n",
    "            if dep not in built_features:\n",
    "                print(f\"Missing dependency {dep} for {feature_name}\")\n",
    "                return False\n",
    "                \n",
    "        return True\n",
    "    def _save_checkpoint(self, df: pl.DataFrame, feature_set: 'FeatureSet'):\n",
    "        \"\"\"Save feature checkpoint with validation\"\"\"\n",
    "        # Validate before saving\n",
    "        df = self._validate_checkpoint(df, feature_set.name)\n",
    "        df.write_parquet(feature_set.checkpoint_path)\n",
    "        feature_set.last_modified = os.path.getmtime(feature_set.checkpoint_path)\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_sensitive_checkpoint(self, df: pl.DataFrame, feature_name: str) -> pl.DataFrame:\n",
    "        \"\"\"Additional validation for sensitive features\"\"\"\n",
    "        try:\n",
    "            # Initialize sensitive columns with safe defaults\n",
    "            sensitive_defaults = {\n",
    "                'authenticity': {\n",
    "                    'authenticity_score': 0.0,\n",
    "                    'profile_completeness': 0.0,\n",
    "                    'network_balance': 0.0,\n",
    "                    'update_naturalness': 0.0\n",
    "                },\n",
    "                'update_behavior': {\n",
    "                    'profile_update_consistency': 0.0,\n",
    "                    'total_updates': 0,\n",
    "                    'avg_update_interval': 0.0,\n",
    "                    'update_time_std': 0.0\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            if feature_name in sensitive_defaults:\n",
    "                for col, default in sensitive_defaults[feature_name].items():\n",
    "                    if col in df.columns:\n",
    "                        df = df.with_columns(pl.col(col).fill_null(default))\n",
    "                    else:\n",
    "                        df = df.with_columns(pl.lit(default).alias(col))\n",
    "                        \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error validating sensitive checkpoint {feature_name}: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "    def _add_derived_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add derived features with comprehensive null handling\"\"\"\n",
    "        try:\n",
    "            print(\"Building derived features...\")\n",
    "            result = df.clone()\n",
    "            \n",
    "            # Ensure required columns exist\n",
    "            required_cols = {\n",
    "                'following_count': 0.0,\n",
    "                'follower_count': 0.0,\n",
    "                'follower_ratio': 0.0,\n",
    "                'unique_follower_ratio': 0.0,\n",
    "                'follow_velocity': 0.0\n",
    "            }\n",
    "            \n",
    "            # Initialize missing columns\n",
    "            for col, default in required_cols.items():\n",
    "                if col not in result.columns:\n",
    "                    print(f\"Adding missing column {col} with default {default}\")\n",
    "                    result = result.with_columns(pl.lit(default).alias(col))\n",
    "                \n",
    "                # Fill nulls with defaults\n",
    "                result = result.with_columns(\n",
    "                    pl.col(col).fill_null(default).alias(col)\n",
    "                )\n",
    "            \n",
    "            # Safe calculations with explicit null handling\n",
    "            result = result.with_columns([\n",
    "                # Log transformations with null safety\n",
    "                pl.col('follower_ratio').fill_null(0.0).log1p().alias('follower_ratio_log'),\n",
    "                pl.col('unique_follower_ratio').fill_null(0.0).log1p().alias('unique_follower_ratio_log'),\n",
    "                pl.col('follow_velocity').fill_null(0.0).log1p().alias('follow_velocity_log'),\n",
    "                \n",
    "                # Binary flags with safe comparisons\n",
    "                (pl.when(pl.col('follower_count').fill_null(0) > pl.col('following_count').fill_null(0))\n",
    "                .then(1)\n",
    "                .otherwise(0)\n",
    "                ).alias('has_more_followers'),\n",
    "                \n",
    "                # Balance ratios with safe division\n",
    "                ((pl.col('following_count').fill_null(0) - pl.col('follower_count').fill_null(0)).abs() / \n",
    "                (pl.col('following_count').fill_null(0) + pl.col('follower_count').fill_null(0) + 1)\n",
    "                ).alias('follow_balance_ratio')\n",
    "            ])\n",
    "            \n",
    "            # Cap extreme values with safe operations\n",
    "            for col in ['follower_ratio', 'unique_follower_ratio', 'follow_velocity']:\n",
    "                if col in result.columns:\n",
    "                    safe_col = pl.col(col).fill_null(0.0)\n",
    "                    p99 = result.select(safe_col.quantile(0.99)).item()\n",
    "                    result = result.with_columns([\n",
    "                        safe_col.clip(0.0, p99).alias(f'{col}_capped')\n",
    "                    ])\n",
    "            \n",
    "            print(\"Derived features completed successfully\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in derived features: {str(e)}\")\n",
    "            raise\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook code\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score,\n",
    "    precision_recall_curve, precision_score, recall_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import polars as pl\n",
    "\n",
    "class SybilDetectionSystem:\n",
    "    def __init__(self, \n",
    "                 feature_engineering: 'FeatureEngineering',\n",
    "                 confidence_thresholds: Dict[str, float] = None,\n",
    "                 authenticity_thresholds: Dict[str, float] = None):\n",
    "        self.feature_engineering = feature_engineering\n",
    "        self.model = None\n",
    "        self.feature_names = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.confidence_thresholds = confidence_thresholds or {\n",
    "            'high': 0.95,\n",
    "            'medium': 0.85,\n",
    "            'low': 0.70\n",
    "        }\n",
    "        self.authenticity_thresholds = authenticity_thresholds or {\n",
    "            'high': 0.8,\n",
    "            'medium': 0.6,\n",
    "            'low': 0.4\n",
    "        }\n",
    "        self.feature_importance = {}\n",
    "        self.shap_values = {}\n",
    "        self.base_models = {}\n",
    "        self.shap_explainers = {}\n",
    "            \n",
    "    def prepare_features(self, df: pl.DataFrame, scale: bool = False) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"Prepare features with comprehensive feature selection and validation\"\"\"\n",
    "        try:\n",
    "            # Define feature groups\n",
    "\n",
    "            valid_cols = [col for col in df.columns if \n",
    "                        df[col].dtype in [pl.Float64, pl.Int64] or\n",
    "                        str(df[col].dtype).startswith(('Float', 'Int'))]\n",
    "            \n",
    "            print(f\"\\nTotal numeric features available: {len(valid_cols)}\")\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            features = df.select(valid_cols).fill_null(0)\n",
    "            for col in valid_cols:\n",
    "                col_dtype = str(features[col].dtype)\n",
    "                \n",
    "                if col_dtype.startswith('list') or col_dtype.startswith('List'):\n",
    "                    print(f\"Converting list column {col} to length feature\")\n",
    "                    features = features.with_columns([\n",
    "                        pl.when(pl.col(col).is_null())\n",
    "                        .then(0)\n",
    "                        .otherwise(pl.col(col).list.len())\n",
    "                        .alias(col)\n",
    "                    ])\n",
    "            \n",
    "            # Handle infinite values and extreme outliers\n",
    "            for col in valid_cols:\n",
    "                col_stats = features.select(\n",
    "                    pl.col(col).quantile(0.01).alias('q01'),\n",
    "                    pl.col(col).quantile(0.99).alias('q99'),\n",
    "                    pl.col(col).mean().alias('mean'),\n",
    "                    pl.col(col).std().alias('std')\n",
    "                )\n",
    "                \n",
    "                q01 = col_stats['q01'][0]\n",
    "                q99 = col_stats['q99'][0]\n",
    "                mean_val = col_stats['mean'][0]\n",
    "                std_val = col_stats['std'][0]\n",
    "                \n",
    "                # Define reasonable bounds for the column\n",
    "                lower_bound = max(q01, mean_val - 3 * std_val)\n",
    "                upper_bound = min(q99, mean_val + 3 * std_val)\n",
    "                \n",
    "                # Clip values to bounds and replace infinities\n",
    "                features = features.with_columns([\n",
    "                    pl.when(pl.col(col).is_infinite())\n",
    "                    .then(pl.lit(None))\n",
    "                    .otherwise(pl.col(col))\n",
    "                    .alias(col)\n",
    "                ])\n",
    "                \n",
    "                features = features.with_columns([\n",
    "                    pl.col(col).clip(lower_bound, upper_bound).alias(col)\n",
    "                ])\n",
    "                \n",
    "                # Fill remaining nulls with median\n",
    "                median_val = features.select(pl.col(col).median())[0][0]\n",
    "                features = features.with_columns([\n",
    "                    pl.col(col).fill_null(median_val).alias(col)\n",
    "                ])\n",
    "                \n",
    "                # Convert to numeric if needed\n",
    "                if features[col].dtype not in [pl.Float64, pl.Int64]:\n",
    "                    features = features.with_columns([\n",
    "                        pl.col(col).cast(pl.Float64).alias(col)\n",
    "                    ])\n",
    "\n",
    "            # Convert to numpy array\n",
    "            feature_array = features.to_numpy()\n",
    "            \n",
    "            if scale:\n",
    "                feature_array = self.scaler.fit_transform(feature_array)\n",
    "\n",
    "            print(f\"\\nFinal feature matrix shape: {feature_array.shape}\")\n",
    "            print(f\"Using {len(valid_cols)} features\")\n",
    "            \n",
    "            # Verify no infinite values remain\n",
    "            if np.any(np.isinf(feature_array)):\n",
    "                raise ValueError(\"Infinite values still present after preprocessing\")\n",
    "\n",
    "            return feature_array, valid_cols\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing features: {str(e)}\")\n",
    "            print(f\"Available columns: {df.columns}\")\n",
    "            raise\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray, feature_names: List[str]):\n",
    "        \"\"\"Train the model with stacking and SHAP explanations\"\"\"\n",
    "        self.feature_names = feature_names\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Define base models\n",
    "        base_model_configs = {\n",
    "            'xgb': xgb.XGBClassifier(eval_metric='auc', random_state=42),\n",
    "            'rf': RandomForestClassifier(n_jobs=-1, random_state=42, class_weight='balanced'),\n",
    "            'lgbm': LGBMClassifier(n_jobs=-1, random_state=42, class_weight='balanced')\n",
    "        }\n",
    "        \n",
    "        # Train and calibrate base models\n",
    "        for name, model in base_model_configs.items():\n",
    "            print(f\"\\nStarting training for {name}...\")\n",
    "            study = optuna.create_study(direction='maximize', study_name=f'optuna_{name}')\n",
    "            \n",
    "            def objective(trial):\n",
    "                params = self.get_hyperparameters(name, trial)\n",
    "                model.set_params(**params)\n",
    "                cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='average_precision')\n",
    "                return cv_scores.mean()\n",
    "            \n",
    "            study.optimize(objective, n_trials=50, timeout=600)\n",
    "            print(f\"Best parameters for {name}: {study.best_params}\")\n",
    "            print(f\"Best CV score for {name}: {study.best_value}\")\n",
    "            \n",
    "            # Train base model\n",
    "            best_model = type(model)(**study.best_params)\n",
    "            best_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Store SHAP explainer\n",
    "            try:\n",
    "                explainer = shap.TreeExplainer(best_model)\n",
    "                self.shap_explainers[name] = explainer\n",
    "                print(f\"SHAP explainer created for {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating SHAP explainer for {name}: {str(e)}\")\n",
    "                self.shap_explainers[name] = None\n",
    "            \n",
    "            # Calibrate model\n",
    "            print(f\"Calibrating {name}...\")\n",
    "            calibrated_model = CalibratedClassifierCV(best_model, cv=5)\n",
    "            calibrated_model.fit(X_train, y_train)\n",
    "            self.base_models[name] = calibrated_model\n",
    "        \n",
    "        # Build stacked model\n",
    "        print(\"\\nBuilding stacked model...\")\n",
    "        self.build_stacked_model(X_train, y_train)\n",
    "        \n",
    "        # Create final ensemble\n",
    "        print(\"\\nCreating ensemble...\")\n",
    "        self.model = VotingClassifier(\n",
    "            estimators=[\n",
    "                (name, model) for name, model in self.base_models.items()\n",
    "            ] + [('meta_learner', self.meta_learner)],\n",
    "            voting='soft',\n",
    "            weights=[0.25, 0.25, 0.25, 0.25]\n",
    "        )\n",
    "        self.model.fit(X_train, y_train)\n",
    "        print(\"Ensemble training complete\")\n",
    "\n",
    "        # Get stability metrics\n",
    "        stability_results = detector.add_cross_validation_stability(X_train, y_train)\n",
    "        print(\"\\nCross-validation Stability Metrics:\")\n",
    "        print(f\"Mean prediction variance: {stability_results['mean_prediction_variance']:.4f}\")\n",
    "        print(f\"Max prediction variance: {stability_results['max_prediction_variance']:.4f}\")\n",
    "        print(f\"Mean prediction range: {stability_results['mean_prediction_range']:.4f}\")\n",
    "        print(f\"Percentage of stable predictions: {stability_results['stable_prediction_percentage']:.2%}\")        \n",
    "\n",
    "        # Split some validation data\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Optimize ensemble weights\n",
    "        weights = self.optimize_ensemble_weights(X_val, y_val)\n",
    "        print(\"Optimized model weights:\", weights)\n",
    "        \n",
    "        # Final evaluation\n",
    "        final_predictions, unstable_indices = self.predict_with_stability(X_test)\n",
    "        print(f\"Number of unstable predictions: {len(unstable_indices)}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def build_stacked_model(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Build stacked model using base model predictions\"\"\"\n",
    "        base_preds = np.zeros((len(self.base_models), len(X)))\n",
    "        for i, (name, model) in enumerate(self.base_models.items()):\n",
    "            base_preds[i] = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        meta_features = np.column_stack([base_preds.T, X])\n",
    "        meta_learner = LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=3,\n",
    "            num_leaves=8,\n",
    "            feature_fraction=0.8,\n",
    "            bagging_fraction=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        meta_learner.fit(meta_features, y)\n",
    "        self.meta_learner = meta_learner\n",
    "\n",
    "    def get_feature_explanations(self, model_name: str, X: np.ndarray, instance_index: int) -> Dict:\n",
    "        \"\"\"Get SHAP explanations for a specific instance\"\"\"\n",
    "        try:\n",
    "            if instance_index < 0 or instance_index >= X.shape[0]:\n",
    "                print(f\"Invalid instance index: {instance_index}\")\n",
    "                return {}\n",
    "                \n",
    "            if model_name not in self.shap_explainers or self.shap_explainers[model_name] is None:\n",
    "                print(f\"No SHAP explainer available for {model_name}\")\n",
    "                return {}\n",
    "            \n",
    "            explainer = self.shap_explainers[model_name]\n",
    "            shap_values = explainer.shap_values(X[instance_index:instance_index+1])\n",
    "            \n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[1] if len(shap_values) > 1 else shap_values[0]\n",
    "            \n",
    "            shap_instance = shap_values[0]\n",
    "            top_indices = np.argsort(np.abs(shap_instance))[-5:][::-1]\n",
    "            \n",
    "            return {\n",
    "                self.feature_names[i]: float(shap_instance[i])\n",
    "                for i in top_indices\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting SHAP explanations: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _calculate_feature_importance(self):\n",
    "        \"\"\"Calculate and store aggregated feature importance from base models\"\"\"\n",
    "        try:\n",
    "            for name, model in self.base_models.items():\n",
    "                # Access base estimator within CalibratedClassifierCV\n",
    "                if isinstance(model, CalibratedClassifierCV):\n",
    "                    # Try to access base_estimator_ (scikit-learn >=0.24)\n",
    "                    if hasattr(model, 'base_estimator_') and model.base_estimator_ is not None:\n",
    "                        base_estimator = model.base_estimator_\n",
    "                    # For older scikit-learn versions\n",
    "                    elif hasattr(model, 'base_estimator') and model.base_estimator is not None:\n",
    "                        base_estimator = model.base_estimator\n",
    "                    else:\n",
    "                        print(f\"Model {name} does not have a base estimator.\")\n",
    "                        continue\n",
    "                else:\n",
    "                    base_estimator = model\n",
    "\n",
    "                # Retrieve feature importances\n",
    "                if hasattr(base_estimator, 'feature_importances_'):\n",
    "                    importances = base_estimator.feature_importances_\n",
    "                    for feat, imp in zip(self.feature_names, importances):\n",
    "                        self.feature_importance[feat] = self.feature_importance.get(feat, 0) + imp\n",
    "                else:\n",
    "                    print(f\"No feature_importances_ attribute for model {name}.\")\n",
    "\n",
    "            # Average importances across models\n",
    "            num_models = len(self.base_models)\n",
    "            if num_models > 0:\n",
    "                self.feature_importance = {k: v / num_models for k, v in self.feature_importance.items()}\n",
    "                print(\"Feature importance calculated.\")\n",
    "            else:\n",
    "                print(\"No models available to calculate feature importance.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating feature importance: {str(e)}\")\n",
    "    def select_important_features(self, X: np.ndarray, y: np.ndarray, feature_names: List[str], \n",
    "                                threshold: float = 0.01) -> List[str]:\n",
    "        \"\"\"Select features based on SHAP importance\"\"\"\n",
    "        model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "        importance_vals = np.abs(shap_values).mean(0)\n",
    "        \n",
    "        importance = dict(zip(feature_names, importance_vals))\n",
    "        selected_features = [f for f, imp in importance.items() \n",
    "                            if imp > threshold * np.max(importance_vals)]\n",
    "        \n",
    "        print(f\"\\nSelected {len(selected_features)}/{len(feature_names)} features\")\n",
    "        print(\"Top 10 features:\", sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "        return selected_features\n",
    "\n",
    "    def get_hyperparameters(self, model_name: str, trial: optuna.Trial) -> Dict:\n",
    "        \"\"\"Get optimized hyperparameters with regularization\"\"\"\n",
    "        if model_name == 'xgb':\n",
    "            return {\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1, 10),\n",
    "                'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 10.0),\n",
    "                'gamma': trial.suggest_float('gamma', 0, 5)\n",
    "            }\n",
    "        elif model_name == 'lgbm':\n",
    "            return {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),\n",
    "                'lambda_l1': trial.suggest_float('lambda_l1', 0, 10),\n",
    "                'lambda_l2': trial.suggest_float('lambda_l2', 0, 10)\n",
    "            }\n",
    "        else:  # RandomForest\n",
    "            return {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "            }\n",
    "\n",
    "    def analyze_feature_interactions(self, X: np.ndarray, top_k: int = 10) -> List[Tuple[str, str, float]]:\n",
    "        \"\"\"Analyze most important feature interactions\"\"\"\n",
    "        model = self.base_models['xgb']\n",
    "        if hasattr(model, 'base_estimator_'):\n",
    "            model = model.base_estimator_\n",
    "        \n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_interaction_values = explainer.shap_interaction_values(X)\n",
    "        \n",
    "        # Calculate interaction strengths\n",
    "        n_features = len(self.feature_names)\n",
    "        interactions = []\n",
    "        for i in range(n_features):\n",
    "            for j in range(i+1, n_features):\n",
    "                strength = np.abs(shap_interaction_values[:, i, j]).mean()\n",
    "                interactions.append((\n",
    "                    self.feature_names[i],\n",
    "                    self.feature_names[j],\n",
    "                    float(strength)\n",
    "                ))\n",
    "        \n",
    "        # Return top K interactions\n",
    "        return sorted(interactions, key=lambda x: x[2], reverse=True)[:top_k]\n",
    "\n",
    "    def build_stacked_model(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Build a stacked model with meta-learner\"\"\"\n",
    "        # Create base predictions\n",
    "        base_preds = np.zeros((len(self.base_models), len(X)))\n",
    "        for i, (name, model) in enumerate(self.base_models.items()):\n",
    "            base_preds[i] = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Train meta-learner\n",
    "        meta_features = np.column_stack([\n",
    "            base_preds.T,  # Base predictions\n",
    "            X  # Original features\n",
    "        ])\n",
    "        \n",
    "        meta_learner = LGBMClassifier(\n",
    "            n_estimators=100,   \n",
    "            learning_rate=0.01,\n",
    "            max_depth=3,\n",
    "            num_leaves=8,\n",
    "            feature_fraction=0.8,\n",
    "            bagging_fraction=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        meta_learner.fit(meta_features, y)\n",
    "        self.meta_learner = meta_learner\n",
    "    def predict_with_uncertainty(self, features: np.ndarray, \n",
    "                                 authenticity_features: np.ndarray) -> List[Dict]:\n",
    "        \"\"\"Enhanced predictions with uncertainty estimation\"\"\"\n",
    "        # Get predictions from all models\n",
    "        predictions = []\n",
    "        for name, model in self.base_models.items():\n",
    "            pred_proba = model.predict_proba(features)[:, 1]\n",
    "            predictions.append(pred_proba)\n",
    "        \n",
    "        # Calculate ensemble statistics\n",
    "        predictions = np.array(predictions)\n",
    "        mean_probs = predictions.mean(axis=0)\n",
    "        std_probs = predictions.std(axis=0)\n",
    "        \n",
    "        # Calculate prediction intervals\n",
    "        confidence_interval = stats.norm.interval(0.95, loc=mean_probs, scale=std_probs)\n",
    "        \n",
    "        results = []\n",
    "        for i, (prob, std, auth_scores) in enumerate(zip(mean_probs, std_probs, authenticity_features)):\n",
    "            authenticity_score = np.mean([\n",
    "                auth_scores[0],  # authenticity_score\n",
    "                auth_scores[1],  # engagement_quality\n",
    "                auth_scores[2],  # natural_behavior_score\n",
    "                auth_scores[3]   # account_stability\n",
    "            ])\n",
    "            \n",
    "            # Enhanced confidence assessment\n",
    "            model_uncertainty = std / prob if prob > 0 else std\n",
    "            confidence = self._assess_confidence(prob, authenticity_score, model_uncertainty)\n",
    "            \n",
    "            results.append({\n",
    "                'is_bot': prob >= 0.5,\n",
    "                'is_authentic': authenticity_score >= self.authenticity_thresholds['medium'],\n",
    "                'bot_probability': float(prob),\n",
    "                'authenticity_score': float(authenticity_score),\n",
    "                'confidence': confidence,\n",
    "                'uncertainty': float(std),\n",
    "                'prediction_interval': (float(confidence_interval[0][i]), \n",
    "                                         float(confidence_interval[1][i]))\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _assess_confidence(self, prob: float, authenticity: float, \n",
    "                          uncertainty: float) -> str:\n",
    "        \"\"\"Enhanced confidence assessment with uncertainty consideration\"\"\"\n",
    "        # Adjust thresholds based on uncertainty\n",
    "        uncertainty_penalty = uncertainty * 2\n",
    "        \n",
    "        if prob <= 0.1 and authenticity >= self.authenticity_thresholds['high'] and uncertainty < 0.1:\n",
    "            return 'high_authentic'\n",
    "        elif prob <= 0.2 and authenticity >= self.authenticity_thresholds['medium'] and uncertainty < 0.15:\n",
    "            return 'medium_authentic'\n",
    "        elif prob >= (self.confidence_thresholds['high'] + uncertainty_penalty):\n",
    "            return 'high_bot'\n",
    "        elif prob >= (self.confidence_thresholds['medium'] + uncertainty_penalty):\n",
    "            return 'medium_bot'\n",
    "        else:\n",
    "            return 'uncertain'\n",
    "    def get_feature_explanations(self, model_name: str, X: np.ndarray, instance_index: int) -> Dict:\n",
    "        \"\"\"Get SHAP explanations for a specific instance using the underlying base model\"\"\"\n",
    "        try:\n",
    "            # Index check\n",
    "            if instance_index < 0 or instance_index >= X.shape[0]:\n",
    "                print(f\"Error: instance_index {instance_index} is out of bounds for test set with size {X.shape[0]}.\")\n",
    "                return {}\n",
    "            \n",
    "            if model_name not in self.base_models:\n",
    "                print(f\"No model available for {model_name}.\")\n",
    "                return {}\n",
    "            \n",
    "            model = self.base_models[model_name]\n",
    "            \n",
    "            # Get the underlying base model from the CalibratedClassifierCV\n",
    "            if isinstance(model, CalibratedClassifierCV):\n",
    "                # Access the first calibrated classifier's base estimator\n",
    "                base_model = model.calibrated_classifiers_[0].base_estimator\n",
    "                print(f\"Using base estimator from calibrated classifier for {model_name}\")\n",
    "            else:\n",
    "                base_model = model\n",
    "                print(f\"Using model directly for {model_name}\")\n",
    "                \n",
    "            try:\n",
    "                print(f\"Creating SHAP explainer for model type: {type(base_model)}\")\n",
    "                explainer = shap.TreeExplainer(base_model)\n",
    "                \n",
    "                # Use small subset of data for explanation\n",
    "                instance_data = X[instance_index:instance_index+1]\n",
    "                print(f\"Calculating SHAP values for instance shape: {instance_data.shape}\")\n",
    "                \n",
    "                shap_vals = explainer.shap_values(instance_data)\n",
    "                \n",
    "                # Handle different SHAP value formats\n",
    "                if isinstance(shap_vals, list):\n",
    "                    if len(shap_vals) > 1:\n",
    "                        shap_instance = shap_vals[1][0]  # For binary classification\n",
    "                    else:\n",
    "                        shap_instance = shap_vals[0][0]\n",
    "                else:\n",
    "                    shap_instance = shap_vals[0]\n",
    "                \n",
    "                # Get top feature contributions\n",
    "                top_indices = np.argsort(np.abs(shap_instance))[-5:][::-1]\n",
    "                explanations = {\n",
    "                    self.feature_names[i]: float(shap_instance[i]) \n",
    "                    for i in top_indices\n",
    "                }\n",
    "                \n",
    "                return explanations\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating SHAP values for {model_name}: {str(e)}\")\n",
    "                print(f\"Model type: {type(base_model)}\")\n",
    "                return {}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting feature explanations: {str(e)}\")\n",
    "            return {}\n",
    "    def add_cross_validation_stability(self, X: np.ndarray, y: np.ndarray, n_splits: int = 5) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Measure prediction stability across different CV folds.\n",
    "        Returns metrics about how consistent predictions are across folds.\n",
    "        \"\"\"\n",
    "        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Initialize array to store all predictions for each sample\n",
    "        all_predictions = np.zeros((len(X), n_splits))\n",
    "        all_predictions[:] = np.nan  # Fill with NaN to track which predictions we get\n",
    "        \n",
    "        # Get predictions from each fold\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Train model on this fold\n",
    "            self.model.fit(X_train, y_train)\n",
    "            fold_proba = self.model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            # Store predictions in the right spots\n",
    "            all_predictions[val_idx, fold_idx] = fold_proba\n",
    "        \n",
    "        # Calculate variance for each sample (ignoring NaN values)\n",
    "        sample_variances = np.nanvar(all_predictions, axis=1)\n",
    "        sample_ranges = np.nanmax(all_predictions, axis=1) - np.nanmin(all_predictions, axis=1)\n",
    "        \n",
    "        # Compute stability metrics\n",
    "        stability_metrics = {\n",
    "            'mean_prediction_variance': np.mean(sample_variances),\n",
    "            'max_prediction_variance': np.max(sample_variances),\n",
    "            'mean_prediction_range': np.mean(sample_ranges),\n",
    "            'max_prediction_range': np.max(sample_ranges),\n",
    "            'stable_prediction_percentage': np.mean(sample_variances < 0.1)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nPrediction matrix shape: {all_predictions.shape}\")\n",
    "        print(f\"Number of samples with predictions: {np.sum(~np.isnan(all_predictions.mean(axis=1)))}\")\n",
    "        print(f\"Average predictions per sample: {np.mean(~np.isnan(all_predictions)):.2f}\")\n",
    "        \n",
    "        return stability_metrics    \n",
    "    def optimize_ensemble_weights(self, X: np.ndarray, y: np.ndarray) -> List[float]:\n",
    "        \"\"\"Optimize ensemble weights based on individual model performance\"\"\"\n",
    "        try:\n",
    "            # Get individual model performances\n",
    "            model_scores = {}\n",
    "            \n",
    "            # Score base models\n",
    "            for name, model in self.base_models.items():\n",
    "                score = roc_auc_score(y, model.predict_proba(X)[:, 1])\n",
    "                model_scores[name] = score\n",
    "                print(f\"{name} ROC AUC: {score:.4f}\")\n",
    "            \n",
    "            # Score meta learner on combined predictions\n",
    "            base_preds = np.zeros((len(self.base_models), len(X)))\n",
    "            for i, (name, model) in enumerate(self.base_models.items()):\n",
    "                base_preds[i] = model.predict_proba(X)[:, 1]\n",
    "            \n",
    "            meta_features = np.column_stack([base_preds.T, X])\n",
    "            meta_score = roc_auc_score(y, self.meta_learner.predict_proba(meta_features)[:, 1])\n",
    "            model_scores['meta_learner'] = meta_score\n",
    "            print(f\"Meta learner ROC AUC: {meta_score:.4f}\")\n",
    "            \n",
    "            # Calculate weights based on relative performance\n",
    "            total_score = sum(model_scores.values())\n",
    "            weights = [score/total_score for score in model_scores.values()]\n",
    "            \n",
    "            # Update ensemble with new weights\n",
    "            self.model = VotingClassifier(\n",
    "                estimators=[\n",
    "                    (name, model) for name, model in self.base_models.items()\n",
    "                ] + [('meta_learner', self.meta_learner)],\n",
    "                voting='soft',\n",
    "                weights=weights\n",
    "            )\n",
    "            self.model.fit(X, y)  # Refit with new weights\n",
    "            \n",
    "            return weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error optimizing weights: {str(e)}\")\n",
    "            return [0.25, 0.25, 0.25, 0.25]  # Default weights\n",
    "\n",
    "    def predict_with_stability(self, X: np.ndarray) -> Tuple[np.ndarray, List[int]]:\n",
    "        \"\"\"Make predictions with stability assessment\"\"\"\n",
    "        try:\n",
    "            # Get predictions from base models\n",
    "            base_predictions = np.zeros((len(self.base_models), len(X)))\n",
    "            for i, (name, model) in enumerate(self.base_models.items()):\n",
    "                base_predictions[i] = model.predict_proba(X)[:, 1]\n",
    "            \n",
    "            # Calculate prediction statistics\n",
    "            mean_predictions = np.mean(base_predictions, axis=0)\n",
    "            std_predictions = np.std(base_predictions, axis=0)\n",
    "            \n",
    "            # Identify unstable predictions (high variance between models)\n",
    "            unstable_indices = np.where(std_predictions > 0.2)[0]\n",
    "            \n",
    "            # Get ensemble predictions\n",
    "            predictions = self.model.predict_proba(X)\n",
    "            \n",
    "            # Adjust confidence for unstable predictions\n",
    "            confidence_adjustments = 1 - np.clip(std_predictions, 0, 0.5)\n",
    "            adjusted_predictions = predictions * confidence_adjustments.reshape(-1, 1)\n",
    "            \n",
    "            return adjusted_predictions, unstable_indices.tolist()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction with stability: {str(e)}\")\n",
    "            return self.model.predict_proba(X), []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# former version\n",
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Dict, List, Tuple\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score,\n",
    "    precision_recall_curve, precision_score, recall_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Configure Polars for memory usage\n",
    "pl.Config.set_streaming_chunk_size(1_000_000)\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "\n",
    "class LazyDatasetLoader:\n",
    "    \"\"\"Memory-efficient dataset loader with checkpoint-based FID filtering\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, checkpoint_dir: str, debug_mode: bool = True, sample_size: int = 700_000, fids_to_ensure: List[int] = None):\n",
    "        self.data_path = data_path\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self._cached_dataset = None\n",
    "        self._cached_name = None\n",
    "        self.debug_mode = debug_mode\n",
    "        self.sample_size = sample_size\n",
    "        self.base_fids = None\n",
    "        self.fids_to_ensure = fids_to_ensure\n",
    "\n",
    "\n",
    "    def set_base_fids(self, fids):\n",
    "        \"\"\"Set base FIDs to maintain consistent filtering\"\"\"\n",
    "        self.base_fids = fids\n",
    "        print(f\"Set base FIDs: {len(fids)} records\")\n",
    "        \n",
    "        \n",
    "    def get_checkpoint_fids(self):\n",
    "        \"\"\"Get base FIDs from profile checkpoint if it exists\"\"\"\n",
    "        profile_checkpoint = f\"{self.checkpoint_dir}/profile_features.parquet\"\n",
    "        if os.path.exists(profile_checkpoint):\n",
    "            df = pl.read_parquet(profile_checkpoint)\n",
    "            if 'fid' in df.columns:\n",
    "                self.base_fids = df['fid']\n",
    "                print(f\"Loaded base FIDs from checkpoint: {len(self.base_fids)} records\")\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def get_dataset(self, name: str, columns: List[str] = None, source=\"farcaster\") -> pl.DataFrame:\n",
    "        \"\"\"Get dataset with checkpoint-based FID filtering\"\"\"\n",
    "        if self._cached_dataset is not None:\n",
    "            self._cached_dataset = None\n",
    "            \n",
    "        if source == \"farcaster\":\n",
    "            path = f\"{self.data_path}/farcaster-{name}-0-1733162400.parquet\"\n",
    "        elif source == \"nindexer\":\n",
    "            path = f\"{self.data_path}/nindexer-{name}-0-1733508243.parquet\"\n",
    "        try:\n",
    "            scan_query = pl.scan_parquet(path)\n",
    "            if columns:\n",
    "                scan_query = scan_query.select(columns)\n",
    "                \n",
    "            if self.debug_mode:\n",
    "                if self.base_fids is None:\n",
    "                    # Try to get FIDs from checkpoint first\n",
    "                    if not self.get_checkpoint_fids():\n",
    "                        if name == 'profile_with_addresses':\n",
    "                            self._cached_dataset = scan_query.limit(self.sample_size).collect()\n",
    "                            dataset_with_fids = scan_query.filter(pl.col('fid').is_in(self.fids_to_ensure)).collect()\n",
    "                            if len(dataset_with_fids) > 0:\n",
    "                                self._cached_dataset = pl.concat([self._cached_dataset, dataset_with_fids], how='diagonal').unique(subset='fid')\n",
    "\n",
    "                            self.base_fids = self._cached_dataset['fid']\n",
    "                            print(f\"Established new base FIDs from {name}: {len(self.base_fids)} records\")\n",
    "                        else:\n",
    "                            print(f\"Warning: No base FIDs available for {name}\")\n",
    "                            self._cached_dataset = scan_query.limit(self.sample_size).collect()\n",
    "                else:\n",
    "                    print(f\"Filtering {name} by {len(self.base_fids)} base FIDs\")\n",
    "                    self._cached_dataset = (scan_query\n",
    "                        .filter(pl.col('fid').is_in(self.base_fids))\n",
    "                        .collect())\n",
    "            else:\n",
    "                self._cached_dataset = scan_query.collect()\n",
    "                    \n",
    "            print(f\"Loaded {name}: {len(self._cached_dataset)} records\")\n",
    "            return self._cached_dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {name}: {str(e)}\")\n",
    "            raise\n",
    "            return pl.DataFrame()\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the cached dataset\"\"\"\n",
    "        self._cached_dataset = None\n",
    "        self._cached_name = None        \n",
    "\n",
    "class FeatureSet:\n",
    "    \"\"\"Track feature dependencies and versioning\"\"\"\n",
    "    def __init__(self, name: str, version: str, dependencies: List[str] = None):\n",
    "        self.name = name\n",
    "        self.version = version  # Version of feature calculation logic\n",
    "        self.dependencies = dependencies or []\n",
    "        self.checkpoint_path = None\n",
    "        self.last_modified = None\n",
    "\n",
    "class FeatureEngineering:\n",
    "    \"\"\"Enhanced bot detection system\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, checkpoint_dir: str, fids_to_ensure: List[int] = None):\n",
    "        self.data_path = data_path\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.loader = LazyDatasetLoader(data_path, checkpoint_dir, fids_to_ensure=fids_to_ensure)\n",
    "        self.fids_to_ensure = fids_to_ensure\n",
    "        \n",
    "        # Define comprehensive feature dependencies and versions\n",
    "        self.feature_sets = {\n",
    "            # Base features\n",
    "            'profile': FeatureSet('profile', '1.0'),\n",
    "            'network': FeatureSet('network', '1.0'),\n",
    "            'temporal': FeatureSet('temporal', '1.0', ['network']),\n",
    "            \n",
    "            # Activity features\n",
    "            'cast': FeatureSet('cast', '1.0'),\n",
    "            'reaction': FeatureSet('reaction', '1.0'),\n",
    "            'channel': FeatureSet('channel', '1.0'),\n",
    "            'verification': FeatureSet('verification', '1.0'),\n",
    "            \n",
    "            # Account features\n",
    "            'user_data': FeatureSet('user_data', '1.0'),\n",
    "            'storage': FeatureSet('storage', '1.0'),\n",
    "            'signers': FeatureSet('signers', '1.0'),\n",
    "            \n",
    "            # Interaction patterns\n",
    "            'engagement': FeatureSet('engagement', '1.0', \n",
    "                ['cast', 'reaction', 'channel']),\n",
    "            'mentions': FeatureSet('mentions', '1.0', \n",
    "                ['cast', 'network']),\n",
    "            'reply_patterns': FeatureSet('reply_patterns', '1.0', \n",
    "                ['cast', 'temporal']),\n",
    "            \n",
    "            # Network quality\n",
    "            'network_quality': FeatureSet('network_quality', '1.0', \n",
    "                ['network', 'engagement']),\n",
    "            'power_user_interaction': FeatureSet('power_user_interaction', '1.0', \n",
    "                ['network', 'temporal']),\n",
    "            'cluster_analysis': FeatureSet('cluster_analysis', '1.0', \n",
    "                ['network', 'engagement']),\n",
    "            \n",
    "            # Behavioral patterns\n",
    "            'activity_patterns': FeatureSet('activity_patterns', '1.0', \n",
    "                ['temporal', 'cast', 'reaction']),\n",
    "            'update_behavior': FeatureSet('update_behavior', '1.0', \n",
    "                ['user_data', 'profile']),\n",
    "            'verification_patterns': FeatureSet('verification_patterns', '1.0', \n",
    "                ['verification', 'temporal']),\n",
    "            \n",
    "            # Meta features\n",
    "            'authenticity': FeatureSet('authenticity', '2.0', [\n",
    "                'profile', 'network', 'channel', 'verification',\n",
    "                'engagement', 'network_quality', 'activity_patterns'\n",
    "            ]),\n",
    "            'influence': FeatureSet('influence', '1.0', [\n",
    "                'network', 'engagement', 'power_user_interaction'\n",
    "            ]),\n",
    "            \n",
    "            # Final derived features\n",
    "            'derived': FeatureSet('derived', '2.0', [\n",
    "                'network', 'temporal', 'authenticity',\n",
    "                'engagement', 'network_quality', 'influence'\n",
    "            ]),\n",
    "\n",
    "            # nindexer features\n",
    "            'enhanced_network': FeatureSet('enhanced_network', '1.0', \n",
    "                ['network']),\n",
    "            'enhanced_profile': FeatureSet('enhanced_profile', '1.0', \n",
    "                ['profile']),\n",
    "            'neynar_score': FeatureSet('neynar_score', '1.0'),\n",
    "\n",
    "            'name_patterns': FeatureSet('name_patterns', '1.0', ['profile']),\n",
    "            'content_patterns': FeatureSet('content_patterns', '1.0', ['cast']),\n",
    "            'advanced_temporal': FeatureSet('advanced_temporal', '1.0', ['temporal', 'cast', 'reaction']),\n",
    "            'reward_gaming': FeatureSet('reward_gaming', '1.0', ['cast', 'reaction', 'temporal']),\n",
    "            'engagement_authenticity': FeatureSet('engagement_authenticity', '1.0', ['network', 'cast', 'reaction'])\n",
    "\n",
    "        }\n",
    "        \n",
    "        # Initialize checkpoint tracking\n",
    "        self._init_checkpoints()\n",
    "\n",
    "    def _analyze_name_patterns(self, text: str) -> Dict[str, int]:\n",
    "        \"\"\"Analyze username/display name patterns\"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'random_numbers': 0,\n",
    "                'wallet_pattern': 0,\n",
    "                'excessive_symbols': 0,\n",
    "                'airdrop_terms': 0,\n",
    "                'has_year': 0\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'random_numbers': int(bool(re.findall(r'\\d{4,}', text))),\n",
    "            'wallet_pattern': int(bool(re.findall(r'0x[a-fA-F0-9]{40}', text))),\n",
    "            'excessive_symbols': int(bool(re.findall(r'[_.\\-]{2,}', text))),\n",
    "            'airdrop_terms': int(any(term in text.lower() for term in ['airdrop', 'farm', 'degen', 'wojak'])),\n",
    "            'has_year': int(bool(re.findall(r'20[12]\\d', text)))\n",
    "        }\n",
    "\n",
    "    def _analyze_content_patterns(self, text: str) -> Dict[str, int]:\n",
    "        \"\"\"Analyze content for spam/bot patterns\"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'template_structure': 0,\n",
    "                'multiple_cta': 0,\n",
    "                'urgency_terms': 0,\n",
    "                'excessive_emojis': 0,\n",
    "                'price_mentions': 0\n",
    "            }\n",
    "        \n",
    "        text = text.lower()\n",
    "        return {\n",
    "            'template_structure': int(bool(re.findall(r'\\[.*?\\]|\\{.*?\\}|\\<.*?\\>', text))),\n",
    "            'multiple_cta': int(len(re.findall(r'click|join|follow|claim|grab', text)) > 2),\n",
    "            'urgency_terms': int(bool(re.findall(r'hurry|limited|fast|quick|soon|ending', text))),\n",
    "            'excessive_emojis': int(len(re.findall(r'[\\U0001F300-\\U0001F9FF]', text)) > 5),\n",
    "            'price_mentions': int(bool(re.findall(r'\\$\\d+|\\d+\\$', text))),\n",
    "            'excessive_symbols': int(bool(re.findall(r'[_.\\-]{2,}', text))),\n",
    "            'airdrop_terms': int(any(term in text.lower() for term in ['airdrop', 'farm', 'degen', 'wojak'])),\n",
    "        }\n",
    "        \n",
    "    def validate_dimensions(func):\n",
    "        \"\"\"Decorator to validate DataFrame dimensions\"\"\"\n",
    "        def wrapper(self, df: pl.DataFrame, *args, **kwargs):\n",
    "            input_shape = len(df)\n",
    "            try:\n",
    "                result = func(self, df, *args, **kwargs)\n",
    "                if len(result) != input_shape:\n",
    "                    print(f\"Warning: Shape mismatch in {func.__name__}. Input: {input_shape}, Output: {len(result)}\")\n",
    "                    # Don't force join or filtering here. Just warn.\n",
    "                return result.fill_null(0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {func.__name__}: {str(e)}\")\n",
    "                raise\n",
    "        return wrapper\n",
    "\n",
    "        \n",
    "    def get_dataset_columns(self, name: str) -> List[str]:\n",
    "        \"\"\"Get the list of columns from the dataset without loading data\"\"\"\n",
    "        path = f\"{self.data_path}/farcaster-{name}-0-1733162400.parquet\"\n",
    "        ds = pl.scan_parquet(path)\n",
    "        return ds.columns\n",
    "        \n",
    "    def _init_checkpoints(self):\n",
    "        \"\"\"Initialize checkpoint paths and check existing files\"\"\"\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        for name, feature_set in self.feature_sets.items():\n",
    "            path = f\"{self.checkpoint_dir}/{name}_features.parquet\"\n",
    "            feature_set.checkpoint_path = path\n",
    "            \n",
    "            if os.path.exists(path):\n",
    "                feature_set.last_modified = os.path.getmtime(path)\n",
    "    \n",
    "    def _needs_rebuild(self, feature_set: FeatureSet) -> bool:\n",
    "        \"\"\"Check if feature set needs to be rebuilt\"\"\"\n",
    "        # Always rebuild if no checkpoint exists\n",
    "        if not os.path.exists(feature_set.checkpoint_path):\n",
    "            return True\n",
    "                \n",
    "        return False\n",
    "\n",
    "\n",
    "    def extract_profile_features(self) -> pl.DataFrame:\n",
    "        \"\"\"Extract comprehensive profile features\"\"\"\n",
    "        profiles = self.loader.get_dataset('profile_with_addresses', \n",
    "            ['fid', 'fname', 'bio', 'avatar_url', 'verified_addresses', 'display_name'])\n",
    "        \n",
    "        # Filter valid profiles and cast fid type immediately\n",
    "        profiles = (profiles\n",
    "            .filter(pl.col('fname').is_not_null() & (pl.col('fname') != \"\"))\n",
    "            .with_columns(pl.col('fid').cast(pl.Int64)))\n",
    "        \n",
    "        df = profiles.with_columns([\n",
    "            pl.col('fname').str.contains(r'\\.eth$').cast(pl.Int32).alias('has_ens'),\n",
    "            (pl.col('bio').is_not_null() & (pl.col('bio') != \"\")).cast(pl.Int32).alias('has_bio'),\n",
    "            pl.col('avatar_url').is_not_null().cast(pl.Int32).alias('has_avatar'),\n",
    "            pl.when(pl.col('verified_addresses').str.contains(','))\n",
    "            .then(pl.col('verified_addresses').str.contains(',').cast(pl.Int32) + 1)\n",
    "            .otherwise(pl.when(pl.col('verified_addresses') != '[]')\n",
    "                        .then(1)\n",
    "                        .otherwise(0))\n",
    "            .alias('verification_count'),\n",
    "            (pl.col('display_name').is_not_null()).cast(pl.Int32).alias('has_display_name')\n",
    "        ])\n",
    "        \n",
    "        self.loader.clear_cache()\n",
    "        return df\n",
    "    def add_blocking_behavior(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add memory-efficient blocking behavior features\"\"\"\n",
    "        blocks = self.loader.get_dataset('blocks', ['blocker_fid', 'blocked_fid'])\n",
    "        \n",
    "        blocking_features = (\n",
    "            blocks.group_by('blocker_fid')\n",
    "            .agg([\n",
    "                pl.count().alias('blocks_made'),\n",
    "                pl.n_unique('blocked_fid').alias('unique_blocks')\n",
    "            ])\n",
    "            .with_columns([\n",
    "                (pl.col('blocks_made') / (pl.col('unique_blocks') + 1)).alias('block_repeat_ratio')\n",
    "            ])\n",
    "            .rename({'blocker_fid': 'fid'})\n",
    "        )\n",
    "        \n",
    "        self.loader.clear_cache()\n",
    "        return df.join(blocking_features, on='fid', how='left').fill_null(0)\n",
    "\n",
    "    def add_enhanced_verification_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add verification features with proper string handling\"\"\"\n",
    "        try:\n",
    "            # Initialize result with defaults\n",
    "            result = df.with_columns([\n",
    "                pl.lit(0).alias('total_verifications'),\n",
    "                pl.lit(0).alias('eth_verifications'),\n",
    "                pl.lit(0.0).alias('verification_timing_std'),\n",
    "                pl.lit(0).alias('platforms_verified'),\n",
    "                pl.lit(None).alias('first_platform_verification'),\n",
    "                pl.lit(None).alias('last_platform_verification'),\n",
    "                pl.lit(0).alias('verification_span_days')\n",
    "            ])\n",
    "            \n",
    "            # Process on-chain verifications\n",
    "            verifications = self.loader.get_dataset('verifications', \n",
    "                ['fid', 'claim', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            if verifications is not None and len(verifications) > 0:\n",
    "                verif_features = (\n",
    "                    verifications\n",
    "                    .filter(pl.col('deleted_at').is_null())\n",
    "                    .with_columns([\n",
    "                        pl.col('timestamp').cast(pl.Datetime)\n",
    "                    ])\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        pl.len().alias('total_verifications'),\n",
    "                        pl.col('claim').str.contains('ethSignature').sum().alias('eth_verifications'),\n",
    "                        # Convert durations to floats and fill nulls before std()\n",
    "                        pl.col('timestamp')\n",
    "                            .diff()\n",
    "                            .dt.total_seconds()\n",
    "                            .cast(pl.Float64)\n",
    "                            .fill_null(0)\n",
    "                            .std()\n",
    "                            .fill_null(0)\n",
    "                            .alias('verification_timing_std')\n",
    "                    ])\n",
    "                )\n",
    "                verif_features = verif_features.unique(subset=['fid']) \n",
    "                result = result.join(verif_features, on='fid', how='left')\n",
    "            \n",
    "            # Process platform verifications\n",
    "            acc_verifications = self.loader.get_dataset('account_verifications', \n",
    "                ['fid', 'platform', 'platform_username', 'verified_at'])\n",
    "            \n",
    "            if acc_verifications is not None and len(acc_verifications) > 0:\n",
    "                platform_features = (\n",
    "                    acc_verifications\n",
    "                    .with_columns([\n",
    "                        pl.col('platform_username').map_elements(lambda x: len(str(x)) if x else 0, return_dtype=pl.Int64),\n",
    "                        pl.col('verified_at').cast(pl.Datetime)\n",
    "                    ])\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        pl.n_unique('platform').alias('platforms_verified'),\n",
    "                        pl.col('verified_at').min().alias('first_platform_verification'),\n",
    "                        pl.col('verified_at').max().alias('last_platform_verification')\n",
    "                    ])\n",
    "                )\n",
    "                platform_features = platform_features.unique(subset=['fid']) \n",
    "                result = result.join(platform_features, on='fid', how='left')\n",
    "\n",
    "                result = result.with_columns([\n",
    "                    # First ensure both columns are Datetime\n",
    "                    pl.col('last_platform_verification').cast(pl.Datetime),\n",
    "                    pl.col('first_platform_verification').cast(pl.Datetime)\n",
    "                ])\n",
    "\n",
    "                # Compute duration safely in a separate step\n",
    "                result = result.with_columns([\n",
    "                    (pl.col('last_platform_verification') - pl.col('first_platform_verification'))\n",
    "                        .alias('verification_duration')\n",
    "                ])\n",
    "\n",
    "                # Now handle the null durations and convert to days\n",
    "                result = result.with_columns([\n",
    "                    pl.when(pl.col('verification_duration').is_not_null())\n",
    "                    .then(\n",
    "                        pl.col('verification_duration')\n",
    "                        .dt.total_days()  # This should return Float64 if duration is valid\n",
    "                        .fill_null(0.0)   # fill null if any appear\n",
    "                    )\n",
    "                    .otherwise(0.0)\n",
    "                    .alias('verification_span_days')\n",
    "                ])\n",
    "\n",
    "                # Drop the intermediate column if not needed\n",
    "                result = result.drop('verification_duration')\n",
    "\n",
    "            self.loader.clear_cache()\n",
    "            return result.fill_null(0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in verification features: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "    def add_cast_behavior_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add cast behavior features including link and media analysis\"\"\"\n",
    "        try:\n",
    "            base_fids = df['fid']\n",
    "            print(f\"Processing casts for {len(base_fids)} FIDs\")\n",
    "            \n",
    "            # Get casts with all needed fields\n",
    "            casts_df = self.loader.get_dataset('casts', columns=[\n",
    "                'fid', 'text', 'parent_hash', 'mentions', 'deleted_at', \n",
    "                'timestamp', 'embeds'  # Adding embeds for media detection\n",
    "            ])\n",
    "            \n",
    "            # Calculate features safely\n",
    "            valid_casts = casts_df.filter(pl.col('deleted_at').is_null())\n",
    "            def analyze_spam_patterns(text: str) -> Dict[str, int]:\n",
    "                if not text:\n",
    "                    return {'airdrop': 0, 'money': 0, 'rewards': 0, 'claim': 0, 'moxie': 0}\n",
    "                    \n",
    "                text = text.lower()\n",
    "                spam_keywords = ['airdrop', 'money', 'rewards', 'claim', 'moxie', 'nft', 'drop']\n",
    "                return {\n",
    "                    word: text.count(word) \n",
    "                    for word in spam_keywords\n",
    "                }\n",
    "                \n",
    "            def get_symbol_ratios(text: str) -> Dict[str, float]:\n",
    "                if not text:\n",
    "                    return {'at_symbol_ratio': 0, 'dollar_symbol_ratio': 0, 'link_ratio': 0}\n",
    "                    \n",
    "                total_length = len(text)\n",
    "                return {\n",
    "                    'at_symbol_ratio': text.count('@') / total_length if total_length > 0 else 0,\n",
    "                    'dollar_symbol_ratio': text.count('$') / total_length if total_length > 0 else 0,\n",
    "                    'link_ratio': len(re.findall(r'http[s]?://', text)) / total_length if total_length > 0 else 0\n",
    "                }\n",
    "            # Helper function to count links in text\n",
    "            def count_links(text):\n",
    "                if not text:\n",
    "                    return 0\n",
    "                # Look for common URL patterns\n",
    "                url_patterns = ['http://', 'https://', 'www.']\n",
    "                return sum(1 for pattern in url_patterns if pattern in text.lower())\n",
    "            \n",
    "            # Helper function to count media items in embeds\n",
    "            def count_media(embeds):\n",
    "                if not embeds or embeds == '[]':\n",
    "                    return 0\n",
    "                try:\n",
    "                    # Count image URLs in embeds\n",
    "                    return embeds.lower().count('image')\n",
    "                except:\n",
    "                    return 0\n",
    "            \n",
    "            # Add link and media detection\n",
    "            cast_features = (valid_casts\n",
    "                .with_columns([\n",
    "                    # Existing features\n",
    "                    pl.when(pl.col('text').is_not_null())\n",
    "                    .then(pl.col('text').map_elements(lambda x: len(x) if x else 0, return_dtype=pl.Int64))\n",
    "                    .otherwise(0)\n",
    "                    .alias('cast_length'),\n",
    "                    pl.col('parent_hash').is_not_null().cast(pl.Int32).alias('is_reply'),\n",
    "                    (pl.col('mentions').is_not_null() & \n",
    "                    (pl.col('mentions') != '') & \n",
    "                    (pl.col('mentions') != '[]')).cast(pl.Int32).alias('has_mentions'),\n",
    "                    \n",
    "                    # New features for links and media\n",
    "                    pl.when(pl.col('text').is_not_null())\n",
    "                    .then(pl.col('text').map_elements(count_links, return_dtype=pl.Int32))\n",
    "                    .otherwise(0)\n",
    "                    .alias('link_count'),\n",
    "                    \n",
    "                    pl.when(pl.col('embeds').is_not_null())\n",
    "                    .then(pl.col('embeds').map_elements(count_media, return_dtype=pl.Int32))\n",
    "                    .otherwise(0)\n",
    "                    .alias('media_count'),\n",
    "                    \n",
    "                    # Flag for casts containing both link and media\n",
    "                    (pl.when(pl.col('text').is_not_null())\n",
    "                    .then(pl.col('text').map_elements(count_links, return_dtype=pl.Int32))\n",
    "                    .otherwise(0) > 0 &\n",
    "                    pl.when(pl.col('embeds').is_not_null())\n",
    "                    .then(pl.col('embeds').map_elements(count_media, return_dtype=pl.Int32))\n",
    "                    .otherwise(0) > 0)\n",
    "                    .cast(pl.Int32)\n",
    "                    .alias('has_link_and_media'),\n",
    "\n",
    "                    pl.col('text').map_elements(analyze_spam_patterns, return_dtype=pl.Utf8).alias('spam_counts'),\n",
    "                    pl.col('text').map_elements(get_symbol_ratios, return_dtype=pl.Utf8).alias('symbol_ratios'),\n",
    "                    pl.col('text').map_elements(self._analyze_content_patterns, return_dtype=pl.Utf8).alias('content_patterns')\n",
    "\n",
    "                ])\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    # Existing metrics\n",
    "                    pl.len().alias('cast_count'),\n",
    "                    pl.col('cast_length').mean().alias('avg_cast_length'),\n",
    "                    pl.col('is_reply').sum().alias('reply_count'),\n",
    "                    pl.col('has_mentions').sum().alias('mentions_count'),\n",
    "                    \n",
    "                    # New metrics for links\n",
    "                    pl.col('link_count').sum().alias('total_links'),\n",
    "                    (pl.col('link_count') > 0).sum().alias('casts_with_links'),\n",
    "                    (pl.col('link_count') / pl.len()).alias('link_ratio'),\n",
    "                    \n",
    "                    # New metrics for media\n",
    "                    pl.col('media_count').sum().alias('total_media'),\n",
    "                    (pl.col('media_count') > 0).sum().alias('casts_with_media'),\n",
    "                    (pl.col('media_count') / pl.len()).alias('media_ratio'),\n",
    "                    \n",
    "                    # Spam metrics\n",
    "                    (pl.col('spam_counts').map_elements(lambda x: x['airdrop']).sum() / pl.len())\n",
    "                        .alias('airdrop_mention_ratio'),\n",
    "                    (pl.col('spam_counts').map_elements(lambda x: sum(x.values())).sum() / pl.len())\n",
    "                        .alias('spam_keyword_ratio'),\n",
    "                        \n",
    "                    # Symbol usage metrics\n",
    "                    pl.col('symbol_ratios').map_elements(lambda x: x['at_symbol_ratio']).mean()\n",
    "                        .alias('avg_at_symbol_ratio'),\n",
    "                    pl.col('symbol_ratios').map_elements(lambda x: x['dollar_symbol_ratio']).mean()\n",
    "                        .alias('avg_dollar_symbol_ratio'),\n",
    "                    pl.col('symbol_ratios').map_elements(lambda x: x['link_ratio']).mean()\n",
    "                        .alias('avg_link_ratio'),\n",
    "                        \n",
    "                    # Combined metrics\n",
    "                    pl.col('has_link_and_media').sum().alias('casts_with_both'),\n",
    "                    (pl.col('has_link_and_media').sum() / pl.len()).alias('multimedia_ratio')\n",
    "\n",
    "                    # Content pattern metrics\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['template_structure'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('template_usage_ratio'),\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['multiple_cta'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('cta_heavy_ratio'),\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['urgency_terms'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('urgency_ratio'),\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['excessive_emojis'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('emoji_spam_ratio'),\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['price_mentions'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('price_mention_ratio'),\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['excessive_symbols'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('symbol_spam_ratio'),\n",
    "                    (pl.col('content_patterns').map_elements(lambda x: x['airdrop_terms'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('airdrop_term_ratio')\n",
    "                ]))\n",
    "            \n",
    "            cast_features = cast_features.unique(subset=['fid']) \n",
    "            # Join and handle nulls\n",
    "            result = df.join(cast_features, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "            # Add derived ratios\n",
    "            result = result.with_columns([\n",
    "                # Percentage of casts that contain links\n",
    "                (pl.col('casts_with_links') / pl.col('cast_count')).alias('link_usage_rate'),\n",
    "                # Percentage of casts that contain media\n",
    "                (pl.col('casts_with_media') / pl.col('cast_count')).alias('media_usage_rate'),\n",
    "                # Average number of links per cast with links\n",
    "                (pl.col('total_links') / (pl.col('casts_with_links') + 1)).alias('avg_links_per_link_cast'),\n",
    "                # Average number of media items per cast with media\n",
    "                (pl.col('total_media') / (pl.col('casts_with_media') + 1)).alias('avg_media_per_media_cast')\n",
    "            ])\n",
    "            \n",
    "            return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in cast behavior: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "\n",
    "    def add_influence_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add influence features with proper error handling\"\"\"\n",
    "        try:\n",
    "            # Ensure required columns exist and are properly initialized\n",
    "            required_cols = ['follower_count', 'following_count', 'total_reactions', 'cast_count']\n",
    "            for col in required_cols:\n",
    "                if col not in df.columns:\n",
    "                    df = df.with_columns(pl.lit(0).alias(col))\n",
    "            \n",
    "            # Calculate time span if possible\n",
    "            if 'first_follow' in df.columns and 'last_follow' in df.columns:\n",
    "                df = df.with_columns([\n",
    "                    pl.when(pl.col('last_follow').is_not_null() & pl.col('first_follow').is_not_null())\n",
    "                    .then((pl.col('last_follow') - pl.col('first_follow')).dt.total_hours())\n",
    "                    .otherwise(0)\n",
    "                    .alias('follow_time_span_hours')\n",
    "                ])\n",
    "            else:\n",
    "                df = df.with_columns(pl.lit(0).alias('follow_time_span_hours'))\n",
    "\n",
    "            # Calculate influence metrics safely\n",
    "            df = df.with_columns([\n",
    "                # Normalize influence metrics\n",
    "                ((pl.col('follower_count').fill_null(0) * 0.4 +\n",
    "                pl.col('total_reactions').fill_null(0) * 0.3 +\n",
    "                pl.col('cast_count').fill_null(0) * 0.3) / \n",
    "                (pl.col('following_count').fill_null(0) + 1)\n",
    "                ).alias('influence_score'),\n",
    "                \n",
    "                # Safe engagement rate calculation\n",
    "                (pl.when(pl.col('cast_count') > 0)\n",
    "                .then(pl.col('total_reactions') / pl.col('cast_count'))\n",
    "                .otherwise(0)\n",
    "                ).alias('engagement_rate'),\n",
    "                \n",
    "                # Safe follower growth rate calculation\n",
    "                (pl.when(pl.col('follow_time_span_hours') > 0)\n",
    "                .then(pl.col('follower_count') / pl.col('follow_time_span_hours'))\n",
    "                .otherwise(0)\n",
    "                ).alias('follower_growth_rate')\n",
    "            ])\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in influence features: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "    def add_storage_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add storage features with updated functions\"\"\"\n",
    "        storage = self.loader.get_dataset('storage', ['fid', 'units', 'deleted_at'])\n",
    "        \n",
    "        storage_features = (\n",
    "            storage.filter(pl.col('deleted_at').is_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.col('units').mean().alias('avg_storage_units'),\n",
    "                pl.col('units').max().alias('max_storage_units'),\n",
    "                pl.len().alias('storage_update_count')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        self.loader.clear_cache()\n",
    "        storage_features = storage_features.unique(subset=['fid']) \n",
    "        return df.join(storage_features, on='fid', how='left').fill_null(0)\n",
    "    def add_user_data_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Extract features from user_data with better error handling\"\"\"\n",
    "        try:\n",
    "            user_data = self.loader.get_dataset('user_data', \n",
    "                ['fid', 'type', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            if user_data is None or len(user_data) == 0:\n",
    "                return df.with_columns([\n",
    "                    pl.lit(0).alias('total_user_data_updates'),\n",
    "                    pl.lit(0.0).alias('avg_update_interval')\n",
    "                ])\n",
    "                \n",
    "            update_features = (\n",
    "                user_data.filter(pl.col('deleted_at').is_null())\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    pl.len().alias('total_user_data_updates'),\n",
    "                    pl.col('timestamp').diff().mean().dt.total_hours().fill_null(0)\n",
    "                        .alias('avg_update_interval')\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            self.loader.clear_cache()\n",
    "            update_features = update_features.unique(subset=['fid']) \n",
    "            return df.join(update_features, on='fid', how='left').fill_null(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in user_data features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0).alias('total_user_data_updates'),\n",
    "                pl.lit(0.0).alias('avg_update_interval')\n",
    "            ])\n",
    "    def add_signer_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Extract features from signer behavior\"\"\"\n",
    "        signers = self.loader.get_dataset('signers', \n",
    "            ['fid', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        signer_features = (\n",
    "            signers.filter(pl.col('deleted_at').is_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.count().alias('signer_count'),\n",
    "                pl.col('timestamp').diff().mean().dt.total_hours().alias('avg_hours_between_signers'),\n",
    "                pl.col('timestamp').diff().std().dt.total_hours().alias('std_hours_between_signers')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        self.loader.clear_cache()\n",
    "        signer_features = signer_features.unique(subset=['fid']) \n",
    "        return df.join(signer_features, on='fid', how='left').fill_null(0)\n",
    "        \n",
    "    def add_reaction_patterns(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add reaction pattern features with dimension validation\"\"\"\n",
    "        try:\n",
    "            base_fids = df['fid']\n",
    "            print(f\"Processing reactions for {len(base_fids)} FIDs\")\n",
    "            \n",
    "            reactions = self.loader.get_dataset('reactions', \n",
    "                ['fid', 'reaction_type', 'target_fid', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            # First filter by base FIDs\n",
    "            reactions = reactions.filter(pl.col('fid').is_in(base_fids))\n",
    "            reaction_features = (\n",
    "                reactions.filter(pl.col('deleted_at').is_null())\n",
    "                .with_columns([\n",
    "                    pl.col('timestamp').cast(pl.Datetime)\n",
    "                ])\n",
    "                .sort('timestamp')\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    pl.len().alias('total_reactions'),\n",
    "                    (pl.col('reaction_type') == 1).sum().alias('like_count'),\n",
    "                    (pl.col('reaction_type') == 2).sum().alias('recast_count'),\n",
    "                    pl.n_unique('target_fid').alias('unique_users_reacted_to'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().mean().alias('avg_hours_between_reactions'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().std().alias('std_hours_between_reactions')\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            # Calculate ratios only after joining back to maintain dimensions\n",
    "            reaction_features = reaction_features.unique(subset=['fid']) \n",
    "            result = df.join(reaction_features, on='fid', how='left', coalesce=True).fill_null(0)\n",
    "            \n",
    "            result = result.with_columns([\n",
    "                (pl.col('like_count') / (pl.col('total_reactions') + 1)).alias('like_ratio'),\n",
    "                (pl.col('recast_count') / (pl.col('total_reactions') + 1)).alias('recast_ratio'),\n",
    "                (pl.col('unique_users_reacted_to') / (pl.col('total_reactions') + 1)).alias('reaction_diversity'),\n",
    "                (pl.col('like_count') / (pl.col('recast_count') + 1)).alias('likes_to_recasts_ratio'),\n",
    "            ])\n",
    "            \n",
    "            # Verify dimensions\n",
    "            if len(result) != len(df):\n",
    "                print(f\"Warning: Reaction features shape mismatch. Expected {len(df)}, got {len(result)}\")\n",
    "                result = result.filter(pl.col('fid').is_in(base_fids))\n",
    "                \n",
    "            self.loader.clear_cache()\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in reaction patterns: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "    def build_network_quality_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Build network quality features with safer dependency handling\"\"\"\n",
    "        try:\n",
    "            # Ensure required base metrics exist\n",
    "            base_metrics = {\n",
    "                'engagement_score': 0.0,\n",
    "                'following_count': 0,\n",
    "                'follower_count': 0\n",
    "            }\n",
    "            \n",
    "            result = self._validate_and_ensure_features(df, base_metrics)\n",
    "            \n",
    "            # Load power users\n",
    "            power_users = self.loader.get_dataset('power_users', ['fid'])\n",
    "            if power_users is None or len(power_users) == 0:\n",
    "                return result.with_columns([\n",
    "                    pl.lit(0).alias('power_reply_count'),\n",
    "                    pl.lit(0).alias('power_mentions_count')\n",
    "                ])\n",
    "            \n",
    "            # Calculate power user metrics\n",
    "            power_fids = power_users['fid'].cast(pl.Int64).unique()\n",
    "            casts = self.loader.get_dataset('casts', \n",
    "                ['fid', 'parent_fid', 'mentions', 'deleted_at'])\n",
    "                \n",
    "            if casts is not None and len(casts) > 0:\n",
    "                power_fid_str = str(power_fids[0])\n",
    "\n",
    "                power_metrics = (\n",
    "                    casts.filter(pl.col('deleted_at').is_null())\n",
    "                    .with_columns([\n",
    "                        pl.col('parent_fid').cast(pl.Int64).is_in(power_fids)\n",
    "                            .alias('is_power_reply'),\n",
    "                        pl.when(pl.col('mentions').is_not_null() & pl.col('mentions').str.contains(power_fid_str))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias('has_power_mention')\n",
    "                    ])\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        pl.sum('is_power_reply').alias('power_reply_count'),\n",
    "                        pl.sum('has_power_mention').alias('power_mentions_count')\n",
    "                    ])\n",
    "                )\n",
    "                \n",
    "                result = result.join(power_metrics, on='fid', how='left').fill_null(0)\n",
    "                \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in network quality features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0).alias('power_reply_count'),\n",
    "                pl.lit(0).alias('power_mentions_count')\n",
    "            ])\n",
    "\n",
    "    def add_network_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add network features with proper error handling and null safety\"\"\"\n",
    "        try:\n",
    "            links = self.loader.get_dataset('links', \n",
    "                ['fid', 'target_fid', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            # Filter valid links first\n",
    "            valid_links = links.filter(pl.col('deleted_at').is_null())\n",
    "            \n",
    "            # Calculate following patterns safely\n",
    "            following = (valid_links\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    pl.len().alias('following_count'),\n",
    "                    pl.n_unique('target_fid').alias('unique_following_count'),\n",
    "                    pl.col('timestamp').min().alias('first_follow'),\n",
    "                    pl.col('timestamp').max().alias('last_follow')\n",
    "                ])\n",
    "                .fill_null(0))\n",
    "            \n",
    "            # Calculate follower patterns separately\n",
    "            followers = (valid_links\n",
    "                .group_by('target_fid')\n",
    "                .agg([\n",
    "                    pl.len().alias('follower_count'),\n",
    "                    pl.n_unique('fid').alias('unique_follower_count')\n",
    "                ])\n",
    "                .rename({'target_fid': 'fid'})\n",
    "                .fill_null(0))\n",
    "            \n",
    "            # Join both patterns\n",
    "            result = df.join(following, on='fid', how='left').fill_null(0)\n",
    "            result = result.join(followers, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "            # Calculate ratios safely with null handling\n",
    "            result = result.with_columns([\n",
    "                (pl.col('follower_count') / (pl.col('following_count') + 1))\n",
    "                    .alias('follower_ratio'),\n",
    "                (pl.col('unique_follower_count') / (pl.col('unique_following_count') + 1))\n",
    "                    .alias('unique_follower_ratio'),\n",
    "                \n",
    "                # Add log transformations\n",
    "                (pl.col('follower_count') / (pl.col('following_count') + 1))\n",
    "                    .log1p()\n",
    "                    .alias('follower_ratio_log'),\n",
    "                (pl.col('unique_follower_count') / (pl.col('unique_following_count') + 1))\n",
    "                    .log1p()\n",
    "                    .alias('unique_follower_ratio_log')\n",
    "            ])\n",
    "            \n",
    "            return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in network features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0).alias('following_count'),\n",
    "                pl.lit(0).alias('unique_following_count'),\n",
    "                pl.lit(0).alias('follower_count'),\n",
    "                pl.lit(0).alias('unique_follower_count'),\n",
    "                pl.lit(0.0).alias('follower_ratio'),\n",
    "                pl.lit(0.0).alias('unique_follower_ratio'),\n",
    "                pl.lit(0.0).alias('follower_ratio_log'),\n",
    "                pl.lit(0.0).alias('unique_follower_ratio_log')\n",
    "            ])\n",
    "    def add_temporal_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add enhanced temporal features with burst detection\"\"\"\n",
    "        try:\n",
    "            links = self.loader.get_dataset('links', ['fid', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            # Ensure timestamp is datetime type\n",
    "            valid_links = (links\n",
    "                .filter(pl.col('deleted_at').is_null())\n",
    "                .filter(pl.col('timestamp').is_not_null())\n",
    "                .with_columns([\n",
    "                    pl.col('timestamp').cast(pl.Datetime).alias('timestamp')\n",
    "                ]))\n",
    "            \n",
    "            temporal_features = (valid_links\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    # Basic temporal features\n",
    "                    pl.len().alias('total_activity'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().mean().alias('avg_hours_between_actions'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().std().alias('std_hours_between_actions'),\n",
    "                    pl.col('timestamp').dt.weekday().std().alias('weekday_variance'),\n",
    "                    (pl.col('timestamp').diff().dt.total_hours() < 1).sum().alias('rapid_actions'),\n",
    "                    (pl.col('timestamp').diff().dt.total_hours() > 24).sum().alias('long_gaps'),\n",
    "                    \n",
    "                    # New temporal features\n",
    "                    pl.col('timestamp').diff().dt.total_hours().quantile(0.9).alias('p90_time_between_actions'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().quantile(0.1).alias('p10_time_between_actions'),\n",
    "                    \n",
    "                    # Calculate burst ratio (actions within 1 hour of each other)\n",
    "                    (pl.col('timestamp').diff().dt.total_hours() < 1).sum().alias('actions_in_bursts'),\n",
    "                    \n",
    "                    # Calculate velocity\n",
    "                    (pl.col('timestamp').max() - pl.col('timestamp').min()).dt.total_hours().alias('time_span')\n",
    "                ]))\n",
    "            \n",
    "            # Add derived temporal metrics\n",
    "            result = df.join(temporal_features, on='fid', how='left').fill_null(0)\n",
    "            result = result.with_columns([\n",
    "                # Burst activity ratio\n",
    "                (pl.col('actions_in_bursts') / (pl.col('total_activity') + 1)).alias('burst_activity_ratio'),\n",
    "                \n",
    "                # Activity spread (ratio of actual timespan to expected even distribution)\n",
    "                (pl.col('time_span') / ((pl.col('total_activity') + 1) * pl.col('avg_hours_between_actions'))).alias('activity_spread'),\n",
    "                \n",
    "                # Temporal irregularity (variation in action timing)\n",
    "                (pl.col('std_hours_between_actions') / (pl.col('avg_hours_between_actions') + 1)).alias('temporal_irregularity'),\n",
    "                \n",
    "                # Follow velocity (follows per hour)\n",
    "                (pl.col('total_activity') / (pl.col('time_span') + 1)).alias('follow_velocity')\n",
    "            ])\n",
    "            \n",
    "            return result.fill_null(0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in temporal features: {str(e)}\")\n",
    "            raise\n",
    "            return df.fill_null(0)\n",
    "# \n",
    "    # def add_advanced_temporal_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add advanced temporal features for bot detection\"\"\"\n",
    "        try:\n",
    "            activities = []\n",
    "            \n",
    "            # Collect cast timestamps\n",
    "            casts = self.loader.get_dataset('casts', ['fid', 'timestamp', 'deleted_at'])\n",
    "            if casts is not None:\n",
    "                valid_casts = casts.filter(pl.col('deleted_at').is_null())\n",
    "                activities.append(valid_casts.select(['fid', 'timestamp']))\n",
    "            \n",
    "            # Collect reaction timestamps\n",
    "            reactions = self.loader.get_dataset('reactions', ['fid', 'timestamp', 'deleted_at'])\n",
    "            if reactions is not None:\n",
    "                valid_reactions = reactions.filter(pl.col('deleted_at').is_null())\n",
    "                activities.append(valid_reactions.select(['fid', 'timestamp']))\n",
    "            \n",
    "            if not activities:\n",
    "                return df\n",
    "            \n",
    "            # Combine all activities\n",
    "            all_activities = pl.concat(activities)\n",
    "            \n",
    "            temporal_features = (all_activities\n",
    "                .sort(['fid', 'timestamp'])\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    # Robotic timing detection\n",
    "                    (pl.col('timestamp').diff().dt.total_seconds().std() < 1)\n",
    "                        .cast(pl.Int32)\n",
    "                        .alias('has_robotic_timing'),\n",
    "                    \n",
    "                    # Rapid actions\n",
    "                    (pl.col('timestamp').diff().dt.total_seconds() < 2)\n",
    "                        .sum()\n",
    "                        .alias('rapid_action_count'),\n",
    "                    \n",
    "                    # Activity bursts\n",
    "                    (pl.col('timestamp').diff().dt.total_hours().gt(24).sum())\n",
    "                        .alias('long_dormancy_periods'),\n",
    "                        \n",
    "                    # Time between bursts\n",
    "                    pl.col('timestamp').diff().dt.total_hours().mean().alias('avg_burst_interval')\n",
    "                ]))\n",
    "            \n",
    "            return df.join(temporal_features, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in advanced temporal features: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def add_power_user_interaction_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Enhanced power user interaction analysis\"\"\"\n",
    "        try:\n",
    "            # Load power users\n",
    "            power_users = self.loader.get_dataset('warpcast_power_users', ['fid'])\n",
    "            if power_users is None or len(power_users) == 0:\n",
    "                print(\"Warning: No power users found\")\n",
    "                return df.with_columns([\n",
    "                    pl.lit(0).alias('power_user_replies'),\n",
    "                    pl.lit(0).alias('power_user_mentions'),\n",
    "                    pl.lit(0).alias('power_user_reactions'),\n",
    "                    pl.lit(0).alias('power_user_interaction_ratio')\n",
    "                ])\n",
    "            \n",
    "            # Ensure power_fids are Int64\n",
    "            power_fids = power_users['fid'].cast(pl.Int64).unique()\n",
    "            \n",
    "            # Get interactions with power users\n",
    "            casts = self.loader.get_dataset('casts', \n",
    "                ['fid', 'parent_fid', 'mentions', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            # Process cast interactions\n",
    "            power_fid_str = str(power_fids[0])\n",
    "            power_cast_features = (\n",
    "                casts.filter(pl.col('deleted_at').is_null())\n",
    "                .with_columns([\n",
    "                    pl.col('parent_fid').cast(pl.Int64).is_in(power_fids).alias('is_power_reply'),\n",
    "               pl.when(pl.col('mentions').is_not_null() & pl.col('mentions').str.contains(power_fid_str))\n",
    "        .then(1)\n",
    "        .otherwise(0)\n",
    "        .alias('has_power_mention')\n",
    "\n",
    "                ])\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    pl.sum('is_power_reply').alias('power_user_replies'),\n",
    "                    pl.sum('has_power_mention').alias('power_user_mentions'),\n",
    "                    pl.len().alias('total_casts')\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            # Get reaction data\n",
    "            reactions = self.loader.get_dataset('reactions', \n",
    "                ['fid', 'target_fid', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            power_reaction_features = (\n",
    "                reactions.filter(pl.col('deleted_at').is_null())\n",
    "                .with_columns([\n",
    "                    pl.col('target_fid').cast(pl.Int64).is_in(power_fids).alias('is_power_reaction')\n",
    "                ])\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    pl.sum('is_power_reaction').alias('power_user_reactions'),\n",
    "                    pl.len().alias('total_reactions')\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            # Join features\n",
    "            result = df.join(power_cast_features, on='fid', how='left')\n",
    "            result = result.join(power_reaction_features, on='fid', how='left')\n",
    "            \n",
    "            # Calculate interaction ratios\n",
    "            result = result.with_columns([\n",
    "                pl.col('power_user_replies').fill_null(0),\n",
    "                pl.col('power_user_mentions').fill_null(0),\n",
    "                pl.col('power_user_reactions').fill_null(0),\n",
    "                pl.col('total_casts').fill_null(0),\n",
    "                pl.col('total_reactions').fill_null(0)\n",
    "            ])\n",
    "            \n",
    "            # Calculate overall interaction ratio\n",
    "            result = result.with_columns([\n",
    "                ((pl.col('power_user_replies') + \n",
    "                pl.col('power_user_mentions') + \n",
    "                pl.col('power_user_reactions')) / \n",
    "                (pl.col('total_casts') + pl.col('total_reactions') + 1)\n",
    "                ).alias('power_user_interaction_ratio')\n",
    "            ])\n",
    "            \n",
    "            return result.fill_null(0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in power user interaction features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0).alias('power_user_replies'),\n",
    "                pl.lit(0).alias('power_user_mentions'),\n",
    "                pl.lit(0).alias('power_user_reactions'),\n",
    "                pl.lit(0).alias('power_user_interaction_ratio')\n",
    "            ])\n",
    "\n",
    "    def add_activity_patterns_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add activity patterns with fully safe calculations\"\"\"\n",
    "        try:\n",
    "            print(\"Processing activity patterns...\")\n",
    "            \n",
    "            # Get activity data\n",
    "            casts = self.loader.get_dataset('casts', ['fid', 'timestamp', 'deleted_at'])\n",
    "            reactions = self.loader.get_dataset('reactions', ['fid', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            # Initialize result with default values\n",
    "            result = df.with_columns([\n",
    "                pl.lit(0.0).alias('hour_diversity'),\n",
    "                pl.lit(0.0).alias('weekday_diversity'),\n",
    "                pl.lit(0.0).alias('total_activities')\n",
    "            ])\n",
    "            \n",
    "            # Process activities if data exists\n",
    "            if casts is not None and reactions is not None:\n",
    "                # Combine valid activities\n",
    "                activities = pl.concat([\n",
    "                    casts.filter(pl.col('deleted_at').is_null())\n",
    "                        .select(['fid', 'timestamp']),\n",
    "                    reactions.filter(pl.col('deleted_at').is_null())\n",
    "                        .select(['fid', 'timestamp'])\n",
    "                ])\n",
    "                \n",
    "                if len(activities) > 0:\n",
    "                    # Calculate activity metrics\n",
    "                    activity_features = (activities\n",
    "                        .with_columns([\n",
    "                            pl.col('timestamp').cast(pl.Datetime).dt.hour().alias('hour'),\n",
    "                            pl.col('timestamp').cast(pl.Datetime).dt.weekday().alias('weekday')\n",
    "                        ])\n",
    "                        .group_by('fid')\n",
    "                        .agg([\n",
    "                            pl.col('hour').value_counts()\n",
    "                                .std().fill_null(0).alias('hour_diversity'),\n",
    "                            pl.col('weekday').value_counts()\n",
    "                                .std().fill_null(0).alias('weekday_diversity'),\n",
    "                            pl.len().alias('total_activities')\n",
    "                        ])\n",
    "                    )\n",
    "                    \n",
    "                    # Update result with calculated features\n",
    "                    result = df.join(activity_features, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "            print(\"Activity patterns calculated successfully\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in activity patterns: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0.0).alias('hour_diversity'),\n",
    "                pl.lit(0.0).alias('weekday_diversity'),\n",
    "                pl.lit(0.0).alias('total_activities')\n",
    "            ])\n",
    "    def verify_matrix(self, df: pl.DataFrame):\n",
    "        \"\"\"Verify the final feature matrix has no list columns\"\"\"\n",
    "        for col in df.columns:\n",
    "            dtype = df[col].dtype\n",
    "            if str(dtype).startswith('List'):\n",
    "                raise ValueError(f\"Column {col} is still a list type: {dtype}\")\n",
    "            if dtype not in [pl.Float64, pl.Int64]:\n",
    "                raise ValueError(f\"Column {col} is not numeric: {dtype}\")\n",
    "                \n",
    "    def add_mentions_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Analyze mention patterns with proper null handling\"\"\"\n",
    "        try:\n",
    "            base_fids = df['fid']\n",
    "            print(f\"Processing mentions for {len(base_fids)} FIDs\")\n",
    "            \n",
    "            casts = self.loader.get_dataset('casts', ['fid', 'mentions', 'deleted_at'])\n",
    "            \n",
    "            # Filter by base FIDs first\n",
    "            casts = casts.filter(pl.col('fid').is_in(base_fids))\n",
    "            \n",
    "            # Parse mentions as JSON and handle counts\n",
    "            mention_features = (\n",
    "                casts.filter(pl.col('deleted_at').is_null())\n",
    "                .with_columns([\n",
    "                    # Parse JSON string to array and count elements\n",
    "                    pl.when(\n",
    "                        pl.col('mentions').is_not_null() & \n",
    "                        (pl.col('mentions') != '') & \n",
    "                        (pl.col('mentions') != '[]')\n",
    "                    )\n",
    "                    .then(pl.col('mentions').str.json_decode().list.len())\n",
    "                    .otherwise(0)\n",
    "                    .alias('mention_count'),\n",
    "                    \n",
    "                    # Flag for casts with mentions\n",
    "                    (pl.col('mentions').is_not_null() & \n",
    "                    (pl.col('mentions') != '') & \n",
    "                    (pl.col('mentions') != '[]')\n",
    "                    ).cast(pl.Int32).alias('has_mentions')\n",
    "                ])\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    # Count total casts with mentions\n",
    "                    pl.col('has_mentions').sum().alias('casts_with_mentions'),\n",
    "                    # Total mentions\n",
    "                    pl.col('mention_count').sum().alias('total_mentions'),\n",
    "                    # Average mentions per cast\n",
    "                    pl.col('mention_count').mean().alias('avg_mentions_per_cast')\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            # Join and add ratios\n",
    "            result = df.join(mention_features, on='fid', how='left', coalesce=True).fill_null(0)\n",
    "            \n",
    "            # Add derived metrics\n",
    "            result = result.with_columns([\n",
    "                (pl.col('casts_with_mentions') / (pl.col('cast_count') + 1)).alias('mention_frequency'),\n",
    "                (pl.col('avg_mentions_per_cast') / (pl.col('cast_count') + 1)).alias('mention_ratio')\n",
    "            ])\n",
    "            \n",
    "            print(f\"Mentions features complete. Shape: {result.shape}\")\n",
    "            self.loader.clear_cache()\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in mentions features: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "    def add_reply_patterns_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add reply features with updated functions\"\"\"\n",
    "        casts = self.loader.get_dataset('casts', \n",
    "            ['fid', 'parent_hash', 'parent_fid', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        reply_features = (\n",
    "            casts.filter(pl.col('deleted_at').is_null())\n",
    "            .filter(pl.col('parent_hash').is_not_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.len().alias('total_replies'),\n",
    "                pl.n_unique('parent_fid').alias('unique_users_replied_to'),\n",
    "                pl.col('timestamp').diff().mean().dt.total_seconds()\n",
    "                    .alias('avg_seconds_between_replies'),\n",
    "                pl.col('timestamp').diff().std().dt.total_seconds()\n",
    "                    .alias('std_seconds_between_replies')\n",
    "            ])\n",
    "            .with_columns([\n",
    "                (pl.col('unique_users_replied_to') / pl.col('total_replies'))\n",
    "                    .alias('reply_diversity'),\n",
    "                (pl.col('std_seconds_between_replies') / \n",
    "                pl.col('avg_seconds_between_replies')).alias('reply_timing_variability')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        self.loader.clear_cache()\n",
    "        return df.join(reply_features, on='fid', how='left').fill_null(0)\n",
    "\n",
    "    # def add_cluster_analysis_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "    #     \"\"\"Analyze network clustering with updated functions\"\"\"\n",
    "    #     try:\n",
    "    #         links = self.loader.get_dataset('links', \n",
    "    #             ['fid', 'target_fid', 'deleted_at'])\n",
    "            \n",
    "    #         valid_links = links.filter(pl.col('deleted_at').is_null())\n",
    "            \n",
    "    #         # Calculate clustering features\n",
    "    #         cluster_features = (\n",
    "    #             valid_links.join(\n",
    "    #                 valid_links.rename({'fid': 'mutual_fid', 'target_fid': 'mutual_target'}),\n",
    "    #                 left_on='target_fid',\n",
    "    #                 right_on='mutual_fid'\n",
    "    #             )\n",
    "    #             .group_by('fid')\n",
    "    #             .agg([\n",
    "    #                 pl.n_unique('mutual_target').alias('mutual_connections'),\n",
    "    #                 pl.len().alias('potential_triangles')\n",
    "    #             ])\n",
    "    #             .with_columns([\n",
    "    #                 (pl.col('mutual_connections') / (pl.col('potential_triangles') + 1))\n",
    "    #                 .alias('clustering_coefficient')\n",
    "    #             ])\n",
    "    #         )\n",
    "            \n",
    "    #         self.loader.clear_cache()\n",
    "    #         return df.join(cluster_features, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error in cluster analysis: {str(e)}\")\n",
    "    #         raise\n",
    "    #         return df.with_columns([\n",
    "    #             pl.lit(0).alias('mutual_connections'),\n",
    "    #             pl.lit(0).alias('potential_triangles'),\n",
    "    #             pl.lit(0.0).alias('clustering_coefficient')\n",
    "    #         ])\n",
    "\n",
    "    def add_authenticity_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add authenticity features with comprehensive null handling\"\"\"\n",
    "        try:\n",
    "            print(\"Building authenticity features...\")\n",
    "            \n",
    "            # Initialize with safe default values\n",
    "            result = df.clone()\n",
    "            required_cols = {\n",
    "                'has_bio': 0,\n",
    "                'has_avatar': 0,\n",
    "                'verification_count': 0,\n",
    "                'has_ens': 0,\n",
    "                'following_count': 0.0,\n",
    "                'follower_count': 0.0,\n",
    "                'total_updates': 0,\n",
    "                'avg_update_interval': 0.0,\n",
    "                'profile_update_consistency': 0.0\n",
    "            }\n",
    "            \n",
    "            # Ensure all required columns exist with proper types\n",
    "            for col, default in required_cols.items():\n",
    "                if col not in result.columns:\n",
    "                    print(f\"Adding missing column {col} with default {default}\")\n",
    "                    result = result.with_columns(pl.lit(default).alias(col))\n",
    "                \n",
    "                # Fill nulls with defaults\n",
    "                result = result.with_columns(\n",
    "                    pl.col(col).fill_null(default).alias(col)\n",
    "                )\n",
    "            \n",
    "            result = result.with_columns(\n",
    "                pl.col('total_updates').cast(pl.Float64).fill_null(0),\n",
    "                pl.col('profile_update_consistency').cast(pl.Float64).fill_null(0)\n",
    "            )\n",
    "\n",
    "            # Safe calculations with explicit null handling\n",
    "            result = result.with_columns([\n",
    "                # Profile completeness (0-1) with safe operations\n",
    "                ((pl.col('has_bio').fill_null(0) + \n",
    "                pl.col('has_avatar').fill_null(0) + \n",
    "                pl.col('has_ens').fill_null(0) + \n",
    "                (pl.col('verification_count').fill_null(0) > 0).cast(pl.Int64)) / 4.0\n",
    "                ).alias('profile_completeness'),\n",
    "                \n",
    "                # Network balance (0-1) with safe division\n",
    "                (pl.when(pl.col('following_count').fill_null(0) + pl.col('follower_count').fill_null(0) > 0)\n",
    "                .then(1.0 - (pl.col('following_count').fill_null(0) - pl.col('follower_count').fill_null(0)).abs() /\n",
    "                    (pl.col('following_count').fill_null(0) + pl.col('follower_count').fill_null(0)))\n",
    "                .otherwise(0.0)\n",
    "                ).alias('network_balance'),\n",
    "                \n",
    "                # Update naturalness (0-1) with safe comparisons\n",
    "                    (pl.when(pl.col('total_updates') > 0)\n",
    "                .then(1.0 - pl.col('profile_update_consistency').clip(0.0, 1.0))\n",
    "                .otherwise(0.0))\n",
    "                .alias('update_naturalness')\n",
    "            ])\n",
    "            \n",
    "            # Calculate final authenticity score with weights\n",
    "            result = result.with_columns([\n",
    "                (pl.col('profile_completeness').fill_null(0.0) * 0.4 +\n",
    "                pl.col('network_balance').fill_null(0.0) * 0.3 +\n",
    "                pl.col('update_naturalness').fill_null(0.0) * 0.3\n",
    "                ).alias('authenticity_score')\n",
    "            ])\n",
    "            \n",
    "            print(\"Authenticity features completed successfully\")\n",
    "            return result.drop(['profile_completeness', 'network_balance', 'update_naturalness'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in authenticity features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns(pl.lit(0.0).alias('authenticity_score'))\n",
    "    def add_update_behavior_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add update behavior features with comprehensive null handling\"\"\"\n",
    "        try:\n",
    "            print(\"Building update behavior features...\")\n",
    "            \n",
    "            # Initialize result with default values\n",
    "            result = df.clone().with_columns([\n",
    "                pl.lit(0.0).alias('profile_update_consistency'),\n",
    "                pl.lit(0).alias('total_updates'),\n",
    "                pl.lit(0.0).alias('avg_update_interval'),\n",
    "                pl.lit(0.0).alias('update_time_std')\n",
    "            ])\n",
    "            \n",
    "            # Get user data\n",
    "            user_data = self.loader.get_dataset('user_data', ['fid', 'timestamp', 'deleted_at'])\n",
    "            if user_data is None or len(user_data) == 0:\n",
    "                return result\n",
    "                \n",
    "            # Process updates with strict null handling\n",
    "            valid_updates = (user_data\n",
    "                .filter(pl.col('deleted_at').is_null())\n",
    "                .filter(pl.col('timestamp').is_not_null())\n",
    "                .with_columns([\n",
    "                    pl.col('timestamp').cast(pl.Datetime).alias('timestamp')\n",
    "                ]))\n",
    "            \n",
    "            if len(valid_updates) == 0:\n",
    "                return result\n",
    "                            \n",
    "            update_metrics = (valid_updates\n",
    "                .sort(['fid', 'timestamp'])\n",
    "                .group_by('fid')\n",
    "                .agg([\n",
    "                    pl.len().alias('total_updates'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().mean().alias('avg_update_interval'),\n",
    "                    pl.col('timestamp').diff().dt.total_hours().std().alias('update_time_std')\n",
    "                ]))\n",
    "\n",
    "            # Ensure all columns are numeric and nulls are handled\n",
    "            update_metrics = update_metrics.with_columns([\n",
    "                pl.col('total_updates').cast(pl.Float64).fill_null(0),\n",
    "                pl.col('avg_update_interval').cast(pl.Float64).fill_null(0),\n",
    "                pl.col('update_time_std').cast(pl.Float64).fill_null(0)\n",
    "            ])\n",
    "\n",
    "            update_metrics = update_metrics.with_columns([\n",
    "                pl.when(pl.col('avg_update_interval') > 0)\n",
    "                .then(pl.col('update_time_std') / pl.col('avg_update_interval'))\n",
    "                .otherwise(0.0)\n",
    "                .alias('profile_update_consistency')\n",
    "            ])\n",
    "\n",
    "            # Join new features safely\n",
    "            update_metrics = update_metrics.unique(subset=['fid']) \n",
    "            result = result.join(update_metrics, on='fid', how='left')\n",
    "            \n",
    "            # Fill any remaining nulls\n",
    "            result = result.with_columns([\n",
    "                pl.col('total_updates').cast(pl.Float64).fill_null(0),\n",
    "                pl.col('avg_update_interval').cast(pl.Float64).fill_null(0),\n",
    "                pl.col('update_time_std').cast(pl.Float64).fill_null(0),\n",
    "                pl.col('profile_update_consistency').cast(pl.Float64).fill_null(0)\n",
    "            ])\n",
    "            \n",
    "            self.loader.clear_cache()\n",
    "            print(\"Update behavior features completed successfully\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in update behavior features: {str(e)}\")\n",
    "            raise\n",
    "            print(f\"Returning dataframe with default values\")\n",
    "            return df.with_columns([\n",
    "                pl.lit(0.0).alias('profile_update_consistency'),\n",
    "                pl.lit(0).alias('total_updates'),\n",
    "                pl.lit(0.0).alias('avg_update_interval'),\n",
    "                pl.lit(0.0).alias('update_time_std')\n",
    "            ])\n",
    "\n",
    "    def add_verification_patterns_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add verification patterns with safe calculations\"\"\"\n",
    "        try:\n",
    "            # Initialize with default columns\n",
    "            result = df.clone()\n",
    "            default_cols = {\n",
    "                'avg_hours_between_verifications': 0.0,\n",
    "                'std_hours_between_verifications': 0.0,\n",
    "                'rapid_verifications': 0,\n",
    "                'avg_hours_between_platform_verifs': 0.0,\n",
    "                'std_hours_between_platform_verifs': 0.0\n",
    "            }\n",
    "            \n",
    "            # Add on-chain verification patterns\n",
    "            verifications = self.loader.get_dataset('verifications', \n",
    "                ['fid', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            if verifications is not None and len(verifications) > 0:\n",
    "                valid_verifs = verifications.filter(pl.col('deleted_at').is_null())\n",
    "                \n",
    "                if len(valid_verifs) > 0:\n",
    "                    verif_patterns = (\n",
    "                        valid_verifs\n",
    "                        .with_columns(pl.col('timestamp').cast(pl.Datetime))\n",
    "                        .group_by('fid')\n",
    "                        .agg([\n",
    "                            pl.col('timestamp').diff().dt.total_hours().mean().fill_null(0)\n",
    "                                .alias('avg_hours_between_verifications'),\n",
    "                            pl.col('timestamp').diff().dt.total_hours().std().fill_null(0)\n",
    "                                .alias('std_hours_between_verifications'),\n",
    "                            (pl.col('timestamp').diff().dt.total_hours() < 1).sum().fill_null(0)\n",
    "                                .alias('rapid_verifications')\n",
    "                        ])\n",
    "                    )\n",
    "                    result = result.join(verif_patterns, on='fid', how='left')\n",
    "            \n",
    "            # Add platform verification patterns\n",
    "            acc_verifications = self.loader.get_dataset('account_verifications', \n",
    "                ['fid', 'verified_at'])\n",
    "            \n",
    "            if acc_verifications is not None and len(acc_verifications) > 0:\n",
    "                platform_patterns = (\n",
    "                    acc_verifications\n",
    "                    .with_columns(pl.col('verified_at').cast(pl.Datetime))\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        pl.col('verified_at').diff().dt.total_hours().mean().fill_null(0)\n",
    "                            .alias('avg_hours_between_platform_verifs'),\n",
    "                        pl.col('verified_at').diff().dt.total_hours().std().fill_null(0)\n",
    "                            .alias('std_hours_between_platform_verifs')\n",
    "                    ])\n",
    "                )\n",
    "                platform_patterns = platform_patterns.unique(subset=['fid']) \n",
    "                result = result.join(platform_patterns, on='fid', how='left')\n",
    "            \n",
    "            # Add any missing columns with defaults\n",
    "            for col, default in default_cols.items():\n",
    "                if col not in result.columns:\n",
    "                    result = result.with_columns(pl.lit(default).alias(col))\n",
    "                else:\n",
    "                    result = result.with_columns(pl.col(col).fill_null(default))\n",
    "            \n",
    "            self.loader.clear_cache()\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in verification patterns: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([pl.lit(v).alias(k) for k, v in default_cols.items()])\n",
    "    def _validate_required_columns(self, df: pl.DataFrame, required_cols: List[str]):\n",
    "        \"\"\"Validate required columns exist\"\"\"\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "            \n",
    "    def _get_feature_build_order(self):\n",
    "        \"\"\"Get correct feature build order based on dependencies\"\"\"\n",
    "        visited = set()\n",
    "        order = []\n",
    "        \n",
    "        def visit(name):\n",
    "            if name in visited:\n",
    "                return\n",
    "            visited.add(name)\n",
    "            feature_set = self.feature_sets[name]\n",
    "            for dep in feature_set.dependencies:\n",
    "                visit(dep)\n",
    "            order.append(name)\n",
    "        \n",
    "        for name in self.feature_sets:\n",
    "            visit(name)\n",
    "        return order\n",
    "\n",
    "\n",
    "    def _validate_feature_addition(self, original_df: pl.DataFrame, \n",
    "                                new_df: pl.DataFrame,\n",
    "                                base_fids: pl.Series,\n",
    "                                feature_name: str) -> pl.DataFrame:\n",
    "        \"\"\"Validate and fix feature addition results\"\"\"\n",
    "        if new_df is None:\n",
    "            print(f\"Error: {feature_name} returned None\")\n",
    "            raise\n",
    "            return original_df\n",
    "            \n",
    "        if len(new_df) != len(original_df):\n",
    "            print(f\"Warning: Shape mismatch in {feature_name}. Expected {len(original_df)}, got {len(new_df)}\")\n",
    "            new_df = new_df.filter(pl.col('fid').is_in(base_fids))\n",
    "            if len(new_df) != len(original_df):\n",
    "                return original_df\n",
    "                \n",
    "        # Cast numeric columns and handle nulls\n",
    "        new_cols = [c for c in new_df.columns if c not in original_df.columns]\n",
    "        if new_cols:\n",
    "            try:\n",
    "                new_df = new_df.with_columns([\n",
    "                    pl.col(c).cast(pl.Float64).fill_null(0) \n",
    "                    for c in new_cols \n",
    "                    if self._is_numeric_dtype(new_df[c].dtype)\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                print(f\"Error casting columns in {feature_name}: {str(e)}\")\n",
    "                raise\n",
    "                return original_df\n",
    "                \n",
    "        return new_df\n",
    "\n",
    "\n",
    "    def add_enhanced_channel_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add channel features with improved error handling\"\"\"\n",
    "        try:\n",
    "            # Prepare result DataFrame with default values\n",
    "            result = df.with_columns([\n",
    "                pl.lit(0).alias('unique_channels_followed'),\n",
    "                pl.lit(0).alias('rapid_channel_follows'),\n",
    "                pl.lit(0.0).alias('channel_follow_hour_std'),\n",
    "                pl.lit(0).alias('channel_memberships'),\n",
    "                pl.lit(0).alias('unique_channel_memberships'),\n",
    "                pl.lit(0.0).alias('channel_follow_burst_ratio'),\n",
    "                pl.lit(0.0).alias('channel_engagement_ratio')\n",
    "            ])\n",
    "            \n",
    "            # Process channel follows if available\n",
    "            channel_follows = self.loader.get_dataset('channel_follows', \n",
    "                ['fid', 'channel_id', 'timestamp', 'deleted_at'])\n",
    "            \n",
    "            if channel_follows is not None and len(channel_follows) > 0:\n",
    "                follow_features = (\n",
    "                    channel_follows.filter(pl.col('deleted_at').is_null())\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        pl.n_unique('channel_id').alias('unique_channels_followed'),\n",
    "                        (pl.col('timestamp').diff().dt.total_seconds() < 60)\n",
    "                            .sum().alias('rapid_channel_follows'),\n",
    "                        pl.col('timestamp').dt.hour().value_counts()\n",
    "                            .std().alias('channel_follow_hour_std')\n",
    "                    ])\n",
    "                )\n",
    "                # Join follow features safely\n",
    "                if len(follow_features) > 0:\n",
    "                    follow_features = follow_features.unique(subset=['fid']) \n",
    "                    result = result.join(follow_features, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "            # Process channel memberships if available\n",
    "            channel_members = self.loader.get_dataset('channel_members', \n",
    "                ['fid', 'channel_id', 'deleted_at'])\n",
    "            \n",
    "            if channel_members is not None and len(channel_members) > 0:\n",
    "                member_features = (\n",
    "                    channel_members.filter(pl.col('deleted_at').is_null())\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        pl.len().alias('channel_memberships'),\n",
    "                        pl.n_unique('channel_id').alias('unique_channel_memberships')\n",
    "                    ])\n",
    "                )\n",
    "                # Join member features safely\n",
    "                if len(member_features) > 0:\n",
    "                    member_features = member_features.unique(subset=['fid']) \n",
    "                    result = result.join(member_features, on='fid', how='left').fill_null(0)\n",
    "            \n",
    "            # Calculate derived metrics safely\n",
    "            result = result.with_columns([\n",
    "                (pl.col('rapid_channel_follows') / pl.col('unique_channels_followed').add(1))\n",
    "                    .alias('channel_follow_burst_ratio'),\n",
    "                (pl.col('channel_memberships') / pl.col('unique_channel_memberships').add(1))\n",
    "                    .alias('channel_engagement_ratio')\n",
    "            ])\n",
    "            \n",
    "            self.loader.clear_cache()\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in channel features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0).alias('unique_channels_followed'),\n",
    "                pl.lit(0).alias('rapid_channel_follows'),\n",
    "                pl.lit(0.0).alias('channel_follow_hour_std'),\n",
    "                pl.lit(0).alias('channel_memberships'),\n",
    "                pl.lit(0).alias('unique_channel_memberships'),\n",
    "                pl.lit(0.0).alias('channel_follow_burst_ratio'),\n",
    "                pl.lit(0.0).alias('channel_engagement_ratio')\n",
    "            ])\n",
    "\n",
    "    def add_engagement_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add engagement features with improved dependency handling\"\"\"\n",
    "        try:\n",
    "            print(\"Processing engagement features...\")\n",
    "            \n",
    "            # Initialize required columns with defaults\n",
    "            required_cols = {\n",
    "                'cast_count': 0,\n",
    "                'total_reactions': 0,\n",
    "                'channel_memberships': 0\n",
    "            }\n",
    "            \n",
    "            # Ensure base columns exist\n",
    "            result = df.clone()\n",
    "            for col, default in required_cols.items():\n",
    "                if col not in result.columns:\n",
    "                    result = result.with_columns(pl.lit(default).alias(col))\n",
    "                else:\n",
    "                    result = result.with_columns(pl.col(col).fill_null(default))\n",
    "            \n",
    "            # Calculate engagement metrics safely\n",
    "            result = result.with_columns([\n",
    "                # Overall engagement score\n",
    "                ((pl.col('cast_count') + \n",
    "                pl.col('total_reactions') + \n",
    "                pl.col('channel_memberships')) / 3.0\n",
    "                ).alias('engagement_score'),\n",
    "                \n",
    "                # Activity balance\n",
    "                (pl.col('cast_count') / pl.col('total_reactions').add(1))\n",
    "                    .alias('creation_consumption_ratio')\n",
    "            ])\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in engagement features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0.0).alias('engagement_score'),\n",
    "                pl.lit(0.0).alias('creation_consumption_ratio')\n",
    "            ])\n",
    "    def _validate_and_ensure_features(self, df: pl.DataFrame, \n",
    "                                required_features: Dict[str, float]) -> pl.DataFrame:\n",
    "        \"\"\"Enhanced feature validation with null handling\"\"\"\n",
    "        result = df.clone()\n",
    "        \n",
    "        for feature, default_value in required_features.items():\n",
    "            if feature not in result.columns:\n",
    "                print(f\"Adding missing feature {feature} with default value {default_value}\")\n",
    "                result = result.with_columns(pl.lit(default_value).alias(feature))\n",
    "            else:\n",
    "                result = result.with_columns(\n",
    "                    pl.when(pl.col(feature).is_null())\n",
    "                    .then(pl.lit(default_value))\n",
    "                    .otherwise(pl.col(feature))\n",
    "                    .alias(feature)\n",
    "                )\n",
    "        \n",
    "        return result\n",
    "    def _load_checkpoint(self, feature_set: FeatureSet, base_fids: pl.Series) -> pl.DataFrame:\n",
    "        \"\"\"Enhanced checkpoint loading with proper list type handling\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading checkpoint: {feature_set.checkpoint_path}\")\n",
    "            checkpoint_df = pl.read_parquet(feature_set.checkpoint_path)\n",
    "            checkpoint_df = checkpoint_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "\n",
    "            # Handle each column based on its type\n",
    "            for col in checkpoint_df.columns:\n",
    "                if col == 'fid':\n",
    "                    continue\n",
    "                    \n",
    "                dtype_str = str(checkpoint_df[col].dtype).lower()\n",
    "                \n",
    "                # Skip list type columns\n",
    "                if 'list' in dtype_str:\n",
    "                    continue\n",
    "                    \n",
    "                # Handle numeric columns\n",
    "                if any(num_type in dtype_str for num_type in ['int', 'float', 'decimal']):\n",
    "                    checkpoint_df = checkpoint_df.with_columns([\n",
    "                        pl.col(col).cast(pl.Float64).fill_null(0)\n",
    "                    ])\n",
    "\n",
    "            # Debug info\n",
    "            print(f\"Checkpoint fid type: {checkpoint_df['fid'].dtype}\")\n",
    "            print(f\"Base fids type: {base_fids.dtype}\")\n",
    "            \n",
    "            # Force consistent FID types\n",
    "            checkpoint_df = checkpoint_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "            base_fids = base_fids.cast(pl.Int64)\n",
    "            \n",
    "            # Filter to base_fids\n",
    "            filtered_df = checkpoint_df.filter(pl.col('fid').is_in(base_fids))\n",
    "            print(f\"Filtered checkpoint from {len(checkpoint_df)} to {len(filtered_df)} rows\")\n",
    "            \n",
    "            # Special handling for certain feature sets\n",
    "            if feature_set.name in ['authenticity', 'update_behavior']:\n",
    "                filtered_df = self._validate_sensitive_checkpoint(filtered_df, feature_set.name)\n",
    "                \n",
    "            return filtered_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _validate_checkpoint_compatibility(self, checkpoint_df: pl.DataFrame, \n",
    "                                        base_fids: pl.Series) -> bool:\n",
    "        \"\"\"Validate checkpoint compatibility with list type handling\"\"\"\n",
    "        try:\n",
    "            if checkpoint_df is None or len(checkpoint_df) == 0:\n",
    "                return False\n",
    "                \n",
    "            # Verify FID column exists and is correct type\n",
    "            if 'fid' not in checkpoint_df.columns:\n",
    "                return False\n",
    "                \n",
    "            checkpoint_fids = checkpoint_df['fid'].cast(pl.Int64)\n",
    "            base_fids = base_fids.cast(pl.Int64)\n",
    "            \n",
    "            # Verify all base FIDs are present\n",
    "            missing_fids = pl.Series(np.setdiff1d(base_fids, checkpoint_fids))\n",
    "            if len(missing_fids) > 0:\n",
    "                print(f\"Missing FIDs in checkpoint: {missing_fids}\")\n",
    "                return False\n",
    "                \n",
    "            # Verify column types\n",
    "            for col in checkpoint_df.columns:\n",
    "                if col == 'fid':\n",
    "                    continue\n",
    "                    \n",
    "                dtype_str = str(checkpoint_df[col].dtype).lower()\n",
    "                # Skip validation for list type columns\n",
    "                if 'list' in dtype_str:\n",
    "                    continue\n",
    "                    \n",
    "                # Validate numeric columns\n",
    "                if any(num_type in dtype_str for num_type in ['int', 'float', 'decimal']):\n",
    "                    try:\n",
    "                        # Test if we can cast to Float64\n",
    "                        checkpoint_df.select(pl.col(col).cast(pl.Float64))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Column {col} failed type validation: {str(e)}\")\n",
    "                        return False\n",
    "                        \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error validating checkpoint compatibility: {str(e)}\")\n",
    "            return False\n",
    "    def add_nindexer_enhanced_network_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add enhanced network features with scalar aggregations\"\"\"\n",
    "        try:\n",
    "            follows = self.loader.get_dataset('follows', \n",
    "                ['fid', 'target_fid', 'timestamp', 'created_at', 'deleted_at'], \n",
    "                source=\"nindexer\")\n",
    "            follow_counts = self.loader.get_dataset('follow_counts',\n",
    "                ['fid', 'follower_count', 'following_count', 'created_at'], \n",
    "                source=\"nindexer\")\n",
    "            \n",
    "            if follows is not None and len(follows) > 0:\n",
    "                valid_follows = follows.filter(pl.col('deleted_at').is_null())\n",
    "                \n",
    "                follow_metrics = (valid_follows\n",
    "                    .with_columns([\n",
    "                        pl.col('timestamp').cast(pl.Datetime),\n",
    "                        pl.col('created_at').cast(pl.Datetime)\n",
    "                    ])\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        (pl.col('timestamp').max() - pl.col('timestamp').min())\n",
    "                            .dt.total_hours()\n",
    "                            .cast(pl.Float64)\n",
    "                            .alias('network_age_hours'),\n",
    "                        pl.len().alias('total_follows'),\n",
    "                        (pl.col('created_at') - pl.col('timestamp'))\n",
    "                            .dt.total_seconds()\n",
    "                            .mean()\n",
    "                            .cast(pl.Float64)\n",
    "                            .alias('avg_follow_latency_seconds')\n",
    "                    ])\n",
    "                    .with_columns([\n",
    "                        (pl.col('total_follows') / \n",
    "                        (pl.col('network_age_hours') + 1))\n",
    "                        .alias('follow_rate_per_hour')\n",
    "                    ]))\n",
    "                \n",
    "                if follow_counts is not None and len(follow_counts) > 0:\n",
    "                    count_metrics = (follow_counts\n",
    "                        .sort('created_at')  # Sort to ensure last() gets most recent\n",
    "                        .group_by('fid')\n",
    "                        .agg([\n",
    "                            pl.col('follower_count')\n",
    "                                .last()\n",
    "                                .cast(pl.Float64)\n",
    "                                .alias('latest_follower_count'),\n",
    "                            pl.col('following_count')\n",
    "                                .last()\n",
    "                                .cast(pl.Float64)\n",
    "                                .alias('latest_following_count')\n",
    "                        ])\n",
    "                        .with_columns([\n",
    "                            (pl.col('latest_follower_count') / \n",
    "                            (pl.col('latest_following_count') + 1))\n",
    "                            .alias('latest_follow_ratio')\n",
    "                        ]))\n",
    "                    \n",
    "                    result = df.join(follow_metrics, on='fid', how='left')\n",
    "                    result = result.join(count_metrics, on='fid', how='left')\n",
    "                else:\n",
    "                    result = df.join(follow_metrics, on='fid', how='left')\n",
    "                \n",
    "                return result.fill_null(0)\n",
    "                \n",
    "            return df.with_columns([\n",
    "                pl.lit(0.0).alias('network_age_hours'),\n",
    "                pl.lit(0.0).alias('total_follows'),\n",
    "                pl.lit(0.0).alias('follow_rate_per_hour'),\n",
    "                pl.lit(0.0).alias('avg_follow_latency_seconds'),\n",
    "                pl.lit(0.0).alias('latest_follower_count'),\n",
    "                pl.lit(0.0).alias('latest_following_count'),\n",
    "                pl.lit(0.0).alias('latest_follow_ratio')\n",
    "            ])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in enhanced network features: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def add_nindexer_enhanced_profile_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add enhanced profile features with scalar aggregations\"\"\"\n",
    "        try:\n",
    "            profiles = self.loader.get_dataset('profiles', \n",
    "                ['fid', 'bio', 'pfp_url', 'url', 'username', \n",
    "                'location', 'created_at', 'updated_at'], \n",
    "                source=\"nindexer\")\n",
    "            \n",
    "            if profiles is not None and len(profiles) > 0:\n",
    "                profile_metrics = (profiles\n",
    "                    .with_columns([\n",
    "                        pl.col('created_at').cast(pl.Datetime),\n",
    "                        pl.col('updated_at').cast(pl.Datetime),\n",
    "                        pl.col('url').is_not_null().cast(pl.Int32).alias('has_url'),\n",
    "                        pl.col('location').is_not_null().cast(pl.Int32).alias('has_location')\n",
    "                    ])\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        # Ensure scalar sum\n",
    "                        (pl.col('has_url') + pl.col('has_location'))\n",
    "                            .cast(pl.Float64)\n",
    "                            .alias('additional_profile_fields'),\n",
    "                        (pl.col('updated_at').max() - pl.col('created_at').min())\n",
    "                            .dt.total_hours()\n",
    "                            .cast(pl.Float64)\n",
    "                            .alias('profile_age_hours'),\n",
    "                        pl.col('location')\n",
    "                            .first()\n",
    "                            .alias('location')\n",
    "                    ]))\n",
    "                \n",
    "                result = df.join(profile_metrics, on='fid', how='left')\n",
    "                \n",
    "                if result.select(pl.col('location').is_not_null().sum()).item() > 0:\n",
    "                    result = result.with_columns([\n",
    "                        pl.col('location')\n",
    "                            .is_not_null()\n",
    "                            .cast(pl.Int32)\n",
    "                            .alias('has_location_info')\n",
    "                    ])\n",
    "                \n",
    "                return result.fill_null(0)\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in enhanced profile features: {str(e)}\")\n",
    "            raise\n",
    "    def add_nindexer_neynar_score_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add Neynar score features and correlations with proper type handling\"\"\"\n",
    "        try:\n",
    "            scores = self.loader.get_dataset('neynar_user_scores', \n",
    "                ['fid', 'score', 'created_at'], source=\"nindexer\")\n",
    "            \n",
    "            if scores is not None and len(scores) > 0:\n",
    "                # Get latest scores per user and ensure we're dealing with scalar values\n",
    "                score_features = (scores\n",
    "                    .with_columns([\n",
    "                        pl.col('created_at').cast(pl.Datetime),\n",
    "                        # Ensure score is handled as a scalar\n",
    "                        pl.when(pl.col('score').is_null())\n",
    "                        .then(0.0)\n",
    "                        .otherwise(pl.col('score'))\n",
    "                        .alias('score')\n",
    "                    ])\n",
    "                    .group_by('fid')\n",
    "                    .agg([\n",
    "                        # Latest score\n",
    "                        pl.col('score').last().alias('neynar_score'),\n",
    "                        # Average score over time\n",
    "                        pl.col('score').mean().alias('avg_neynar_score'),\n",
    "                        # Score stability\n",
    "                        pl.col('score').std().alias('neynar_score_std'),\n",
    "                        # Score trend (positive or negative)\n",
    "                        (pl.col('score').last() - pl.col('score').first()).alias('score_trend')\n",
    "                    ]))\n",
    "                \n",
    "                result = df.join(score_features, on='fid', how='left')\n",
    "                \n",
    "                # Calculate correlation with authenticity score if it exists\n",
    "                if 'authenticity_score' in result.columns:\n",
    "                    result = result.with_columns([\n",
    "                        # Safely calculate score difference\n",
    "                        (pl.col('neynar_score').cast(pl.Float64) - \n",
    "                        pl.col('authenticity_score').cast(pl.Float64))\n",
    "                        .abs()\n",
    "                        .alias('score_divergence'),\n",
    "                        \n",
    "                        # Calculate relative score difference\n",
    "                        ((pl.col('neynar_score').cast(pl.Float64) - \n",
    "                        pl.col('authenticity_score').cast(pl.Float64)) /\n",
    "                        (pl.col('authenticity_score').cast(pl.Float64) + 1e-6))\n",
    "                        .alias('relative_score_diff')\n",
    "                    ])\n",
    "                \n",
    "                # Fill any remaining nulls with 0\n",
    "                result = result.with_columns([\n",
    "                    pl.col('neynar_score').fill_null(0.0),\n",
    "                    pl.col('avg_neynar_score').fill_null(0.0),\n",
    "                    pl.col('neynar_score_std').fill_null(0.0),\n",
    "                    pl.col('score_trend').fill_null(0.0)\n",
    "                ])\n",
    "                \n",
    "                if 'score_divergence' in result.columns:\n",
    "                    result = result.with_columns([\n",
    "                        pl.col('score_divergence').fill_null(0.0),\n",
    "                        pl.col('relative_score_diff').fill_null(0.0)\n",
    "                    ])\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in neynar score features: {str(e)}\")\n",
    "            # Return original dataframe with default columns if error occurs\n",
    "            return df.with_columns([\n",
    "                pl.lit(0.0).alias('neynar_score'),\n",
    "                pl.lit(0.0).alias('avg_neynar_score'),\n",
    "                pl.lit(0.0).alias('neynar_score_std'),\n",
    "                pl.lit(0.0).alias('score_trend')\n",
    "            ])\n",
    "    def add_name_pattern_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add name pattern features with improved error handling\"\"\"\n",
    "        try:\n",
    "            print(\"Building name pattern features...\")\n",
    "            \n",
    "            # Initialize with default values\n",
    "            result = df.clone().with_columns([\n",
    "                pl.lit(0).alias('random_numbers'),\n",
    "                pl.lit(0).alias('wallet_pattern'),\n",
    "                pl.lit(0).alias('excessive_symbols'),\n",
    "                pl.lit(0).alias('airdrop_terms'),\n",
    "                pl.lit(0).alias('has_year')\n",
    "            ])\n",
    "\n",
    "            result = result.with_columns([\n",
    "                    pl.col('fname').map_elements(self._analyze_name_patterns, return_dtype=pl.Utf8).alias('fname_content_patterns'),\n",
    "                    pl.col('bio').map_elements(self._analyze_name_patterns, return_dtype=pl.Utf8).alias('bio_content_patterns'),\n",
    "                ])\n",
    "\n",
    "            result = result.with_columns([\n",
    "                    (pl.col('fname_content_patterns').map_elements(lambda x: x['fname_random_numbers'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('fname_random_numbers'),\n",
    "                    (pl.col('fname_content_patterns').map_elements(lambda x: x['fname_wallet_pattern'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('fname_wallet_pattern'),\n",
    "                    (pl.col('fname_content_patterns').map_elements(lambda x: x['fname_excessive_symbols'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('fname_excessive_symbols'),\n",
    "                    (pl.col('fname_content_patterns').map_elements(lambda x: x['fname_airdrop_terms'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('fname_airdrop_terms'),\n",
    "                    (pl.col('fname_content_patterns').map_elements(lambda x: x['fname_has_year'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('fname_has_year'),\n",
    "                    (pl.col('bio_content_patterns').map_elements(lambda x: x['bio_random_numbers'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('bio_random_numbers'),\n",
    "                    (pl.col('bio_content_patterns').map_elements(lambda x: x['bio_wallet_pattern'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('bio_wallet_pattern'),\n",
    "                    (pl.col('bio_content_patterns').map_elements(lambda x: x['bio_excessive_symbols'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('bio_excessive_symbols'),\n",
    "                    (pl.col('bio_content_patterns').map_elements(lambda x: x['bio_airdrop_terms'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('bio_airdrop_terms'),\n",
    "                    (pl.col('bio_content_patterns').map_elements(lambda x: x['bio_has_year'], return_dtype=pl.Float64).sum() / pl.len())\n",
    "                        .alias('bio_has_year'),\n",
    "            ])\n",
    "\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in name pattern features: {str(e)}\")\n",
    "            raise\n",
    "            return df.with_columns([\n",
    "                pl.lit(0).alias('random_numbers'),\n",
    "                pl.lit(0).alias('wallet_pattern'),\n",
    "                pl.lit(0).alias('excessive_symbols'),\n",
    "                pl.lit(0).alias('airdrop_terms'),\n",
    "                pl.lit(0).alias('has_year')\n",
    "            ])\n",
    "\n",
    "    def build_feature_matrix(self) -> pl.DataFrame:\n",
    "        \"\"\"Build feature matrix with enhanced safety checks while maintaining existing functionality\"\"\"\n",
    "        print(\"Starting feature extraction...\")\n",
    "        \n",
    "        try:\n",
    "            # Load or build profile features\n",
    "            if self._needs_rebuild(self.feature_sets['profile']):\n",
    "                print(\"Building profile features...\")\n",
    "                df = self.extract_profile_features()\n",
    "            else:\n",
    "                print(\"Loading profile features from checkpoint...\")\n",
    "                df = pl.read_parquet(self.feature_sets['profile'].checkpoint_path)\n",
    "            \n",
    "            # Setup base configuration\n",
    "            df = df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "            base_fids = df['fid'].cast(pl.Int64).unique()\n",
    "            df = df.filter(pl.col('fid').is_in(base_fids))\n",
    "            self.loader.set_base_fids(base_fids)\n",
    "            initial_cols = df.columns\n",
    "            print(f\"Base shape: {df.shape}\")\n",
    "\n",
    "            # Define dependencies between features\n",
    "            dependencies = {\n",
    "                'engagement': ['cast', 'reaction', 'channel'],\n",
    "                'network_quality': ['network', 'engagement'],\n",
    "                'activity_patterns': ['temporal', 'cast', 'reaction'],\n",
    "                'mentions': ['cast'],\n",
    "                'reply_patterns': ['cast'],\n",
    "                'update_behavior': ['user_data'],\n",
    "                'verification_patterns': ['verification'],\n",
    "                'authenticity': ['profile', 'network', 'verification', 'engagement']\n",
    "            }\n",
    "\n",
    "            # Track successfully built features\n",
    "            built_features = {'profile'}\n",
    "            \n",
    "            feature_sequence = [\n",
    "                ('network', self.add_network_features),\n",
    "                ('temporal', self.add_temporal_features),\n",
    "                ('cast', self.add_cast_behavior_features),\n",
    "                ('reaction', self.add_reaction_patterns),\n",
    "                ('channel', self.add_enhanced_channel_features),\n",
    "                ('user_data', self.add_user_data_features),\n",
    "                ('verification', self.add_enhanced_verification_features),\n",
    "                ('engagement', self.add_engagement_features),\n",
    "                ('network_quality', self.build_network_quality_features),\n",
    "                ('activity_patterns', self.add_activity_patterns_features),\n",
    "                ('influence', self.add_influence_features),\n",
    "                ('mentions', self.add_mentions_features),\n",
    "                ('reply_patterns', self.add_reply_patterns_features),\n",
    "                ('power_user_interaction', self.add_power_user_interaction_features),\n",
    "                # ('cluster_analysis', self.add_cluster_analysis_features),\n",
    "                ('update_behavior', self.add_update_behavior_features),\n",
    "                ('verification_patterns', self.add_verification_patterns_features),\n",
    "                ('authenticity', self.add_authenticity_features),\n",
    "                ('storage', self.add_storage_features),\n",
    "                ('derived', self._add_derived_features),\n",
    "                ('enhanced_network', self.add_nindexer_enhanced_network_features),\n",
    "                ('enhanced_profile', self.add_nindexer_enhanced_profile_features),\n",
    "                ('neynar_score', self.add_nindexer_neynar_score_features),\n",
    "                ('name_patterns', self.add_name_pattern_features),\n",
    "                # ('content_patterns', self.add_cast_behavior_features),  # Modified version\n",
    "                # ('advanced_temporal', self.add_advanced_temporal_features),\n",
    "                # ('reward_gaming', self.add_reward_gaming_features),\n",
    "                # ('engagement_authenticity', self.add_engagement_authenticity_features)\n",
    "            ]\n",
    "\n",
    "            for feature_name, feature_func in feature_sequence:\n",
    "                feature_set = self.feature_sets[feature_name]\n",
    "                current_cols = set(df.columns)\n",
    "                \n",
    "                try:\n",
    "                    # Check if dependencies are met\n",
    "                    should_rebuild = self._needs_rebuild(feature_set)\n",
    "                    if feature_name in dependencies:\n",
    "                        deps = dependencies[feature_name]\n",
    "                        missing_deps = [dep for dep in deps if dep not in built_features]\n",
    "                        if missing_deps:\n",
    "                            print(f\"Missing dependencies for {feature_name}: {missing_deps}\")\n",
    "                            print(f\"Currently built features: {built_features}\")\n",
    "                            should_rebuild = True\n",
    "\n",
    "                    if should_rebuild:\n",
    "                        print(f\"Building {feature_name} features...\")\n",
    "                        new_df = feature_func(df)\n",
    "                        \n",
    "                        if new_df is not None:\n",
    "                            # Validate and safely join new features\n",
    "                            new_df = self._validate_checkpoint(new_df, feature_name)\n",
    "                            new_df = new_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "                            \n",
    "                            # Only save and update if validation passes\n",
    "                            if self._validate_checkpoint_compatibility(new_df, base_fids):\n",
    "                                self._save_checkpoint(new_df, feature_set)\n",
    "                                df = self._safe_join_features(df, new_df, feature_name)\n",
    "                                built_features.add(feature_name)\n",
    "                                print(f\"Successfully built and saved {feature_name}\")\n",
    "                    else:\n",
    "                        print(f\"Loading {feature_name} features from checkpoint...\")\n",
    "                        checkpoint_df = self._load_checkpoint(feature_set, base_fids)\n",
    "                        \n",
    "                        if checkpoint_df is not None:\n",
    "                            new_cols = [c for c in checkpoint_df.columns if c not in current_cols]\n",
    "                            if new_cols:\n",
    "                                print(f\"Adding {len(new_cols)} new columns from {feature_name}\")\n",
    "                                # Use safe join for checkpoint data too\n",
    "                                df = self._safe_join_features(\n",
    "                                    df,\n",
    "                                    checkpoint_df.select(['fid'] + new_cols),\n",
    "                                    feature_name\n",
    "                                )\n",
    "                                built_features.add(feature_name)\n",
    "                        else:\n",
    "                            print(f\"Failed to load {feature_name} checkpoint, forcing rebuild...\")\n",
    "                            new_df = feature_func(df)\n",
    "                            if new_df is not None:\n",
    "                                new_df = self._validate_checkpoint(new_df, feature_name)\n",
    "                                new_df = new_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "                                self._save_checkpoint(new_df, feature_set)\n",
    "                                df = self._safe_join_features(df, new_df, feature_name)\n",
    "                                built_features.add(feature_name)\n",
    "                    \n",
    "                    print(f\"Shape after {feature_name}: {df.shape}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in {feature_name}: {str(e)}\")\n",
    "                    raise\n",
    "                    continue\n",
    "\n",
    "            # Final validation\n",
    "            df = df.fill_null(0)\n",
    "            df = df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "            \n",
    "            # self.verify_matrix(df)\n",
    "\n",
    "\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Critical error: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def _validate_checkpoint(self, df: pl.DataFrame, name: str) -> pl.DataFrame:\n",
    "        \"\"\"Validate checkpoint data types and ensure type consistency with list handling\"\"\"\n",
    "        try:\n",
    "            # Always ensure fid is Int64 first\n",
    "            if 'fid' in df.columns:\n",
    "                df = df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "                \n",
    "            # Cast numeric columns and handle nulls, excluding list types\n",
    "            numeric_cols = []\n",
    "            for col in df.columns:\n",
    "                if col != 'fid':\n",
    "                    dtype_str = str(df[col].dtype).lower()\n",
    "                    # Check if it's a list type\n",
    "                    if 'list' in dtype_str:\n",
    "                        continue\n",
    "                    # Check if it's a numeric type\n",
    "                    if any(num_type in dtype_str for num_type in ['int', 'float', 'decimal']):\n",
    "                        numeric_cols.append(col)\n",
    "            \n",
    "            if numeric_cols:\n",
    "                df = df.with_columns([\n",
    "                    pl.col(col).cast(pl.Float64).fill_null(0) \n",
    "                    for col in numeric_cols\n",
    "                ])\n",
    "            \n",
    "            return df\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error validating checkpoint {name}: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "    def _is_numeric_dtype(self, dtype) -> bool:\n",
    "        \"\"\"Check if a Polars dtype is numeric, excluding list types\"\"\"\n",
    "        # Convert dtype to string for comparison\n",
    "        dtype_str = str(dtype).lower()\n",
    "        # Exclude list types\n",
    "        if 'list' in dtype_str:\n",
    "            return False\n",
    "        return any(num_type in dtype_str \n",
    "                for num_type in ['int', 'float', 'decimal'])\n",
    "\n",
    "    def _safe_join_features(self, df: pl.DataFrame, \n",
    "                        new_features: pl.DataFrame,\n",
    "                        feature_name: str) -> pl.DataFrame:\n",
    "        \"\"\"Enhanced safe join features with comprehensive null and list handling\"\"\"\n",
    "        try:\n",
    "            if new_features is None or len(new_features) == 0:\n",
    "                print(f\"No valid features to join for {feature_name}\")\n",
    "                return df\n",
    "\n",
    "            # Get new columns\n",
    "            existing_cols = set(df.columns)\n",
    "            new_cols = [c for c in new_features.columns \n",
    "                    if c != 'fid' and c not in existing_cols]\n",
    "                    \n",
    "            if not new_cols:\n",
    "                print(f\"No new columns to add from {feature_name}\")\n",
    "                return df\n",
    "                \n",
    "            # Handle nulls in new features before join\n",
    "            safe_features = new_features.clone()\n",
    "            for col in new_cols:\n",
    "                dtype_str = str(new_features[col].dtype).lower()\n",
    "                if 'list' in dtype_str:\n",
    "                    # For list columns, replace null with empty list\n",
    "                    safe_features = safe_features.with_columns(\n",
    "                        pl.col(col).fill_null([])\n",
    "                    )\n",
    "                elif self._is_numeric_dtype(new_features[col].dtype):\n",
    "                    # For numeric columns, fill null with 0\n",
    "                    safe_features = safe_features.with_columns(\n",
    "                        pl.col(col).fill_null(0.0)\n",
    "                    )\n",
    "            \n",
    "            # Join with guaranteed FID type consistency\n",
    "            safe_features = safe_features.unique(subset=['fid']) \n",
    "            result = df.join(\n",
    "                safe_features.select(['fid'] + new_cols)\n",
    "                .with_columns(pl.col('fid').cast(pl.Int64)),\n",
    "                on='fid',\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # Handle any new nulls that appeared after join\n",
    "            for col in new_cols:\n",
    "                dtype_str = str(result[col].dtype).lower()\n",
    "                if 'list' in dtype_str:\n",
    "                    result = result.with_columns(\n",
    "                        pl.col(col).fill_null([])\n",
    "                    )\n",
    "                elif self._is_numeric_dtype(result[col].dtype):\n",
    "                    result = result.with_columns(\n",
    "                        pl.col(col).fill_null(0.0)\n",
    "                    )\n",
    "                            \n",
    "            return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error joining {feature_name}: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "    def _validate_feature_dependencies(self, feature_name: str, \n",
    "                                built_features: set) -> bool:\n",
    "        \"\"\"Validate feature dependencies are met\"\"\"\n",
    "        if feature_name not in self.feature_sets:\n",
    "            return False\n",
    "            \n",
    "        feature_set = self.feature_sets[feature_name]\n",
    "        for dep in feature_set.dependencies:\n",
    "            if dep not in built_features:\n",
    "                print(f\"Missing dependency {dep} for {feature_name}\")\n",
    "                return False\n",
    "                \n",
    "        return True\n",
    "    def _save_checkpoint(self, df: pl.DataFrame, feature_set: 'FeatureSet'):\n",
    "        \"\"\"Save feature checkpoint with validation\"\"\"\n",
    "        # Validate before saving\n",
    "        df = self._validate_checkpoint(df, feature_set.name)\n",
    "        df.write_parquet(feature_set.checkpoint_path)\n",
    "        feature_set.last_modified = os.path.getmtime(feature_set.checkpoint_path)\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_sensitive_checkpoint(self, df: pl.DataFrame, feature_name: str) -> pl.DataFrame:\n",
    "        \"\"\"Additional validation for sensitive features\"\"\"\n",
    "        try:\n",
    "            # Initialize sensitive columns with safe defaults\n",
    "            sensitive_defaults = {\n",
    "                'authenticity': {\n",
    "                    'authenticity_score': 0.0,\n",
    "                    'profile_completeness': 0.0,\n",
    "                    'network_balance': 0.0,\n",
    "                    'update_naturalness': 0.0\n",
    "                },\n",
    "                'update_behavior': {\n",
    "                    'profile_update_consistency': 0.0,\n",
    "                    'total_updates': 0,\n",
    "                    'avg_update_interval': 0.0,\n",
    "                    'update_time_std': 0.0\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            if feature_name in sensitive_defaults:\n",
    "                for col, default in sensitive_defaults[feature_name].items():\n",
    "                    if col in df.columns:\n",
    "                        df = df.with_columns(pl.col(col).fill_null(default))\n",
    "                    else:\n",
    "                        df = df.with_columns(pl.lit(default).alias(col))\n",
    "                        \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error validating sensitive checkpoint {feature_name}: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "    def _add_derived_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add derived features with comprehensive null handling\"\"\"\n",
    "        try:\n",
    "            print(\"Building derived features...\")\n",
    "            result = df.clone()\n",
    "            \n",
    "            # Ensure required columns exist\n",
    "            required_cols = {\n",
    "                'following_count': 0.0,\n",
    "                'follower_count': 0.0,\n",
    "                'follower_ratio': 0.0,\n",
    "                'unique_follower_ratio': 0.0,\n",
    "                'follow_velocity': 0.0\n",
    "            }\n",
    "            \n",
    "            # Initialize missing columns\n",
    "            for col, default in required_cols.items():\n",
    "                if col not in result.columns:\n",
    "                    print(f\"Adding missing column {col} with default {default}\")\n",
    "                    result = result.with_columns(pl.lit(default).alias(col))\n",
    "                \n",
    "                # Fill nulls with defaults\n",
    "                result = result.with_columns(\n",
    "                    pl.col(col).fill_null(default).alias(col)\n",
    "                )\n",
    "            \n",
    "            # Safe calculations with explicit null handling\n",
    "            result = result.with_columns([\n",
    "                # Log transformations with null safety\n",
    "                pl.col('follower_ratio').fill_null(0.0).log1p().alias('follower_ratio_log'),\n",
    "                pl.col('unique_follower_ratio').fill_null(0.0).log1p().alias('unique_follower_ratio_log'),\n",
    "                pl.col('follow_velocity').fill_null(0.0).log1p().alias('follow_velocity_log'),\n",
    "                \n",
    "                # Binary flags with safe comparisons\n",
    "                (pl.when(pl.col('follower_count').fill_null(0) > pl.col('following_count').fill_null(0))\n",
    "                .then(1)\n",
    "                .otherwise(0)\n",
    "                ).alias('has_more_followers'),\n",
    "                \n",
    "                # Balance ratios with safe division\n",
    "                ((pl.col('following_count').fill_null(0) - pl.col('follower_count').fill_null(0)).abs() / \n",
    "                (pl.col('following_count').fill_null(0) + pl.col('follower_count').fill_null(0) + 1)\n",
    "                ).alias('follow_balance_ratio')\n",
    "            ])\n",
    "            \n",
    "            # Cap extreme values with safe operations\n",
    "            for col in ['follower_ratio', 'unique_follower_ratio', 'follow_velocity']:\n",
    "                if col in result.columns:\n",
    "                    safe_col = pl.col(col).fill_null(0.0)\n",
    "                    p99 = result.select(safe_col.quantile(0.99)).item()\n",
    "                    result = result.with_columns([\n",
    "                        safe_col.clip(0.0, p99).alias(f'{col}_capped')\n",
    "                    ])\n",
    "            \n",
    "            print(\"Derived features completed successfully\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in derived features: {str(e)}\")\n",
    "            raise\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook code\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score,\n",
    "    precision_recall_curve, precision_score, recall_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import polars as pl\n",
    "\n",
    "class SybilDetectionSystem:\n",
    "    def __init__(self, \n",
    "                 feature_engineering: 'FeatureEngineering',\n",
    "                 confidence_thresholds: Dict[str, float] = None,\n",
    "                 authenticity_thresholds: Dict[str, float] = None):\n",
    "        self.feature_engineering = feature_engineering\n",
    "        self.model = None\n",
    "        self.feature_names = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.confidence_thresholds = confidence_thresholds or {\n",
    "            'high': 0.95,\n",
    "            'medium': 0.85,\n",
    "            'low': 0.70\n",
    "        }\n",
    "        self.authenticity_thresholds = authenticity_thresholds or {\n",
    "            'high': 0.8,\n",
    "            'medium': 0.6,\n",
    "            'low': 0.4\n",
    "        }\n",
    "        self.feature_importance = {}\n",
    "        self.shap_values = {}\n",
    "        self.base_models = {}\n",
    "        self.shap_explainers = {}\n",
    "            \n",
    "    def prepare_features(self, df: pl.DataFrame, scale: bool = False) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"Prepare features with comprehensive feature selection and validation\"\"\"\n",
    "        try:\n",
    "            # Define feature groups\n",
    "\n",
    "            valid_cols = [col for col in df.columns if \n",
    "                        df[col].dtype in [pl.Float64, pl.Int64] or\n",
    "                        str(df[col].dtype).startswith(('Float', 'Int'))]\n",
    "            \n",
    "            print(f\"\\nTotal numeric features available: {len(valid_cols)}\")\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            features = df.select(valid_cols).fill_null(0)\n",
    "            for col in valid_cols:\n",
    "                col_dtype = str(features[col].dtype)\n",
    "                \n",
    "                if col_dtype.startswith('list') or col_dtype.startswith('List'):\n",
    "                    print(f\"Converting list column {col} to length feature\")\n",
    "                    features = features.with_columns([\n",
    "                        pl.when(pl.col(col).is_null())\n",
    "                        .then(0)\n",
    "                        .otherwise(pl.col(col).list.len())\n",
    "                        .alias(col)\n",
    "                    ])\n",
    "            \n",
    "            # Handle infinite values and extreme outliers\n",
    "            for col in valid_cols:\n",
    "                col_stats = features.select(\n",
    "                    pl.col(col).quantile(0.01).alias('q01'),\n",
    "                    pl.col(col).quantile(0.99).alias('q99'),\n",
    "                    pl.col(col).mean().alias('mean'),\n",
    "                    pl.col(col).std().alias('std')\n",
    "                )\n",
    "                \n",
    "                q01 = col_stats['q01'][0]\n",
    "                q99 = col_stats['q99'][0]\n",
    "                mean_val = col_stats['mean'][0]\n",
    "                std_val = col_stats['std'][0]\n",
    "                \n",
    "                # Define reasonable bounds for the column\n",
    "                lower_bound = max(q01, mean_val - 3 * std_val)\n",
    "                upper_bound = min(q99, mean_val + 3 * std_val)\n",
    "                \n",
    "                # Clip values to bounds and replace infinities\n",
    "                features = features.with_columns([\n",
    "                    pl.when(pl.col(col).is_infinite())\n",
    "                    .then(pl.lit(None))\n",
    "                    .otherwise(pl.col(col))\n",
    "                    .alias(col)\n",
    "                ])\n",
    "                \n",
    "                features = features.with_columns([\n",
    "                    pl.col(col).clip(lower_bound, upper_bound).alias(col)\n",
    "                ])\n",
    "                \n",
    "                # Fill remaining nulls with median\n",
    "                median_val = features.select(pl.col(col).median())[0][0]\n",
    "                features = features.with_columns([\n",
    "                    pl.col(col).fill_null(median_val).alias(col)\n",
    "                ])\n",
    "                \n",
    "                # Convert to numeric if needed\n",
    "                if features[col].dtype not in [pl.Float64, pl.Int64]:\n",
    "                    features = features.with_columns([\n",
    "                        pl.col(col).cast(pl.Float64).alias(col)\n",
    "                    ])\n",
    "\n",
    "            # Convert to numpy array\n",
    "            feature_array = features.to_numpy()\n",
    "            \n",
    "            if scale:\n",
    "                feature_array = self.scaler.fit_transform(feature_array)\n",
    "\n",
    "            print(f\"\\nFinal feature matrix shape: {feature_array.shape}\")\n",
    "            print(f\"Using {len(valid_cols)} features\")\n",
    "            \n",
    "            # Verify no infinite values remain\n",
    "            if np.any(np.isinf(feature_array)):\n",
    "                raise ValueError(\"Infinite values still present after preprocessing\")\n",
    "\n",
    "            return feature_array, valid_cols\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing features: {str(e)}\")\n",
    "            print(f\"Available columns: {df.columns}\")\n",
    "            raise\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray, feature_names: List[str]):\n",
    "        \"\"\"Train the model with stacking and SHAP explanations\"\"\"\n",
    "        self.feature_names = feature_names\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Define base models\n",
    "        base_model_configs = {\n",
    "            'xgb': xgb.XGBClassifier(eval_metric='auc', random_state=42),\n",
    "            'rf': RandomForestClassifier(n_jobs=-1, random_state=42, class_weight='balanced'),\n",
    "            'lgbm': LGBMClassifier(n_jobs=-1, random_state=42, class_weight='balanced')\n",
    "        }\n",
    "        \n",
    "        # Train and calibrate base models\n",
    "        for name, model in base_model_configs.items():\n",
    "            print(f\"\\nStarting training for {name}...\")\n",
    "            study = optuna.create_study(direction='maximize', study_name=f'optuna_{name}')\n",
    "            \n",
    "            def objective(trial):\n",
    "                params = self.get_hyperparameters(name, trial)\n",
    "                model.set_params(**params)\n",
    "                cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='average_precision')\n",
    "                return cv_scores.mean()\n",
    "            \n",
    "            study.optimize(objective, n_trials=50, timeout=600)\n",
    "            print(f\"Best parameters for {name}: {study.best_params}\")\n",
    "            print(f\"Best CV score for {name}: {study.best_value}\")\n",
    "            \n",
    "            # Train base model\n",
    "            best_model = type(model)(**study.best_params)\n",
    "            best_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Store SHAP explainer\n",
    "            try:\n",
    "                explainer = shap.TreeExplainer(best_model)\n",
    "                self.shap_explainers[name] = explainer\n",
    "                print(f\"SHAP explainer created for {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating SHAP explainer for {name}: {str(e)}\")\n",
    "                self.shap_explainers[name] = None\n",
    "            \n",
    "            # Calibrate model\n",
    "            print(f\"Calibrating {name}...\")\n",
    "            calibrated_model = CalibratedClassifierCV(best_model, cv=5)\n",
    "            calibrated_model.fit(X_train, y_train)\n",
    "            self.base_models[name] = calibrated_model\n",
    "        \n",
    "        # Build stacked model\n",
    "        print(\"\\nBuilding stacked model...\")\n",
    "        self.build_stacked_model(X_train, y_train)\n",
    "        \n",
    "        # Create final ensemble\n",
    "        print(\"\\nCreating ensemble...\")\n",
    "        self.model = VotingClassifier(\n",
    "            estimators=[\n",
    "                (name, model) for name, model in self.base_models.items()\n",
    "            ] + [('meta_learner', self.meta_learner)],\n",
    "            voting='soft',\n",
    "            weights=[0.25, 0.25, 0.25, 0.25]\n",
    "        )\n",
    "        self.model.fit(X_train, y_train)\n",
    "        print(\"Ensemble training complete\")\n",
    "\n",
    "        # Get stability metrics\n",
    "        stability_results = detector.add_cross_validation_stability(X_train, y_train)\n",
    "        print(\"\\nCross-validation Stability Metrics:\")\n",
    "        print(f\"Mean prediction variance: {stability_results['mean_prediction_variance']:.4f}\")\n",
    "        print(f\"Max prediction variance: {stability_results['max_prediction_variance']:.4f}\")\n",
    "        print(f\"Mean prediction range: {stability_results['mean_prediction_range']:.4f}\")\n",
    "        print(f\"Percentage of stable predictions: {stability_results['stable_prediction_percentage']:.2%}\")        \n",
    "\n",
    "        # Split some validation data\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Optimize ensemble weights\n",
    "        weights = self.optimize_ensemble_weights(X_val, y_val)\n",
    "        print(\"Optimized model weights:\", weights)\n",
    "        \n",
    "        # Final evaluation\n",
    "        final_predictions, unstable_indices = self.predict_with_stability(X_test)\n",
    "        print(f\"Number of unstable predictions: {len(unstable_indices)}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def build_stacked_model(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Build stacked model using base model predictions\"\"\"\n",
    "        base_preds = np.zeros((len(self.base_models), len(X)))\n",
    "        for i, (name, model) in enumerate(self.base_models.items()):\n",
    "            base_preds[i] = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        meta_features = np.column_stack([base_preds.T, X])\n",
    "        meta_learner = LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=3,\n",
    "            num_leaves=8,\n",
    "            feature_fraction=0.8,\n",
    "            bagging_fraction=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        meta_learner.fit(meta_features, y)\n",
    "        self.meta_learner = meta_learner\n",
    "\n",
    "    def get_feature_explanations(self, model_name: str, X: np.ndarray, instance_index: int) -> Dict:\n",
    "        \"\"\"Get SHAP explanations for a specific instance\"\"\"\n",
    "        try:\n",
    "            if instance_index < 0 or instance_index >= X.shape[0]:\n",
    "                print(f\"Invalid instance index: {instance_index}\")\n",
    "                return {}\n",
    "                \n",
    "            if model_name not in self.shap_explainers or self.shap_explainers[model_name] is None:\n",
    "                print(f\"No SHAP explainer available for {model_name}\")\n",
    "                return {}\n",
    "            \n",
    "            explainer = self.shap_explainers[model_name]\n",
    "            shap_values = explainer.shap_values(X[instance_index:instance_index+1])\n",
    "            \n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[1] if len(shap_values) > 1 else shap_values[0]\n",
    "            \n",
    "            shap_instance = shap_values[0]\n",
    "            top_indices = np.argsort(np.abs(shap_instance))[-5:][::-1]\n",
    "            \n",
    "            return {\n",
    "                self.feature_names[i]: float(shap_instance[i])\n",
    "                for i in top_indices\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting SHAP explanations: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _calculate_feature_importance(self):\n",
    "        \"\"\"Calculate and store aggregated feature importance from base models\"\"\"\n",
    "        try:\n",
    "            for name, model in self.base_models.items():\n",
    "                # Access base estimator within CalibratedClassifierCV\n",
    "                if isinstance(model, CalibratedClassifierCV):\n",
    "                    # Try to access base_estimator_ (scikit-learn >=0.24)\n",
    "                    if hasattr(model, 'base_estimator_') and model.base_estimator_ is not None:\n",
    "                        base_estimator = model.base_estimator_\n",
    "                    # For older scikit-learn versions\n",
    "                    elif hasattr(model, 'base_estimator') and model.base_estimator is not None:\n",
    "                        base_estimator = model.base_estimator\n",
    "                    else:\n",
    "                        print(f\"Model {name} does not have a base estimator.\")\n",
    "                        continue\n",
    "                else:\n",
    "                    base_estimator = model\n",
    "\n",
    "                # Retrieve feature importances\n",
    "                if hasattr(base_estimator, 'feature_importances_'):\n",
    "                    importances = base_estimator.feature_importances_\n",
    "                    for feat, imp in zip(self.feature_names, importances):\n",
    "                        self.feature_importance[feat] = self.feature_importance.get(feat, 0) + imp\n",
    "                else:\n",
    "                    print(f\"No feature_importances_ attribute for model {name}.\")\n",
    "\n",
    "            # Average importances across models\n",
    "            num_models = len(self.base_models)\n",
    "            if num_models > 0:\n",
    "                self.feature_importance = {k: v / num_models for k, v in self.feature_importance.items()}\n",
    "                print(\"Feature importance calculated.\")\n",
    "            else:\n",
    "                print(\"No models available to calculate feature importance.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating feature importance: {str(e)}\")\n",
    "    def select_important_features(self, X: np.ndarray, y: np.ndarray, feature_names: List[str], \n",
    "                                threshold: float = 0.01) -> List[str]:\n",
    "        \"\"\"Select features based on SHAP importance\"\"\"\n",
    "        model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "        importance_vals = np.abs(shap_values).mean(0)\n",
    "        \n",
    "        importance = dict(zip(feature_names, importance_vals))\n",
    "        selected_features = [f for f, imp in importance.items() \n",
    "                            if imp > threshold * np.max(importance_vals)]\n",
    "        \n",
    "        print(f\"\\nSelected {len(selected_features)}/{len(feature_names)} features\")\n",
    "        print(\"Top 10 features:\", sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "        return selected_features\n",
    "\n",
    "    def get_hyperparameters(self, model_name: str, trial: optuna.Trial) -> Dict:\n",
    "        \"\"\"Get optimized hyperparameters with regularization\"\"\"\n",
    "        if model_name == 'xgb':\n",
    "            return {\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1, 10),\n",
    "                'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 10.0),\n",
    "                'gamma': trial.suggest_float('gamma', 0, 5)\n",
    "            }\n",
    "        elif model_name == 'lgbm':\n",
    "            return {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),\n",
    "                'lambda_l1': trial.suggest_float('lambda_l1', 0, 10),\n",
    "                'lambda_l2': trial.suggest_float('lambda_l2', 0, 10)\n",
    "            }\n",
    "        else:  # RandomForest\n",
    "            return {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "            }\n",
    "\n",
    "    def analyze_feature_interactions(self, X: np.ndarray, top_k: int = 10) -> List[Tuple[str, str, float]]:\n",
    "        \"\"\"Analyze most important feature interactions\"\"\"\n",
    "        model = self.base_models['xgb']\n",
    "        if hasattr(model, 'base_estimator_'):\n",
    "            model = model.base_estimator_\n",
    "        \n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_interaction_values = explainer.shap_interaction_values(X)\n",
    "        \n",
    "        # Calculate interaction strengths\n",
    "        n_features = len(self.feature_names)\n",
    "        interactions = []\n",
    "        for i in range(n_features):\n",
    "            for j in range(i+1, n_features):\n",
    "                strength = np.abs(shap_interaction_values[:, i, j]).mean()\n",
    "                interactions.append((\n",
    "                    self.feature_names[i],\n",
    "                    self.feature_names[j],\n",
    "                    float(strength)\n",
    "                ))\n",
    "        \n",
    "        # Return top K interactions\n",
    "        return sorted(interactions, key=lambda x: x[2], reverse=True)[:top_k]\n",
    "\n",
    "    def build_stacked_model(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Build a stacked model with meta-learner\"\"\"\n",
    "        # Create base predictions\n",
    "        base_preds = np.zeros((len(self.base_models), len(X)))\n",
    "        for i, (name, model) in enumerate(self.base_models.items()):\n",
    "            base_preds[i] = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Train meta-learner\n",
    "        meta_features = np.column_stack([\n",
    "            base_preds.T,  # Base predictions\n",
    "            X  # Original features\n",
    "        ])\n",
    "        \n",
    "        meta_learner = LGBMClassifier(\n",
    "            n_estimators=100,   \n",
    "            learning_rate=0.01,\n",
    "            max_depth=3,\n",
    "            num_leaves=8,\n",
    "            feature_fraction=0.8,\n",
    "            bagging_fraction=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        meta_learner.fit(meta_features, y)\n",
    "        self.meta_learner = meta_learner\n",
    "    def predict_with_uncertainty(self, features: np.ndarray, \n",
    "                                 authenticity_features: np.ndarray) -> List[Dict]:\n",
    "        \"\"\"Enhanced predictions with uncertainty estimation\"\"\"\n",
    "        # Get predictions from all models\n",
    "        predictions = []\n",
    "        for name, model in self.base_models.items():\n",
    "            pred_proba = model.predict_proba(features)[:, 1]\n",
    "            predictions.append(pred_proba)\n",
    "        \n",
    "        # Calculate ensemble statistics\n",
    "        predictions = np.array(predictions)\n",
    "        mean_probs = predictions.mean(axis=0)\n",
    "        std_probs = predictions.std(axis=0)\n",
    "        \n",
    "        # Calculate prediction intervals\n",
    "        confidence_interval = stats.norm.interval(0.95, loc=mean_probs, scale=std_probs)\n",
    "        \n",
    "        results = []\n",
    "        for i, (prob, std, auth_scores) in enumerate(zip(mean_probs, std_probs, authenticity_features)):\n",
    "            authenticity_score = np.mean([\n",
    "                auth_scores[0],  # authenticity_score\n",
    "                auth_scores[1],  # engagement_quality\n",
    "                auth_scores[2],  # natural_behavior_score\n",
    "                auth_scores[3]   # account_stability\n",
    "            ])\n",
    "            \n",
    "            # Enhanced confidence assessment\n",
    "            model_uncertainty = std / prob if prob > 0 else std\n",
    "            confidence = self._assess_confidence(prob, authenticity_score, model_uncertainty)\n",
    "            \n",
    "            results.append({\n",
    "                'is_bot': prob >= 0.5,\n",
    "                'is_authentic': authenticity_score >= self.authenticity_thresholds['medium'],\n",
    "                'bot_probability': float(prob),\n",
    "                'authenticity_score': float(authenticity_score),\n",
    "                'confidence': confidence,\n",
    "                'uncertainty': float(std),\n",
    "                'prediction_interval': (float(confidence_interval[0][i]), \n",
    "                                         float(confidence_interval[1][i]))\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _assess_confidence(self, prob: float, authenticity: float, \n",
    "                          uncertainty: float) -> str:\n",
    "        \"\"\"Enhanced confidence assessment with uncertainty consideration\"\"\"\n",
    "        # Adjust thresholds based on uncertainty\n",
    "        uncertainty_penalty = uncertainty * 2\n",
    "        \n",
    "        if prob <= 0.1 and authenticity >= self.authenticity_thresholds['high'] and uncertainty < 0.1:\n",
    "            return 'high_authentic'\n",
    "        elif prob <= 0.2 and authenticity >= self.authenticity_thresholds['medium'] and uncertainty < 0.15:\n",
    "            return 'medium_authentic'\n",
    "        elif prob >= (self.confidence_thresholds['high'] + uncertainty_penalty):\n",
    "            return 'high_bot'\n",
    "        elif prob >= (self.confidence_thresholds['medium'] + uncertainty_penalty):\n",
    "            return 'medium_bot'\n",
    "        else:\n",
    "            return 'uncertain'\n",
    "    def get_feature_explanations(self, model_name: str, X: np.ndarray, instance_index: int) -> Dict:\n",
    "        \"\"\"Get SHAP explanations for a specific instance using the underlying base model\"\"\"\n",
    "        try:\n",
    "            # Index check\n",
    "            if instance_index < 0 or instance_index >= X.shape[0]:\n",
    "                print(f\"Error: instance_index {instance_index} is out of bounds for test set with size {X.shape[0]}.\")\n",
    "                return {}\n",
    "            \n",
    "            if model_name not in self.base_models:\n",
    "                print(f\"No model available for {model_name}.\")\n",
    "                return {}\n",
    "            \n",
    "            model = self.base_models[model_name]\n",
    "            \n",
    "            # Get the underlying base model from the CalibratedClassifierCV\n",
    "            if isinstance(model, CalibratedClassifierCV):\n",
    "                # Access the first calibrated classifier's base estimator\n",
    "                base_model = model.calibrated_classifiers_[0].base_estimator\n",
    "                print(f\"Using base estimator from calibrated classifier for {model_name}\")\n",
    "            else:\n",
    "                base_model = model\n",
    "                print(f\"Using model directly for {model_name}\")\n",
    "                \n",
    "            try:\n",
    "                print(f\"Creating SHAP explainer for model type: {type(base_model)}\")\n",
    "                explainer = shap.TreeExplainer(base_model)\n",
    "                \n",
    "                # Use small subset of data for explanation\n",
    "                instance_data = X[instance_index:instance_index+1]\n",
    "                print(f\"Calculating SHAP values for instance shape: {instance_data.shape}\")\n",
    "                \n",
    "                shap_vals = explainer.shap_values(instance_data)\n",
    "                \n",
    "                # Handle different SHAP value formats\n",
    "                if isinstance(shap_vals, list):\n",
    "                    if len(shap_vals) > 1:\n",
    "                        shap_instance = shap_vals[1][0]  # For binary classification\n",
    "                    else:\n",
    "                        shap_instance = shap_vals[0][0]\n",
    "                else:\n",
    "                    shap_instance = shap_vals[0]\n",
    "                \n",
    "                # Get top feature contributions\n",
    "                top_indices = np.argsort(np.abs(shap_instance))[-5:][::-1]\n",
    "                explanations = {\n",
    "                    self.feature_names[i]: float(shap_instance[i]) \n",
    "                    for i in top_indices\n",
    "                }\n",
    "                \n",
    "                return explanations\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating SHAP values for {model_name}: {str(e)}\")\n",
    "                print(f\"Model type: {type(base_model)}\")\n",
    "                return {}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting feature explanations: {str(e)}\")\n",
    "            return {}\n",
    "    def add_cross_validation_stability(self, X: np.ndarray, y: np.ndarray, n_splits: int = 5) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Measure prediction stability across different CV folds.\n",
    "        Returns metrics about how consistent predictions are across folds.\n",
    "        \"\"\"\n",
    "        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Initialize array to store all predictions for each sample\n",
    "        all_predictions = np.zeros((len(X), n_splits))\n",
    "        all_predictions[:] = np.nan  # Fill with NaN to track which predictions we get\n",
    "        \n",
    "        # Get predictions from each fold\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Train model on this fold\n",
    "            self.model.fit(X_train, y_train)\n",
    "            fold_proba = self.model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            # Store predictions in the right spots\n",
    "            all_predictions[val_idx, fold_idx] = fold_proba\n",
    "        \n",
    "        # Calculate variance for each sample (ignoring NaN values)\n",
    "        sample_variances = np.nanvar(all_predictions, axis=1)\n",
    "        sample_ranges = np.nanmax(all_predictions, axis=1) - np.nanmin(all_predictions, axis=1)\n",
    "        \n",
    "        # Compute stability metrics\n",
    "        stability_metrics = {\n",
    "            'mean_prediction_variance': np.mean(sample_variances),\n",
    "            'max_prediction_variance': np.max(sample_variances),\n",
    "            'mean_prediction_range': np.mean(sample_ranges),\n",
    "            'max_prediction_range': np.max(sample_ranges),\n",
    "            'stable_prediction_percentage': np.mean(sample_variances < 0.1)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nPrediction matrix shape: {all_predictions.shape}\")\n",
    "        print(f\"Number of samples with predictions: {np.sum(~np.isnan(all_predictions.mean(axis=1)))}\")\n",
    "        print(f\"Average predictions per sample: {np.mean(~np.isnan(all_predictions)):.2f}\")\n",
    "        \n",
    "        return stability_metrics    \n",
    "    def optimize_ensemble_weights(self, X: np.ndarray, y: np.ndarray) -> List[float]:\n",
    "        \"\"\"Optimize ensemble weights based on individual model performance\"\"\"\n",
    "        try:\n",
    "            # Get individual model performances\n",
    "            model_scores = {}\n",
    "            \n",
    "            # Score base models\n",
    "            for name, model in self.base_models.items():\n",
    "                score = roc_auc_score(y, model.predict_proba(X)[:, 1])\n",
    "                model_scores[name] = score\n",
    "                print(f\"{name} ROC AUC: {score:.4f}\")\n",
    "            \n",
    "            # Score meta learner on combined predictions\n",
    "            base_preds = np.zeros((len(self.base_models), len(X)))\n",
    "            for i, (name, model) in enumerate(self.base_models.items()):\n",
    "                base_preds[i] = model.predict_proba(X)[:, 1]\n",
    "            \n",
    "            meta_features = np.column_stack([base_preds.T, X])\n",
    "            meta_score = roc_auc_score(y, self.meta_learner.predict_proba(meta_features)[:, 1])\n",
    "            model_scores['meta_learner'] = meta_score\n",
    "            print(f\"Meta learner ROC AUC: {meta_score:.4f}\")\n",
    "            \n",
    "            # Calculate weights based on relative performance\n",
    "            total_score = sum(model_scores.values())\n",
    "            weights = [score/total_score for score in model_scores.values()]\n",
    "            \n",
    "            # Update ensemble with new weights\n",
    "            self.model = VotingClassifier(\n",
    "                estimators=[\n",
    "                    (name, model) for name, model in self.base_models.items()\n",
    "                ] + [('meta_learner', self.meta_learner)],\n",
    "                voting='soft',\n",
    "                weights=weights\n",
    "            )\n",
    "            self.model.fit(X, y)  # Refit with new weights\n",
    "            \n",
    "            return weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error optimizing weights: {str(e)}\")\n",
    "            return [0.25, 0.25, 0.25, 0.25]  # Default weights\n",
    "\n",
    "    def predict_with_stability(self, X: np.ndarray) -> Tuple[np.ndarray, List[int]]:\n",
    "        \"\"\"Make predictions with stability assessment\"\"\"\n",
    "        try:\n",
    "            # Get predictions from base models\n",
    "            base_predictions = np.zeros((len(self.base_models), len(X)))\n",
    "            for i, (name, model) in enumerate(self.base_models.items()):\n",
    "                base_predictions[i] = model.predict_proba(X)[:, 1]\n",
    "            \n",
    "            # Calculate prediction statistics\n",
    "            mean_predictions = np.mean(base_predictions, axis=0)\n",
    "            std_predictions = np.std(base_predictions, axis=0)\n",
    "            \n",
    "            # Identify unstable predictions (high variance between models)\n",
    "            unstable_indices = np.where(std_predictions > 0.2)[0]\n",
    "            \n",
    "            # Get ensemble predictions\n",
    "            predictions = self.model.predict_proba(X)\n",
    "            \n",
    "            # Adjust confidence for unstable predictions\n",
    "            confidence_adjustments = 1 - np.clip(std_predictions, 0, 0.5)\n",
    "            adjusted_predictions = predictions * confidence_adjustments.reshape(-1, 1)\n",
    "            \n",
    "            return adjusted_predictions, unstable_indices.tolist()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction with stability: {str(e)}\")\n",
    "            return self.model.predict_proba(X), []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Re-initialize after building\n",
    "feature_eng = FeatureEngineering(\"data\", \"checkpoints\")\n",
    "detector = SybilDetectionSystem(feature_eng)\n",
    "\n",
    "# Build feature matrix\n",
    "matrix = feature_eng.build_feature_matrix()\n",
    "print(\"Feature matrix built\")\n",
    "\n",
    "# Load labels\n",
    "labels_df = pl.read_csv('data/labels.csv')\n",
    "labels_df = labels_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "\n",
    "# Ensure matrix fid is Int64\n",
    "matrix = matrix.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "\n",
    "# Join with matching types\n",
    "data = matrix.join(labels_df, on='fid', how='inner')\n",
    "\n",
    "# Extract features and labels\n",
    "X, feature_names = detector.prepare_features(data.drop(['bot', 'fid']))\n",
    "y = data['bot'].to_numpy()\n",
    "\n",
    "# Perform train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "fids = data['fid'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_fids, test_fids = train_test_split(\n",
    "    X, y, fids,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # ensures class distribution is preserved\n",
    ")\n",
    "\n",
    "# Define a path to save the model checkpoint\n",
    "model_checkpoint_path = \"checkpoints/sybil_detector_model.pkl\"\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "# Check if a model checkpoint already exists\n",
    "if os.path.exists(model_checkpoint_path):\n",
    "    # Load the detector object (including the trained model)\n",
    "    detector = joblib.load(model_checkpoint_path)\n",
    "    print(\"Loaded model checkpoint. Skipping training.\")\n",
    "else:\n",
    "    # Train the model using the training set only\n",
    "    detector.train(X_train, y_train, feature_names)\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    joblib.dump(detector, model_checkpoint_path)\n",
    "    print(f\"Model checkpoint saved to {model_checkpoint_path}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred_proba = detector.model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Compute and plot SHAP for X_test:\n",
    "shap_values_test = detector.shap_explainers['xgb'].shap_values(X_test)\n",
    "if isinstance(shap_values_test, list) and len(shap_values_test) > 1:\n",
    "    shap_values_test = shap_values_test[1]\n",
    "assert shap_values_test.shape[0] == X_test.shape[0], \"Mismatch in rows between shap_values and X_test!\"\n",
    "shap.summary_plot(shap_values_test, X_test, feature_names=feature_names)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()\n",
    "\n",
    "# Compute evaluation metrics\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred)\n",
    "test_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest ROC AUC:\", test_roc_auc)\n",
    "print(\"Test F1 Score:\", test_f1)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "\n",
    "# SHAP explanations for a specific instance\n",
    "instance_index = 0\n",
    "model_name = 'xgb'\n",
    "explanations = detector.get_feature_explanations(model_name, X_test, instance_index)\n",
    "print(f\"\\nSHAP explanations for instance {instance_index} from {model_name}:\")\n",
    "print(explanations)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Authentic', 'Bot'], yticklabels=['Authentic', 'Bot'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score\n",
    "\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "print(\"Cohen's Kappa:\", kappa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 1. Get all FIDs from the full matrix\n",
    "all_fids = matrix['fid'].unique()\n",
    "labeled_fids = data['fid'].unique()\n",
    "\n",
    "# 2. Convert to numpy arrays for set operations\n",
    "all_fids_np = all_fids.to_numpy()\n",
    "labeled_fids_np = labeled_fids.to_numpy()\n",
    "\n",
    "# 3. Find FIDs that aren't in the labeled dataset\n",
    "unlabeled_fids = np.setdiff1d(all_fids_np, labeled_fids_np)\n",
    "\n",
    "# 4. Filter matrix to get unlabeled data\n",
    "unlabeled_data = matrix.filter(pl.col('fid').is_in(unlabeled_fids))\n",
    "\n",
    "# 5. Get profiles with fnames\n",
    "ds = feature_eng.loader.get_dataset('profile_with_addresses')\n",
    "valid_profiles = ds.filter(pl.col('fname').is_not_null())\n",
    "\n",
    "# Ensure valid_profiles has unique fids\n",
    "valid_profiles = valid_profiles.unique(subset=['fid'])\n",
    "\n",
    "# 5. Get profiles with fnames (already deduplicated above)\n",
    "valid_fids = valid_profiles['fid'].unique()\n",
    "\n",
    "# 6. Filter unlabeled data to only include profiles with fnames\n",
    "unlabeled_data_filtered = unlabeled_data.filter(pl.col('fid').is_in(valid_fids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'profile_update_consistency' column exists\n",
    "if 'profile_update_consistency' in unlabeled_data_filtered.columns:\n",
    "    print(unlabeled_data_filtered.select(['profile_update_consistency']).describe())\n",
    "else:\n",
    "    print(\"Column 'profile_update_consistency' not found in unlabeled_data_filtered.\")\n",
    "\n",
    "# Compute statistics for given features\n",
    "for feature in ['profile_update_consistency', 'influence_score', 'follower_count']:\n",
    "    if feature in data.columns:\n",
    "        bot_values = data.filter(pl.col('bot') == 1)[feature]\n",
    "        human_values = data.filter(pl.col('bot') == 0)[feature]\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"\\n{feature} statistics:\")\n",
    "        print(\"Bot mean:\", bot_values.mean())\n",
    "        print(\"Human mean:\", human_values.mean())\n",
    "        print(\"Bot std:\", bot_values.std())\n",
    "        print(\"Human std:\", human_values.std())\n",
    "    else:\n",
    "        print(f\"\\nFeature '{feature}' does not exist in the data DataFrame.\")\n",
    "\n",
    "# 7. Prepare features and predictions for unlabeled data\n",
    "X_unlabeled_filtered, valid_features = detector.prepare_features(unlabeled_data_filtered.drop(\"fid\"))\n",
    "\n",
    "# Debug prints to ensure lengths match\n",
    "print(f\"unlabeled_data_filtered shape: {unlabeled_data_filtered.shape}\")\n",
    "print(f\"X_unlabeled_filtered shape: {X_unlabeled_filtered.shape}\")\n",
    "print(f\"model feature length: {len(detector.feature_names)}\")\n",
    "if X_unlabeled_filtered.shape[0] != len(unlabeled_data_filtered):\n",
    "    print(\"Warning: Length mismatch between unlabeled_data_filtered and X_unlabeled_filtered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_filtered = detector.model.predict_proba(X_unlabeled_filtered)[:, 1]\n",
    "y_pred_filtered = (y_pred_proba_filtered >= 0.5).astype(int)\n",
    "\n",
    "# Compute SHAP values for unlabeled data\n",
    "# Extract the underlying xgb model from the ensemble if needed\n",
    "xgb_model = detector.base_models['xgb'].base_estimator_ if hasattr(detector.base_models['xgb'], 'base_estimator_') else detector.base_models['xgb']\n",
    "\n",
    "explainer_unlabeled = shap.TreeExplainer(xgb_model.estimator)\n",
    "shap_values_unlabeled = explainer_unlabeled.shap_values(X_unlabeled_filtered)\n",
    "\n",
    "\n",
    "# 8. Get SHAP explanations for human predictions\n",
    "human_explanations = []\n",
    "# sample only 10 random accounts\n",
    "fids_array = unlabeled_data_filtered.sample(10)['fid'].to_numpy()\n",
    "\n",
    "# filter only values that would have a SHAP value \n",
    "\n",
    "for i, (pred, fid) in enumerate(zip(y_pred_filtered, fids_array)):\n",
    "    # Ensure i is within the bounds of X_unlabeled_filtered\n",
    "    if i >= X_unlabeled_filtered.shape[0]:\n",
    "        print(f\"Index {i} is out of range for X_unlabeled_filtered of size {X_unlabeled_filtered.shape[0]}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if pred == 0:  # If predicted human\n",
    "        # Retrieve SHAP values from the unlabeled SHAP explainer\n",
    "        shap_values = detector.get_feature_explanations('xgb', X_unlabeled_filtered, i)\n",
    "\n",
    "        # Filter row by fid\n",
    "        row = unlabeled_data_filtered.filter(pl.col('fid') == fid)\n",
    "\n",
    "        # Handle authenticity_score\n",
    "        if 'authenticity_score' in row.columns and len(row) == 1:\n",
    "            authenticity_score = row['authenticity_score'].item()\n",
    "        else:\n",
    "            authenticity_score = 0.0  # default\n",
    "\n",
    "        # Get fname for this fid\n",
    "        fname_series = valid_profiles.filter(pl.col('fid') == fid)['fname']\n",
    "        if len(fname_series) == 1:\n",
    "            fname = fname_series.item()\n",
    "        elif len(fname_series) == 0:\n",
    "            fname = \"Unknown\"\n",
    "        else:\n",
    "            fname = fname_series[0]\n",
    "\n",
    "        # Only negative impact features\n",
    "        negative_reasons = {k: v for k, v in shap_values.items() if v < 0}\n",
    "\n",
    "        human_explanations.append({\n",
    "            'fid': fid,\n",
    "            'fname': fname,\n",
    "            'authenticity_score': authenticity_score,\n",
    "            'human_probability': 1 - y_pred_proba_filtered[i],\n",
    "            'reasons': negative_reasons\n",
    "        })\n",
    "\n",
    "# Print results\n",
    "sample_size = min(10, len(human_explanations))\n",
    "random_accounts = random.sample(human_explanations, sample_size)\n",
    "\n",
    "print(\"\\nHuman accounts with explanations (random sample):\")\n",
    "for i, exp in enumerate(random_accounts, start=1):\n",
    "    print(f\"\\n{i}. {exp['fname']} (FID: {exp['fid']})\")\n",
    "    print(f\"Human Probability: {exp['human_probability']:.2%}\")\n",
    "    print(f\"Authenticity Score: {exp['authenticity_score']:.2f}\")\n",
    "    print(\"Top reasons for human classification:\")\n",
    "    sorted_reasons = sorted(exp['reasons'].items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    for feature, impact in sorted_reasons:\n",
    "        print(f\"  - {feature}: {abs(impact):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distribution for entire unlabeled dataset\n",
    "total_profiles = len(y_pred_filtered)\n",
    "humans_count = (y_pred_filtered == 0).sum()\n",
    "bots_count = (y_pred_filtered == 1).sum()\n",
    "\n",
    "print(\"\\nOverall Distribution Analysis:\")\n",
    "print(f\"Total profiles analyzed: {total_profiles:,}\")\n",
    "print(f\"Predicted humans: {humans_count:,} ({(humans_count/total_profiles)*100:.1f}%)\")\n",
    "print(f\"Predicted bots: {bots_count:,} ({(bots_count/total_profiles)*100:.1f}%)\")\n",
    "\n",
    "# Probability distribution analysis\n",
    "print(\"\\nPrediction Probability Analysis:\")\n",
    "print(f\"Mean bot probability: {y_pred_proba_filtered.mean():.3f}\")\n",
    "print(f\"Median bot probability: {np.median(y_pred_proba_filtered):.3f}\")\n",
    "print(f\"Std dev of bot probability: {y_pred_proba_filtered.std():.3f}\")\n",
    "\n",
    "# Distribution buckets for more detailed view\n",
    "buckets = np.histogram(y_pred_proba_filtered, bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "bucket_counts = buckets[0]\n",
    "bucket_ranges = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\n",
    "\n",
    "print(\"\\nProbability Distribution Breakdown:\")\n",
    "for range_name, count in zip(bucket_ranges, bucket_counts):\n",
    "    percentage = (count/total_profiles)*100\n",
    "    print(f\"{range_name}: {count:,} accounts ({percentage:.1f}%)\")\n",
    "\n",
    "# High confidence predictions\n",
    "high_conf_human = (y_pred_proba_filtered < 0.2).sum()\n",
    "high_conf_bot = (y_pred_proba_filtered > 0.8).sum()\n",
    "\n",
    "print(\"\\nHigh Confidence Predictions:\")\n",
    "print(f\"High confidence humans (p < 0.2): {high_conf_human:,} ({(high_conf_human/total_profiles)*100:.1f}%)\")\n",
    "print(f\"High confidence bots (p > 0.8): {high_conf_bot:,} ({(high_conf_bot/total_profiles)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create histogram\n",
    "bins = np.linspace(0, 1, 21)  # 20 bins for smooth distribution\n",
    "n, bins, patches = ax.hist(y_pred_proba_filtered, bins=bins, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Customize colors based on probability ranges\n",
    "for i, patch in enumerate(patches):\n",
    "    bin_center = bins[i] + (bins[1] - bins[0])/2\n",
    "    if bin_center < 0.2:\n",
    "        patch.set_facecolor('#2ecc71')  # Green for human predictions\n",
    "    elif bin_center > 0.8:\n",
    "        patch.set_facecolor('#e74c3c')  # Red for bot predictions\n",
    "    else:\n",
    "        patch.set_facecolor('#3498db')  # Blue for uncertain predictions\n",
    "\n",
    "# Add vertical lines for key thresholds\n",
    "ax.axvline(x=0.2, color='#27ae60', linestyle='--', alpha=0.5, label='Human threshold (0.2)')\n",
    "ax.axvline(x=0.8, color='#c0392b', linestyle='--', alpha=0.5, label='Bot threshold (0.8)')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Distribution of Bot Probability Scores', pad=20, fontsize=14)\n",
    "ax.set_xlabel('Bot Probability Score', fontsize=12)\n",
    "ax.set_ylabel('Number of Accounts', fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Add text annotations for key statistics\n",
    "stats_text = (\n",
    "    f'Total Profiles: {total_profiles:,}\\n'\n",
    "    f'Mean Probability: {y_pred_proba_filtered.mean():.3f}\\n'\n",
    "    f'Median Probability: {np.median(y_pred_proba_filtered):.3f}\\n'\n",
    "    f'Std Dev: {y_pred_proba_filtered.std():.3f}'\n",
    ")\n",
    "plt.text(0.95, 0.95, stats_text,\n",
    "         transform=ax.transAxes,\n",
    "         verticalalignment='top',\n",
    "         horizontalalignment='right',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Adjust layout to prevent text cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_features(segment: str) -> List[str]:\n",
    "    base_features = ['has_ens', 'has_bio', 'has_avatar', 'verification_count']\n",
    "    \n",
    "    if segment == 'active':\n",
    "        return base_features + [\n",
    "            'cast_timing_entropy',\n",
    "            'reply_ratio',\n",
    "            'mention_patterns',\n",
    "            'engagement_rate'\n",
    "        ]\n",
    "    elif segment == 'low_activity':\n",
    "        return base_features + [\n",
    "            'network_growth_rate',\n",
    "            'initial_behavior_pattern',\n",
    "            'verification_sequence'\n",
    "        ]\n",
    "    else:  # dormant\n",
    "        return base_features + [\n",
    "            'follower_growth_velocity',\n",
    "            'network_structure',\n",
    "            'profile_completion_sequence'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def segment_users_by_behavior(matrix: pl.DataFrame) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Segment users based on behavioral patterns\"\"\"\n",
    "    segments = {\n",
    "        'power_users': matrix.filter(\n",
    "            (pl.col('cast_count') >= 20) & \n",
    "            (pl.col('reply_count') >= 5)\n",
    "        ),\n",
    "        'casual_users': matrix.filter(\n",
    "            (pl.col('cast_count') >= 5) & \n",
    "            (pl.col('cast_count') < 20)\n",
    "        ),\n",
    "        'one_time_users': matrix.filter(\n",
    "            (pl.col('cast_count') > 0) & \n",
    "            (pl.col('cast_count') < 5)\n",
    "        ),\n",
    "        'lurkers': matrix.filter(pl.col('cast_count') == 0)\n",
    "    }\n",
    "    \n",
    "    total = len(matrix)\n",
    "    print(\"\\nUser Segment Distribution:\")\n",
    "    for name, segment in segments.items():\n",
    "        size = len(segment)\n",
    "        if size > 0 and 'bot' in segment.columns:\n",
    "            bot_pct = (segment.filter(pl.col('bot') == 1).shape[0] / size) * 100\n",
    "            print(f\"{name}: {size:,} users ({size/total*100:.1f}%) - {bot_pct:.1f}% bots\")\n",
    "            \n",
    "            metrics = segment.select([\n",
    "                pl.col('cast_count').mean(),\n",
    "                pl.col('follower_count').mean(),\n",
    "                pl.col('following_count').mean()\n",
    "            ]).to_numpy()[0]\n",
    "            \n",
    "            print(f\"  Avg casts: {metrics[0]:.1f}\")\n",
    "            print(f\"  Avg followers: {metrics[1]:.1f}\")\n",
    "            print(f\"  Avg following: {metrics[2]:.1f}\")\n",
    "        else:\n",
    "            print(f\"{name}: {size:,} users ({size/total*100:.1f}%)\")\n",
    "            \n",
    "    return segments\n",
    "\n",
    "def get_segment_specific_features(segment_name: str) -> List[str]:\n",
    "    \"\"\"Get feature list specific to each behavior segment\"\"\"\n",
    "    base_features = [\n",
    "        'has_ens', 'has_bio', 'has_avatar', 'verification_count',  # verification_count here\n",
    "        'following_count', 'follower_count', 'follower_ratio',\n",
    "        'unique_follower_ratio', 'authenticity_score'\n",
    "    ]\n",
    "    \n",
    "    segment_features = {\n",
    "        'power_users': [\n",
    "            'cast_count', 'total_reactions', 'avg_cast_length',\n",
    "            'reply_count', 'mentions_count', 'engagement_score',\n",
    "            'weekday_diversity', 'hour_diversity', 'rapid_actions',\n",
    "            'avg_hours_between_actions', 'std_hours_between_actions',\n",
    "            'power_user_interaction_ratio', 'influence_score'\n",
    "        ],\n",
    "        'casual_users': [\n",
    "            'cast_count', 'total_reactions', 'engagement_score',\n",
    "            'reply_count', 'rapid_actions', 'avg_hours_between_actions',\n",
    "            'avg_cast_length', 'mentions_count'\n",
    "        ],\n",
    "        'one_time_users': [\n",
    "            'cast_count', 'total_reactions', 'profile_update_consistency',\n",
    "            'follower_growth_rate'\n",
    "        ],\n",
    "        'lurkers': [\n",
    "            'profile_update_consistency', 'network_balance',\n",
    "            'follower_growth_rate',\n",
    "            'profile_completeness'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return base_features + segment_features.get(segment_name, [])\n",
    "\n",
    "class RapidModelEvaluator:\n",
    "    def __init__(self, n_cv_splits: int = 5):\n",
    "        self.n_cv_splits = n_cv_splits\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate_model(self, name: str, model: Any, X: np.ndarray, y: np.ndarray) -> Dict:\n",
    "        \"\"\"Quickly evaluate a model with cross-validation\"\"\"\n",
    "        scoring = {\n",
    "            'precision': make_scorer(lambda y_true, y_pred: \n",
    "                precision_recall_fscore_support(y_true, y_pred)[0].mean()),\n",
    "            'recall': make_scorer(lambda y_true, y_pred: \n",
    "                precision_recall_fscore_support(y_true, y_pred)[1].mean()),\n",
    "            'f1': 'f1'\n",
    "        }\n",
    "        \n",
    "        cv_results = cross_validate(\n",
    "            model, X, y,\n",
    "            cv=self.n_cv_splits,\n",
    "            scoring=scoring,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        self.results[name] = {\n",
    "            'test_scores': {\n",
    "                'precision': cv_results['test_precision'].mean(),\n",
    "                'recall': cv_results['test_recall'].mean(),\n",
    "                'f1': cv_results['test_f1'].mean()\n",
    "            },\n",
    "            'fit_time': cv_results['fit_time'].mean()\n",
    "        }\n",
    "        \n",
    "        return self.results[name]\n",
    "    \n",
    "    def compare_models(self, segment_name: str, models: Dict[str, Any], \n",
    "                      X: np.ndarray, y: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"Compare multiple models quickly\"\"\"\n",
    "        self.results = {}  # Reset results for each segment\n",
    "        for name, model in models.items():\n",
    "            print(f\"Evaluating {name} on {segment_name} segment...\")\n",
    "            self.evaluate_model(f\"{name}\", model, X, y)\n",
    "        \n",
    "        results_df = pd.DataFrame.from_dict(\n",
    "            {k: v['test_scores'] for k, v in self.results.items()}, \n",
    "            orient='index'\n",
    "        )\n",
    "        results_df['fit_time'] = [v['fit_time'] for v in self.results.values()]\n",
    "        \n",
    "        return results_df.sort_values('f1', ascending=False)\n",
    "\n",
    "def prepare_segment_features(segment: pl.DataFrame, segment_name: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare features for a given segment\"\"\"\n",
    "    feature_cols = get_segment_specific_features(segment_name)\n",
    "    valid_features = [col for col in feature_cols if col in segment.columns]\n",
    "    print(f\"\\nUsing {len(valid_features)} features for {segment_name}:\", valid_features)\n",
    "    \n",
    "    X = segment.select(valid_features).fill_null(0).to_numpy()\n",
    "    y = segment['bot'].to_numpy() if 'bot' in segment.columns else None\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y, valid_features  # Return valid_features as well\n",
    "\n",
    "def evaluate_segments(matrix: pl.DataFrame, labels_df: pl.DataFrame) -> Dict:\n",
    "    \"\"\"Evaluate models on each behavioral segment\"\"\"\n",
    "    # Ensure consistent FID type\n",
    "    matrix = matrix.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "    labels_df = labels_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "    \n",
    "    # Join with labels\n",
    "    data = matrix.join(labels_df, on='fid', how='inner')\n",
    "    \n",
    "    # Create segments\n",
    "    segmented_users = segment_users_by_behavior(data)\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = RapidModelEvaluator()\n",
    "    \n",
    "    # Store results\n",
    "    segment_results = {}\n",
    "    \n",
    "    # For each segment\n",
    "    for segment_name, segment_data in segmented_users.items():\n",
    "        print(f\"\\nProcessing {segment_name} segment...\")\n",
    "        \n",
    "        # Prepare features\n",
    "        X, y, valid_features = prepare_segment_features(segment_data, segment_name)\n",
    "        \n",
    "        if y is None or len(np.unique(y)) < 2:\n",
    "            print(f\"Skipping {segment_name} - insufficient labels\")\n",
    "            continue\n",
    "        \n",
    "        # Define models with balanced class weights\n",
    "        models = {\n",
    "            'xgb': xgb.XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                scale_pos_weight=sum(y == 0) / sum(y == 1),\n",
    "                random_state=42\n",
    "            ),\n",
    "            'lgbm': LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ),\n",
    "            'rf': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=4,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ),\n",
    "            'logistic': LogisticRegression(\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Evaluate models\n",
    "        results = evaluator.compare_models(segment_name, models, X, y)\n",
    "        segment_results[segment_name] = results\n",
    "        \n",
    "        print(f\"\\nResults for {segment_name}:\")\n",
    "        print(results)\n",
    "        \n",
    "        # Print feature importance for best model\n",
    "        try:\n",
    "            best_model_name = results.index[0]\n",
    "            best_model = models[best_model_name]\n",
    "            \n",
    "            if hasattr(best_model, 'feature_importances_'):\n",
    "                importances = pd.DataFrame({\n",
    "                    'feature': valid_features,\n",
    "                    'importance': best_model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(f\"\\nTop 10 features for {segment_name}:\")\n",
    "                print(importances.head(10))\n",
    "            else:\n",
    "                print(f\"\\nNo feature importances available for {best_model_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating feature importance: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return segment_results\n",
    "\n",
    "\n",
    "results = evaluate_segments(matrix, labels_df)\n",
    "\n",
    "print(\"\\nBest models per segment:\")\n",
    "for segment_name, result_df in results.items():\n",
    "    best_model = result_df.index[0]\n",
    "    best_f1 = result_df.iloc[0]['f1']\n",
    "    best_precision = result_df.iloc[0]['precision']\n",
    "    best_recall = result_df.iloc[0]['recall']\n",
    "    print(f\"{segment_name}:\")\n",
    "    print(f\"  Best model: {best_model}\")\n",
    "    print(f\"  F1: {best_f1:.3f}\")\n",
    "    print(f\"  Precision: {best_precision:.3f}\")\n",
    "    print(f\"  Recall: {best_recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_full_distribution(matrix: pl.DataFrame, labels_df: pl.DataFrame) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"Analyze distribution in both full and labeled datasets\"\"\"\n",
    "    print(\"=== Full Dataset Distribution ===\")\n",
    "    full_segments = segment_users_by_behavior(matrix)\n",
    "    \n",
    "    print(\"\\n=== Labeled Dataset Distribution ===\")\n",
    "    labeled_data = matrix.join(labels_df, on='fid', how='inner')\n",
    "    labeled_segments = segment_users_by_behavior(labeled_data)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    print(\"\\n=== Label Coverage by Segment ===\")\n",
    "    for segment_name in full_segments.keys():\n",
    "        full_count = len(full_segments[segment_name])\n",
    "        labeled_count = len(labeled_segments.get(segment_name, pl.DataFrame()))\n",
    "        coverage = (labeled_count / full_count * 100) if full_count > 0 else 0\n",
    "        print(f\"{segment_name}: {coverage:.1f}% labeled ({labeled_count}/{full_count})\")\n",
    "        \n",
    "    return full_segments, labeled_segments\n",
    "def get_unlabeled_samples(matrix: pl.DataFrame, \n",
    "                         labels_df: pl.DataFrame, \n",
    "                         samples_per_segment: int = 50,\n",
    "                         use_isolation_forest: bool = True) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Get stratified samples of unlabeled data, optionally using anomaly detection\"\"\"\n",
    "    \n",
    "    # Get unlabeled FIDs\n",
    "    labeled_fids = labels_df['fid'].unique()\n",
    "    unlabeled = matrix.filter(~pl.col('fid').is_in(labeled_fids))\n",
    "    \n",
    "    # Segment unlabeled data\n",
    "    segments = segment_users_by_behavior(unlabeled)\n",
    "    \n",
    "    # Features for anomaly detection\n",
    "    anomaly_features = [\n",
    "        'cast_count', 'follower_count', 'following_count',\n",
    "        'authenticity_score', 'total_reactions', 'rapid_actions',\n",
    "        'avg_hours_between_actions', 'std_hours_between_actions'\n",
    "    ]\n",
    "    \n",
    "    samples = {}\n",
    "    for name, segment in segments.items():\n",
    "        print(f\"\\nProcessing {name} segment ({len(segment)} users)\")\n",
    "        \n",
    "        if len(segment) == 0:\n",
    "            continue\n",
    "            \n",
    "        if use_isolation_forest and len(segment) > samples_per_segment:\n",
    "            # Prepare features for anomaly detection\n",
    "            valid_features = [f for f in anomaly_features if f in segment.columns]\n",
    "            if len(valid_features) > 0:\n",
    "                X = segment.select(valid_features).fill_null(0).to_numpy()\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "                \n",
    "                # Use Isolation Forest to identify anomalies\n",
    "                iso_forest = IsolationForest(\n",
    "                    n_estimators=100,\n",
    "                    contamination=0.1,  # Assume 10% anomalies\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                # Get anomaly scores\n",
    "                scores = iso_forest.fit_predict(X)\n",
    "                anomaly_indices = np.where(scores == -1)[0]\n",
    "                normal_indices = np.where(scores == 1)[0]\n",
    "                \n",
    "                # Sample both anomalies and normal cases\n",
    "                n_anomalies = min(samples_per_segment // 4, len(anomaly_indices))\n",
    "                n_normal = samples_per_segment - n_anomalies\n",
    "                \n",
    "                # Create a filter for selected indices\n",
    "                selected_indices = np.concatenate([\n",
    "                    np.random.choice(anomaly_indices, n_anomalies, replace=False),\n",
    "                    np.random.choice(normal_indices, n_normal, replace=False)\n",
    "                ])\n",
    "                \n",
    "                # Create a row number column and filter by selected indices\n",
    "                samples[name] = (segment\n",
    "                    .with_row_count(\"row_nr\")\n",
    "                    .filter(pl.col(\"row_nr\").is_in(selected_indices))\n",
    "                    .drop(\"row_nr\"))\n",
    "                \n",
    "                print(f\"Selected {n_anomalies} potential anomalies and {n_normal} normal cases\")\n",
    "            else:\n",
    "                # Fallback to random sampling if features not available\n",
    "                samples[name] = segment.sample(n=samples_per_segment, seed=42)\n",
    "        else:\n",
    "            # For small segments, take all samples\n",
    "            n_samples = min(samples_per_segment, len(segment))\n",
    "            samples[name] = segment.sample(n=n_samples, seed=42)\n",
    "        \n",
    "        print(f\"Final sample size: {len(samples[name])}\")\n",
    "        \n",
    "        # Print some statistics about the sample\n",
    "        if 'cast_count' in segment.columns:\n",
    "            stats = samples[name].select([\n",
    "                pl.col('cast_count').mean().alias('avg_casts'),\n",
    "                pl.col('follower_count').mean().alias('avg_followers'),\n",
    "                pl.col('following_count').mean().alias('avg_following')\n",
    "            ])\n",
    "            print(\"Sample statistics:\")\n",
    "            print(f\"  Avg casts: {stats['avg_casts'][0]:.1f}\")\n",
    "            print(f\"  Avg followers: {stats['avg_followers'][0]:.1f}\")\n",
    "            print(f\"  Avg following: {stats['avg_following'][0]:.1f}\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def export_samples_for_labeling(samples: Dict[str, pl.DataFrame], \n",
    "                              output_path: str = \"samples_for_labeling.csv\"):\n",
    "    \"\"\"Export samples for manual labeling\"\"\"\n",
    "    # Combine all samples\n",
    "    all_samples = pl.concat([\n",
    "        segment.with_columns(pl.lit(name).alias('segment'))\n",
    "        for name, segment in samples.items()\n",
    "    ])\n",
    "    \n",
    "    # Select relevant columns for labeling\n",
    "    export_columns = [\n",
    "        'fid', 'segment', 'fname',\n",
    "        'cast_count', 'follower_count', 'following_count',\n",
    "        'total_reactions', 'authenticity_score'\n",
    "    ]\n",
    "    \n",
    "    # Only include columns that exist\n",
    "    valid_columns = [col for col in export_columns if col in all_samples.columns]\n",
    "    \n",
    "    # Export to CSV\n",
    "    all_samples.select(valid_columns).write_csv(output_path)\n",
    "    print(f\"\\nExported {len(all_samples)} samples to {output_path}\")\n",
    "    \n",
    "def suggest_priority_accounts(matrix: pl.DataFrame, \n",
    "                              labels_df: pl.DataFrame,\n",
    "                              n_suggestions: int = 50) -> pl.DataFrame:\n",
    "    \"\"\"Suggest priority accounts for labeling based on influence and uncertainty\"\"\"\n",
    "    \n",
    "    # Get unlabeled accounts\n",
    "    labeled_fids = labels_df['fid'].unique()\n",
    "    unlabeled = matrix.filter(~pl.col('fid').is_in(labeled_fids))\n",
    "    \n",
    "    # Calculate influence score\n",
    "    influence_features = [\n",
    "        'follower_count', 'following_count', 'cast_count',\n",
    "        'total_reactions', 'authenticity_score'\n",
    "    ]\n",
    "    \n",
    "    # Only use available features\n",
    "    valid_features = [f for f in influence_features if f in unlabeled.columns]\n",
    "    \n",
    "    if len(valid_features) > 0:\n",
    "        # Normalize features\n",
    "        normalized = unlabeled.select([\n",
    "            'fid',\n",
    "            *(pl.col(f).fill_null(0) / pl.col(f).fill_null(0).max() for f in valid_features)\n",
    "        ])\n",
    "        \n",
    "        # Simple influence score - average of normalized features\n",
    "        influence_df = normalized.with_columns([\n",
    "            pl.fold(acc=0, function=lambda acc, x: acc + x, exprs=[pl.col(f) for f in valid_features])\n",
    "            .alias('influence_score')\n",
    "        ])\n",
    "        \n",
    "        # Get top influential accounts\n",
    "        suggestions = influence_df.sort('influence_score', descending=True).head(n_suggestions)\n",
    "        \n",
    "        # Select only necessary columns for the join\n",
    "        suggestions = suggestions.select(['fid', 'influence_score'])\n",
    "        \n",
    "        # Join back with original features, including 'fname'\n",
    "        result = suggestions.join(unlabeled, on='fid', how='left', suffix=\"_unlabeled\")\n",
    "        \n",
    "        # Ensure 'fname' is included in the final result\n",
    "        if 'fname' in result.columns:\n",
    "            return result.select(['fid', 'fname', 'influence_score', 'follower_count', 'cast_count', 'total_reactions'])\n",
    "        else:\n",
    "            print(\"Warning: 'fname' column not found in the dataset.\")\n",
    "            return result.select(['fid', 'influence_score', 'follower_count', 'cast_count', 'total_reactions'])\n",
    "    else:\n",
    "        print(\"No valid features found for influence calculation\")\n",
    "        return unlabeled.head(n_suggestions)\n",
    "\n",
    "\n",
    "# Analyze current distribution\n",
    "full_segments, labeled_segments = analyze_full_distribution(matrix, labels_df)\n",
    "\n",
    "# Get samples for each segment\n",
    "samples = get_unlabeled_samples(matrix, labels_df, samples_per_segment=50)\n",
    "\n",
    "# Export samples for labeling\n",
    "export_samples_for_labeling(samples)\n",
    "\n",
    "# Get priority suggestions\n",
    "priority_accounts = suggest_priority_accounts(matrix, labels_df, n_suggestions=50)\n",
    "\n",
    "print(\"\\nTop accounts to consider for labeling:\")\n",
    "print(priority_accounts.select([\n",
    "    'fid', 'fname', 'influence_score', 'follower_count', 'cast_count', 'total_reactions'\n",
    "]).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get indices where bot probability is high\n",
    "bot_mask = y_pred_proba_filtered > 0.9\n",
    "bot_fids = fids_array[bot_mask]\n",
    "\n",
    "# Ensure FIDs are the same type (Int64)\n",
    "high_conf_bots = unlabeled_data_filtered.filter(pl.col('fid').cast(pl.Int64).is_in(bot_fids))\n",
    "bot_profiles = valid_profiles.with_columns(pl.col('fid').cast(pl.Int64)).filter(pl.col('fid').is_in(bot_fids))\n",
    "\n",
    "# Sample 10 random high confidence bots\n",
    "sample_size = min(10, len(bot_profiles))\n",
    "random_indices = np.random.choice(range(len(bot_profiles)), sample_size, replace=False)\n",
    "random_bot_sample = bot_profiles.slice(random_indices[0], sample_size)\n",
    "\n",
    "print(f\"\\nSample of High Confidence Bot Predictions (from {len(bot_fids)} total high-confidence bots):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for row in random_bot_sample.iter_rows():\n",
    "    fname = row[0]  # fname is first column\n",
    "    fid = row[6]    # fid is last column\n",
    "    display_name = row[1]  # display_name is second column\n",
    "    \n",
    "    # Get features for this bot\n",
    "    bot_features = high_conf_bots.filter(pl.col('fid') == fid)\n",
    "    \n",
    "    # Find index in original arrays for probability and SHAP values\n",
    "    fid_idx = np.where(fids_array == fid)[0][0]\n",
    "    bot_prob = y_pred_proba_filtered[fid_idx]\n",
    "    \n",
    "    # Get SHAP values explaining why it's classified as a bot\n",
    "    shap_values = detector.get_feature_explanations('xgb', X_unlabeled_filtered, fid_idx)\n",
    "    \n",
    "    print(f\"\\nUsername: {fname}\")\n",
    "    print(f\"Display Name: {display_name}\")\n",
    "    print(f\"FID: {fid}\")\n",
    "    print(f\"Bot Probability: {bot_prob:.2%}\")\n",
    "    \n",
    "    print(\"\\nTop reasons for bot classification:\")\n",
    "    # Sort by absolute value but only show positive values (contributing to bot classification)\n",
    "    sorted_reasons = sorted(shap_values.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    for feature, impact in sorted_reasons:\n",
    "        if impact > 0:  # Only show features pushing towards bot classification\n",
    "            print(f\"  - {feature}: {impact:.3f}\")\n",
    "    \n",
    "    print(\"\\nKey Behavioral Metrics:\")\n",
    "    metrics_to_check = [\n",
    "        'rapid_actions', \n",
    "        'std_hours_between_actions',\n",
    "        'avg_hours_between_actions',\n",
    "        'following_count',\n",
    "        'follower_count',\n",
    "        'total_activity',\n",
    "        'hour_diversity',\n",
    "        'weekday_diversity'\n",
    "    ]\n",
    "    \n",
    "    for metric in metrics_to_check:\n",
    "        if metric in bot_features.columns:\n",
    "            val = bot_features.select(metric).item()\n",
    "            if isinstance(val, (int, float)):\n",
    "                print(f\"{metric}: {val:.2f}\")\n",
    "            else:\n",
    "                print(f\"{metric}: {val}\")\n",
    "            \n",
    "    print(f\"\\nProfile link: https://warpcast.com/{fname}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nTo verify these accounts, check for:\")\n",
    "print(\"1. Highly regular posting patterns (low std_hours_between_actions)\")\n",
    "print(\"2. Unusually high activity rates (high rapid_actions)\")\n",
    "print(\"3. Unnatural timing patterns (low hour_diversity and weekday_diversity)\")\n",
    "print(\"4. Suspicious follower/following ratios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_features(segment: str) -> List[str]:\n",
    "    base_features = ['has_ens', 'has_bio', 'has_avatar', 'verification_count']\n",
    "    \n",
    "    if segment == 'active':\n",
    "        return base_features + [\n",
    "            'cast_timing_entropy',\n",
    "            'reply_ratio',\n",
    "            'mention_patterns',\n",
    "            'engagement_rate'\n",
    "        ]\n",
    "    elif segment == 'low_activity':\n",
    "        return base_features + [\n",
    "            'network_growth_rate',\n",
    "            'initial_behavior_pattern',\n",
    "            'verification_sequence'\n",
    "        ]\n",
    "    else:  # dormant\n",
    "        return base_features + [\n",
    "            'follower_growth_velocity',\n",
    "            'network_structure',\n",
    "            'profile_completion_sequence'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def segment_users_by_behavior(matrix: pl.DataFrame) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Segment users based on behavioral patterns\"\"\"\n",
    "    segments = {\n",
    "        'power_users': matrix.filter(\n",
    "            (pl.col('cast_count') >= 20) & \n",
    "            (pl.col('reply_count') >= 5)\n",
    "        ),\n",
    "        'casual_users': matrix.filter(\n",
    "            (pl.col('cast_count') >= 5) & \n",
    "            (pl.col('cast_count') < 20)\n",
    "        ),\n",
    "        'one_time_users': matrix.filter(\n",
    "            (pl.col('cast_count') > 0) & \n",
    "            (pl.col('cast_count') < 5)\n",
    "        ),\n",
    "        'lurkers': matrix.filter(pl.col('cast_count') == 0)\n",
    "    }\n",
    "    \n",
    "    total = len(matrix)\n",
    "    print(\"\\nUser Segment Distribution:\")\n",
    "    for name, segment in segments.items():\n",
    "        size = len(segment)\n",
    "        if size > 0 and 'bot' in segment.columns:\n",
    "            bot_pct = (segment.filter(pl.col('bot') == 1).shape[0] / size) * 100\n",
    "            print(f\"{name}: {size:,} users ({size/total*100:.1f}%) - {bot_pct:.1f}% bots\")\n",
    "            \n",
    "            metrics = segment.select([\n",
    "                pl.col('cast_count').mean(),\n",
    "                pl.col('follower_count').mean(),\n",
    "                pl.col('following_count').mean()\n",
    "            ]).to_numpy()[0]\n",
    "            \n",
    "            print(f\"  Avg casts: {metrics[0]:.1f}\")\n",
    "            print(f\"  Avg followers: {metrics[1]:.1f}\")\n",
    "            print(f\"  Avg following: {metrics[2]:.1f}\")\n",
    "        else:\n",
    "            print(f\"{name}: {size:,} users ({size/total*100:.1f}%)\")\n",
    "            \n",
    "    return segments\n",
    "\n",
    "def get_segment_specific_features(segment_name: str) -> List[str]:\n",
    "    \"\"\"Get feature list specific to each behavior segment\"\"\"\n",
    "    base_features = [\n",
    "        'has_ens', 'has_bio', 'has_avatar', 'verification_count',  # verification_count here\n",
    "        'following_count', 'follower_count', 'follower_ratio',\n",
    "        'unique_follower_ratio', 'authenticity_score'\n",
    "    ]\n",
    "    \n",
    "    segment_features = {\n",
    "        'power_users': [\n",
    "            'cast_count', 'total_reactions', 'avg_cast_length',\n",
    "            'reply_count', 'mentions_count', 'engagement_score',\n",
    "            'weekday_diversity', 'hour_diversity', 'rapid_actions',\n",
    "            'avg_hours_between_actions', 'std_hours_between_actions',\n",
    "            'power_user_interaction_ratio', 'influence_score'\n",
    "        ],\n",
    "        'casual_users': [\n",
    "            'cast_count', 'total_reactions', 'engagement_score',\n",
    "            'reply_count', 'rapid_actions', 'avg_hours_between_actions',\n",
    "            'avg_cast_length', 'mentions_count'\n",
    "        ],\n",
    "        'one_time_users': [\n",
    "            'cast_count', 'total_reactions', 'profile_update_consistency',\n",
    "            'follower_growth_rate'\n",
    "        ],\n",
    "        'lurkers': [\n",
    "            'profile_update_consistency', 'network_balance',\n",
    "            'follower_growth_rate',\n",
    "            'profile_completeness'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return base_features + segment_features.get(segment_name, [])\n",
    "\n",
    "class RapidModelEvaluator:\n",
    "    def __init__(self, n_cv_splits: int = 5):\n",
    "        self.n_cv_splits = n_cv_splits\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate_model(self, name: str, model: Any, X: np.ndarray, y: np.ndarray) -> Dict:\n",
    "        \"\"\"Quickly evaluate a model with cross-validation\"\"\"\n",
    "        scoring = {\n",
    "            'precision': make_scorer(lambda y_true, y_pred: \n",
    "                precision_recall_fscore_support(y_true, y_pred)[0].mean()),\n",
    "            'recall': make_scorer(lambda y_true, y_pred: \n",
    "                precision_recall_fscore_support(y_true, y_pred)[1].mean()),\n",
    "            'f1': 'f1'\n",
    "        }\n",
    "        \n",
    "        cv_results = cross_validate(\n",
    "            model, X, y,\n",
    "            cv=self.n_cv_splits,\n",
    "            scoring=scoring,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        self.results[name] = {\n",
    "            'test_scores': {\n",
    "                'precision': cv_results['test_precision'].mean(),\n",
    "                'recall': cv_results['test_recall'].mean(),\n",
    "                'f1': cv_results['test_f1'].mean()\n",
    "            },\n",
    "            'fit_time': cv_results['fit_time'].mean()\n",
    "        }\n",
    "        \n",
    "        return self.results[name]\n",
    "    \n",
    "    def compare_models(self, segment_name: str, models: Dict[str, Any], \n",
    "                      X: np.ndarray, y: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"Compare multiple models quickly\"\"\"\n",
    "        self.results = {}  # Reset results for each segment\n",
    "        for name, model in models.items():\n",
    "            print(f\"Evaluating {name} on {segment_name} segment...\")\n",
    "            self.evaluate_model(f\"{name}\", model, X, y)\n",
    "        \n",
    "        results_df = pd.DataFrame.from_dict(\n",
    "            {k: v['test_scores'] for k, v in self.results.items()}, \n",
    "            orient='index'\n",
    "        )\n",
    "        results_df['fit_time'] = [v['fit_time'] for v in self.results.values()]\n",
    "        \n",
    "        return results_df.sort_values('f1', ascending=False)\n",
    "\n",
    "def prepare_segment_features(segment: pl.DataFrame, segment_name: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare features for a given segment\"\"\"\n",
    "    feature_cols = get_segment_specific_features(segment_name)\n",
    "    valid_features = [col for col in feature_cols if col in segment.columns]\n",
    "    print(f\"\\nUsing {len(valid_features)} features for {segment_name}:\", valid_features)\n",
    "    \n",
    "    X = segment.select(valid_features).fill_null(0).to_numpy()\n",
    "    y = segment['bot'].to_numpy() if 'bot' in segment.columns else None\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y, valid_features  # Return valid_features as well\n",
    "\n",
    "def evaluate_segments(matrix: pl.DataFrame, labels_df: pl.DataFrame) -> Dict:\n",
    "    \"\"\"Evaluate models on each behavioral segment\"\"\"\n",
    "    # Ensure consistent FID type\n",
    "    matrix = matrix.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "    labels_df = labels_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "    \n",
    "    # Join with labels\n",
    "    data = matrix.join(labels_df, on='fid', how='inner')\n",
    "    \n",
    "    # Create segments\n",
    "    segmented_users = segment_users_by_behavior(data)\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = RapidModelEvaluator()\n",
    "    \n",
    "    # Store results\n",
    "    segment_results = {}\n",
    "    \n",
    "    # For each segment\n",
    "    for segment_name, segment_data in segmented_users.items():\n",
    "        print(f\"\\nProcessing {segment_name} segment...\")\n",
    "        \n",
    "        # Prepare features\n",
    "        X, y, valid_features = prepare_segment_features(segment_data, segment_name)\n",
    "        \n",
    "        if y is None or len(np.unique(y)) < 2:\n",
    "            print(f\"Skipping {segment_name} - insufficient labels\")\n",
    "            continue\n",
    "        \n",
    "        # Define models with balanced class weights\n",
    "        models = {\n",
    "            'xgb': xgb.XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                scale_pos_weight=sum(y == 0) / sum(y == 1),\n",
    "                random_state=42\n",
    "            ),\n",
    "            'lgbm': LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ),\n",
    "            'rf': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=4,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ),\n",
    "            'logistic': LogisticRegression(\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Evaluate models\n",
    "        results = evaluator.compare_models(segment_name, models, X, y)\n",
    "        segment_results[segment_name] = results\n",
    "        \n",
    "        print(f\"\\nResults for {segment_name}:\")\n",
    "        print(results)\n",
    "        \n",
    "        # Print feature importance for best model\n",
    "        try:\n",
    "            best_model_name = results.index[0]\n",
    "            best_model = models[best_model_name]\n",
    "            \n",
    "            if hasattr(best_model, 'feature_importances_'):\n",
    "                importances = pd.DataFrame({\n",
    "                    'feature': valid_features,\n",
    "                    'importance': best_model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(f\"\\nTop 10 features for {segment_name}:\")\n",
    "                print(importances.head(10))\n",
    "            else:\n",
    "                print(f\"\\nNo feature importances available for {best_model_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating feature importance: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return segment_results\n",
    "\n",
    "\n",
    "results = evaluate_segments(matrix, labels_df)\n",
    "\n",
    "print(\"\\nBest models per segment:\")\n",
    "for segment_name, result_df in results.items():\n",
    "    best_model = result_df.index[0]\n",
    "    best_f1 = result_df.iloc[0]['f1']\n",
    "    best_precision = result_df.iloc[0]['precision']\n",
    "    best_recall = result_df.iloc[0]['recall']\n",
    "    print(f\"{segment_name}:\")\n",
    "    print(f\"  Best model: {best_model}\")\n",
    "    print(f\"  F1: {best_f1:.3f}\")\n",
    "    print(f\"  Precision: {best_precision:.3f}\")\n",
    "    print(f\"  Recall: {best_recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_full_distribution(matrix: pl.DataFrame, labels_df: pl.DataFrame) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"Analyze distribution in both full and labeled datasets\"\"\"\n",
    "    print(\"=== Full Dataset Distribution ===\")\n",
    "    full_segments = segment_users_by_behavior(matrix)\n",
    "    \n",
    "    print(\"\\n=== Labeled Dataset Distribution ===\")\n",
    "    labeled_data = matrix.join(labels_df, on='fid', how='inner')\n",
    "    labeled_segments = segment_users_by_behavior(labeled_data)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    print(\"\\n=== Label Coverage by Segment ===\")\n",
    "    for segment_name in full_segments.keys():\n",
    "        full_count = len(full_segments[segment_name])\n",
    "        labeled_count = len(labeled_segments.get(segment_name, pl.DataFrame()))\n",
    "        coverage = (labeled_count / full_count * 100) if full_count > 0 else 0\n",
    "        print(f\"{segment_name}: {coverage:.1f}% labeled ({labeled_count}/{full_count})\")\n",
    "        \n",
    "    return full_segments, labeled_segments\n",
    "def get_unlabeled_samples(matrix: pl.DataFrame, \n",
    "                         labels_df: pl.DataFrame, \n",
    "                         samples_per_segment: int = 50,\n",
    "                         use_isolation_forest: bool = True) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Get stratified samples of unlabeled data, optionally using anomaly detection\"\"\"\n",
    "    \n",
    "    # Get unlabeled FIDs\n",
    "    labeled_fids = labels_df['fid'].unique()\n",
    "    unlabeled = matrix.filter(~pl.col('fid').is_in(labeled_fids))\n",
    "    \n",
    "    # Segment unlabeled data\n",
    "    segments = segment_users_by_behavior(unlabeled)\n",
    "    \n",
    "    # Features for anomaly detection\n",
    "    anomaly_features = [\n",
    "        'cast_count', 'follower_count', 'following_count',\n",
    "        'authenticity_score', 'total_reactions', 'rapid_actions',\n",
    "        'avg_hours_between_actions', 'std_hours_between_actions'\n",
    "    ]\n",
    "    \n",
    "    samples = {}\n",
    "    for name, segment in segments.items():\n",
    "        print(f\"\\nProcessing {name} segment ({len(segment)} users)\")\n",
    "        \n",
    "        if len(segment) == 0:\n",
    "            continue\n",
    "            \n",
    "        if use_isolation_forest and len(segment) > samples_per_segment:\n",
    "            # Prepare features for anomaly detection\n",
    "            valid_features = [f for f in anomaly_features if f in segment.columns]\n",
    "            if len(valid_features) > 0:\n",
    "                X = segment.select(valid_features).fill_null(0).to_numpy()\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "                \n",
    "                # Use Isolation Forest to identify anomalies\n",
    "                iso_forest = IsolationForest(\n",
    "                    n_estimators=100,\n",
    "                    contamination=0.1,  # Assume 10% anomalies\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                # Get anomaly scores\n",
    "                scores = iso_forest.fit_predict(X)\n",
    "                anomaly_indices = np.where(scores == -1)[0]\n",
    "                normal_indices = np.where(scores == 1)[0]\n",
    "                \n",
    "                # Sample both anomalies and normal cases\n",
    "                n_anomalies = min(samples_per_segment // 4, len(anomaly_indices))\n",
    "                n_normal = samples_per_segment - n_anomalies\n",
    "                \n",
    "                # Create a filter for selected indices\n",
    "                selected_indices = np.concatenate([\n",
    "                    np.random.choice(anomaly_indices, n_anomalies, replace=False),\n",
    "                    np.random.choice(normal_indices, n_normal, replace=False)\n",
    "                ])\n",
    "                \n",
    "                # Create a row number column and filter by selected indices\n",
    "                samples[name] = (segment\n",
    "                    .with_row_count(\"row_nr\")\n",
    "                    .filter(pl.col(\"row_nr\").is_in(selected_indices))\n",
    "                    .drop(\"row_nr\"))\n",
    "                \n",
    "                print(f\"Selected {n_anomalies} potential anomalies and {n_normal} normal cases\")\n",
    "            else:\n",
    "                # Fallback to random sampling if features not available\n",
    "                samples[name] = segment.sample(n=samples_per_segment, seed=42)\n",
    "        else:\n",
    "            # For small segments, take all samples\n",
    "            n_samples = min(samples_per_segment, len(segment))\n",
    "            samples[name] = segment.sample(n=n_samples, seed=42)\n",
    "        \n",
    "        print(f\"Final sample size: {len(samples[name])}\")\n",
    "        \n",
    "        # Print some statistics about the sample\n",
    "        if 'cast_count' in segment.columns:\n",
    "            stats = samples[name].select([\n",
    "                pl.col('cast_count').mean().alias('avg_casts'),\n",
    "                pl.col('follower_count').mean().alias('avg_followers'),\n",
    "                pl.col('following_count').mean().alias('avg_following')\n",
    "            ])\n",
    "            print(\"Sample statistics:\")\n",
    "            print(f\"  Avg casts: {stats['avg_casts'][0]:.1f}\")\n",
    "            print(f\"  Avg followers: {stats['avg_followers'][0]:.1f}\")\n",
    "            print(f\"  Avg following: {stats['avg_following'][0]:.1f}\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def export_samples_for_labeling(samples: Dict[str, pl.DataFrame], \n",
    "                              output_path: str = \"samples_for_labeling.csv\"):\n",
    "    \"\"\"Export samples for manual labeling\"\"\"\n",
    "    # Combine all samples\n",
    "    all_samples = pl.concat([\n",
    "        segment.with_columns(pl.lit(name).alias('segment'))\n",
    "        for name, segment in samples.items()\n",
    "    ])\n",
    "    \n",
    "    # Select relevant columns for labeling\n",
    "    export_columns = [\n",
    "        'fid', 'segment', 'fname',\n",
    "        'cast_count', 'follower_count', 'following_count',\n",
    "        'total_reactions', 'authenticity_score'\n",
    "    ]\n",
    "    \n",
    "    # Only include columns that exist\n",
    "    valid_columns = [col for col in export_columns if col in all_samples.columns]\n",
    "    \n",
    "    # Export to CSV\n",
    "    all_samples.select(valid_columns).write_csv(output_path)\n",
    "    print(f\"\\nExported {len(all_samples)} samples to {output_path}\")\n",
    "    \n",
    "def suggest_priority_accounts(matrix: pl.DataFrame, \n",
    "                              labels_df: pl.DataFrame,\n",
    "                              n_suggestions: int = 50) -> pl.DataFrame:\n",
    "    \"\"\"Suggest priority accounts for labeling based on influence and uncertainty\"\"\"\n",
    "    \n",
    "    # Get unlabeled accounts\n",
    "    labeled_fids = labels_df['fid'].unique()\n",
    "    unlabeled = matrix.filter(~pl.col('fid').is_in(labeled_fids))\n",
    "    \n",
    "    # Calculate influence score\n",
    "    influence_features = [\n",
    "        'follower_count', 'following_count', 'cast_count',\n",
    "        'total_reactions', 'authenticity_score'\n",
    "    ]\n",
    "    \n",
    "    # Only use available features\n",
    "    valid_features = [f for f in influence_features if f in unlabeled.columns]\n",
    "    \n",
    "    if len(valid_features) > 0:\n",
    "        # Normalize features\n",
    "        normalized = unlabeled.select([\n",
    "            'fid',\n",
    "            *(pl.col(f).fill_null(0) / pl.col(f).fill_null(0).max() for f in valid_features)\n",
    "        ])\n",
    "        \n",
    "        # Simple influence score - average of normalized features\n",
    "        influence_df = normalized.with_columns([\n",
    "            pl.fold(acc=0, function=lambda acc, x: acc + x, exprs=[pl.col(f) for f in valid_features])\n",
    "            .alias('influence_score')\n",
    "        ])\n",
    "        \n",
    "        # Get top influential accounts\n",
    "        suggestions = influence_df.sort('influence_score', descending=True).head(n_suggestions)\n",
    "        \n",
    "        # Select only necessary columns for the join\n",
    "        suggestions = suggestions.select(['fid', 'influence_score'])\n",
    "        \n",
    "        # Join back with original features, including 'fname'\n",
    "        result = suggestions.join(unlabeled, on='fid', how='left', suffix=\"_unlabeled\")\n",
    "        \n",
    "        # Ensure 'fname' is included in the final result\n",
    "        if 'fname' in result.columns:\n",
    "            return result.select(['fid', 'fname', 'influence_score', 'follower_count', 'cast_count', 'total_reactions'])\n",
    "        else:\n",
    "            print(\"Warning: 'fname' column not found in the dataset.\")\n",
    "            return result.select(['fid', 'influence_score', 'follower_count', 'cast_count', 'total_reactions'])\n",
    "    else:\n",
    "        print(\"No valid features found for influence calculation\")\n",
    "        return unlabeled.head(n_suggestions)\n",
    "\n",
    "\n",
    "# Analyze current distribution\n",
    "full_segments, labeled_segments = analyze_full_distribution(matrix, labels_df)\n",
    "\n",
    "# Get samples for each segment\n",
    "samples = get_unlabeled_samples(matrix, labels_df, samples_per_segment=50)\n",
    "\n",
    "# Export samples for labeling\n",
    "export_samples_for_labeling(samples)\n",
    "\n",
    "# Get priority suggestions\n",
    "priority_accounts = suggest_priority_accounts(matrix, labels_df, n_suggestions=50)\n",
    "\n",
    "print(\"\\nTop accounts to consider for labeling:\")\n",
    "print(priority_accounts.select([\n",
    "    'fid', 'fname', 'influence_score', 'follower_count', 'cast_count', 'total_reactions'\n",
    "]).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Re-initialize after building\n",
    "feature_eng = FeatureEngineering(\"data\", \"checkpoints\")\n",
    "detector = SybilDetectionSystem(feature_eng)\n",
    "\n",
    "# Build feature matrix\n",
    "matrix = feature_eng.build_feature_matrix()\n",
    "print(\"Feature matrix built\")\n",
    "\n",
    "# Load labels\n",
    "labels_df = pl.read_csv('data/labels.csv')\n",
    "labels_df = labels_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "\n",
    "# Ensure matrix fid is Int64\n",
    "matrix = matrix.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "\n",
    "# Join with matching types\n",
    "data = matrix.join(labels_df, on='fid', how='inner')\n",
    "\n",
    "# Extract features and labels\n",
    "X, feature_names = detector.prepare_features(data.drop(['bot', 'fid']))\n",
    "y = data['bot'].to_numpy()\n",
    "\n",
    "# Perform train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "fids = data['fid'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_fids, test_fids = train_test_split(\n",
    "    X, y, fids,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # ensures class distribution is preserved\n",
    ")\n",
    "\n",
    "# Define a path to save the model checkpoint\n",
    "model_checkpoint_path = \"checkpoints/sybil_detector_model.pkl\"\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "# Check if a model checkpoint already exists\n",
    "if os.path.exists(model_checkpoint_path):\n",
    "    # Load the detector object (including the trained model)\n",
    "    detector = joblib.load(model_checkpoint_path)\n",
    "    print(\"Loaded model checkpoint. Skipping training.\")\n",
    "else:\n",
    "    # Train the model using the training set only\n",
    "    detector.train(X_train, y_train, feature_names)\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    joblib.dump(detector, model_checkpoint_path)\n",
    "    print(f\"Model checkpoint saved to {model_checkpoint_path}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred_proba = detector.model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Compute and plot SHAP for X_test:\n",
    "shap_values_test = detector.shap_explainers['xgb'].shap_values(X_test)\n",
    "if isinstance(shap_values_test, list) and len(shap_values_test) > 1:\n",
    "    shap_values_test = shap_values_test[1]\n",
    "assert shap_values_test.shape[0] == X_test.shape[0], \"Mismatch in rows between shap_values and X_test!\"\n",
    "shap.summary_plot(shap_values_test, X_test, feature_names=feature_names)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()\n",
    "\n",
    "# Compute evaluation metrics\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred)\n",
    "test_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest ROC AUC:\", test_roc_auc)\n",
    "print(\"Test F1 Score:\", test_f1)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "\n",
    "# SHAP explanations for a specific instance\n",
    "instance_index = 0\n",
    "model_name = 'xgb'\n",
    "explanations = detector.get_feature_explanations(model_name, X_test, instance_index)\n",
    "print(f\"\\nSHAP explanations for instance {instance_index} from {model_name}:\")\n",
    "print(explanations)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Authentic', 'Bot'], yticklabels=['Authentic', 'Bot'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score\n",
    "\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "print(\"Cohen's Kappa:\", kappa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 1. Get all FIDs from the full matrix\n",
    "all_fids = matrix['fid'].unique()\n",
    "labeled_fids = data['fid'].unique()\n",
    "\n",
    "# 2. Convert to numpy arrays for set operations\n",
    "all_fids_np = all_fids.to_numpy()\n",
    "labeled_fids_np = labeled_fids.to_numpy()\n",
    "\n",
    "# 3. Find FIDs that aren't in the labeled dataset\n",
    "unlabeled_fids = np.setdiff1d(all_fids_np, labeled_fids_np)\n",
    "\n",
    "# 4. Filter matrix to get unlabeled data\n",
    "unlabeled_data = matrix.filter(pl.col('fid').is_in(unlabeled_fids))\n",
    "\n",
    "# 5. Get profiles with fnames\n",
    "ds = feature_eng.loader.get_dataset('profile_with_addresses')\n",
    "valid_profiles = ds.filter(pl.col('fname').is_not_null())\n",
    "\n",
    "# Ensure valid_profiles has unique fids\n",
    "valid_profiles = valid_profiles.unique(subset=['fid'])\n",
    "\n",
    "# 5. Get profiles with fnames (already deduplicated above)\n",
    "valid_fids = valid_profiles['fid'].unique()\n",
    "\n",
    "# 6. Filter unlabeled data to only include profiles with fnames\n",
    "unlabeled_data_filtered = unlabeled_data.filter(pl.col('fid').is_in(valid_fids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'profile_update_consistency' column exists\n",
    "if 'profile_update_consistency' in unlabeled_data_filtered.columns:\n",
    "    print(unlabeled_data_filtered.select(['profile_update_consistency']).describe())\n",
    "else:\n",
    "    print(\"Column 'profile_update_consistency' not found in unlabeled_data_filtered.\")\n",
    "\n",
    "# Compute statistics for given features\n",
    "for feature in ['profile_update_consistency', 'influence_score', 'follower_count']:\n",
    "    if feature in data.columns:\n",
    "        bot_values = data.filter(pl.col('bot') == 1)[feature]\n",
    "        human_values = data.filter(pl.col('bot') == 0)[feature]\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"\\n{feature} statistics:\")\n",
    "        print(\"Bot mean:\", bot_values.mean())\n",
    "        print(\"Human mean:\", human_values.mean())\n",
    "        print(\"Bot std:\", bot_values.std())\n",
    "        print(\"Human std:\", human_values.std())\n",
    "    else:\n",
    "        print(f\"\\nFeature '{feature}' does not exist in the data DataFrame.\")\n",
    "\n",
    "# 7. Prepare features and predictions for unlabeled data\n",
    "X_unlabeled_filtered, valid_features = detector.prepare_features(unlabeled_data_filtered.drop(\"fid\"))\n",
    "\n",
    "# Debug prints to ensure lengths match\n",
    "print(f\"unlabeled_data_filtered shape: {unlabeled_data_filtered.shape}\")\n",
    "print(f\"X_unlabeled_filtered shape: {X_unlabeled_filtered.shape}\")\n",
    "print(f\"model feature length: {len(detector.feature_names)}\")\n",
    "if X_unlabeled_filtered.shape[0] != len(unlabeled_data_filtered):\n",
    "    print(\"Warning: Length mismatch between unlabeled_data_filtered and X_unlabeled_filtered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_filtered = detector.model.predict_proba(X_unlabeled_filtered)[:, 1]\n",
    "y_pred_filtered = (y_pred_proba_filtered >= 0.5).astype(int)\n",
    "\n",
    "# Compute SHAP values for unlabeled data\n",
    "# Extract the underlying xgb model from the ensemble if needed\n",
    "xgb_model = detector.base_models['xgb'].base_estimator_ if hasattr(detector.base_models['xgb'], 'base_estimator_') else detector.base_models['xgb']\n",
    "\n",
    "explainer_unlabeled = shap.TreeExplainer(xgb_model.estimator)\n",
    "shap_values_unlabeled = explainer_unlabeled.shap_values(X_unlabeled_filtered)\n",
    "\n",
    "\n",
    "# 8. Get SHAP explanations for human predictions\n",
    "human_explanations = []\n",
    "# sample only 10 random accounts\n",
    "fids_array = unlabeled_data_filtered.sample(10)['fid'].to_numpy()\n",
    "\n",
    "# filter only values that would have a SHAP value \n",
    "\n",
    "for i, (pred, fid) in enumerate(zip(y_pred_filtered, fids_array)):\n",
    "    # Ensure i is within the bounds of X_unlabeled_filtered\n",
    "    if i >= X_unlabeled_filtered.shape[0]:\n",
    "        print(f\"Index {i} is out of range for X_unlabeled_filtered of size {X_unlabeled_filtered.shape[0]}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if pred == 0:  # If predicted human\n",
    "        # Retrieve SHAP values from the unlabeled SHAP explainer\n",
    "        shap_values = detector.get_feature_explanations('xgb', X_unlabeled_filtered, i)\n",
    "\n",
    "        # Filter row by fid\n",
    "        row = unlabeled_data_filtered.filter(pl.col('fid') == fid)\n",
    "\n",
    "        # Handle authenticity_score\n",
    "        if 'authenticity_score' in row.columns and len(row) == 1:\n",
    "            authenticity_score = row['authenticity_score'].item()\n",
    "        else:\n",
    "            authenticity_score = 0.0  # default\n",
    "\n",
    "        # Get fname for this fid\n",
    "        fname_series = valid_profiles.filter(pl.col('fid') == fid)['fname']\n",
    "        if len(fname_series) == 1:\n",
    "            fname = fname_series.item()\n",
    "        elif len(fname_series) == 0:\n",
    "            fname = \"Unknown\"\n",
    "        else:\n",
    "            fname = fname_series[0]\n",
    "\n",
    "        # Only negative impact features\n",
    "        negative_reasons = {k: v for k, v in shap_values.items() if v < 0}\n",
    "\n",
    "        human_explanations.append({\n",
    "            'fid': fid,\n",
    "            'fname': fname,\n",
    "            'authenticity_score': authenticity_score,\n",
    "            'human_probability': 1 - y_pred_proba_filtered[i],\n",
    "            'reasons': negative_reasons\n",
    "        })\n",
    "\n",
    "# Print results\n",
    "sample_size = min(10, len(human_explanations))\n",
    "random_accounts = random.sample(human_explanations, sample_size)\n",
    "\n",
    "print(\"\\nHuman accounts with explanations (random sample):\")\n",
    "for i, exp in enumerate(random_accounts, start=1):\n",
    "    print(f\"\\n{i}. {exp['fname']} (FID: {exp['fid']})\")\n",
    "    print(f\"Human Probability: {exp['human_probability']:.2%}\")\n",
    "    print(f\"Authenticity Score: {exp['authenticity_score']:.2f}\")\n",
    "    print(\"Top reasons for human classification:\")\n",
    "    sorted_reasons = sorted(exp['reasons'].items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    for feature, impact in sorted_reasons:\n",
    "        print(f\"  - {feature}: {abs(impact):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distribution for entire unlabeled dataset\n",
    "total_profiles = len(y_pred_filtered)\n",
    "humans_count = (y_pred_filtered == 0).sum()\n",
    "bots_count = (y_pred_filtered == 1).sum()\n",
    "\n",
    "print(\"\\nOverall Distribution Analysis:\")\n",
    "print(f\"Total profiles analyzed: {total_profiles:,}\")\n",
    "print(f\"Predicted humans: {humans_count:,} ({(humans_count/total_profiles)*100:.1f}%)\")\n",
    "print(f\"Predicted bots: {bots_count:,} ({(bots_count/total_profiles)*100:.1f}%)\")\n",
    "\n",
    "# Probability distribution analysis\n",
    "print(\"\\nPrediction Probability Analysis:\")\n",
    "print(f\"Mean bot probability: {y_pred_proba_filtered.mean():.3f}\")\n",
    "print(f\"Median bot probability: {np.median(y_pred_proba_filtered):.3f}\")\n",
    "print(f\"Std dev of bot probability: {y_pred_proba_filtered.std():.3f}\")\n",
    "\n",
    "# Distribution buckets for more detailed view\n",
    "buckets = np.histogram(y_pred_proba_filtered, bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "bucket_counts = buckets[0]\n",
    "bucket_ranges = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\n",
    "\n",
    "print(\"\\nProbability Distribution Breakdown:\")\n",
    "for range_name, count in zip(bucket_ranges, bucket_counts):\n",
    "    percentage = (count/total_profiles)*100\n",
    "    print(f\"{range_name}: {count:,} accounts ({percentage:.1f}%)\")\n",
    "\n",
    "# High confidence predictions\n",
    "high_conf_human = (y_pred_proba_filtered < 0.2).sum()\n",
    "high_conf_bot = (y_pred_proba_filtered > 0.8).sum()\n",
    "\n",
    "print(\"\\nHigh Confidence Predictions:\")\n",
    "print(f\"High confidence humans (p < 0.2): {high_conf_human:,} ({(high_conf_human/total_profiles)*100:.1f}%)\")\n",
    "print(f\"High confidence bots (p > 0.8): {high_conf_bot:,} ({(high_conf_bot/total_profiles)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create histogram\n",
    "bins = np.linspace(0, 1, 21)  # 20 bins for smooth distribution\n",
    "n, bins, patches = ax.hist(y_pred_proba_filtered, bins=bins, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Customize colors based on probability ranges\n",
    "for i, patch in enumerate(patches):\n",
    "    bin_center = bins[i] + (bins[1] - bins[0])/2\n",
    "    if bin_center < 0.2:\n",
    "        patch.set_facecolor('#2ecc71')  # Green for human predictions\n",
    "    elif bin_center > 0.8:\n",
    "        patch.set_facecolor('#e74c3c')  # Red for bot predictions\n",
    "    else:\n",
    "        patch.set_facecolor('#3498db')  # Blue for uncertain predictions\n",
    "\n",
    "# Add vertical lines for key thresholds\n",
    "ax.axvline(x=0.2, color='#27ae60', linestyle='--', alpha=0.5, label='Human threshold (0.2)')\n",
    "ax.axvline(x=0.8, color='#c0392b', linestyle='--', alpha=0.5, label='Bot threshold (0.8)')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Distribution of Bot Probability Scores', pad=20, fontsize=14)\n",
    "ax.set_xlabel('Bot Probability Score', fontsize=12)\n",
    "ax.set_ylabel('Number of Accounts', fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Add text annotations for key statistics\n",
    "stats_text = (\n",
    "    f'Total Profiles: {total_profiles:,}\\n'\n",
    "    f'Mean Probability: {y_pred_proba_filtered.mean():.3f}\\n'\n",
    "    f'Median Probability: {np.median(y_pred_proba_filtered):.3f}\\n'\n",
    "    f'Std Dev: {y_pred_proba_filtered.std():.3f}'\n",
    ")\n",
    "plt.text(0.95, 0.95, stats_text,\n",
    "         transform=ax.transAxes,\n",
    "         verticalalignment='top',\n",
    "         horizontalalignment='right',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Adjust layout to prevent text cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_features(segment: str) -> List[str]:\n",
    "    base_features = ['has_ens', 'has_bio', 'has_avatar', 'verification_count']\n",
    "    \n",
    "    if segment == 'active':\n",
    "        return base_features + [\n",
    "            'cast_timing_entropy',\n",
    "            'reply_ratio',\n",
    "            'mention_patterns',\n",
    "            'engagement_rate'\n",
    "        ]\n",
    "    elif segment == 'low_activity':\n",
    "        return base_features + [\n",
    "            'network_growth_rate',\n",
    "            'initial_behavior_pattern',\n",
    "            'verification_sequence'\n",
    "        ]\n",
    "    else:  # dormant\n",
    "        return base_features + [\n",
    "            'follower_growth_velocity',\n",
    "            'network_structure',\n",
    "            'profile_completion_sequence'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def segment_users_by_behavior(matrix: pl.DataFrame) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Segment users based on behavioral patterns\"\"\"\n",
    "    segments = {\n",
    "        'power_users': matrix.filter(\n",
    "            (pl.col('cast_count') >= 20) & \n",
    "            (pl.col('reply_count') >= 5)\n",
    "        ),\n",
    "        'casual_users': matrix.filter(\n",
    "            (pl.col('cast_count') >= 5) & \n",
    "            (pl.col('cast_count') < 20)\n",
    "        ),\n",
    "        'one_time_users': matrix.filter(\n",
    "            (pl.col('cast_count') > 0) & \n",
    "            (pl.col('cast_count') < 5)\n",
    "        ),\n",
    "        'lurkers': matrix.filter(pl.col('cast_count') == 0)\n",
    "    }\n",
    "    \n",
    "    total = len(matrix)\n",
    "    print(\"\\nUser Segment Distribution:\")\n",
    "    for name, segment in segments.items():\n",
    "        size = len(segment)\n",
    "        if size > 0 and 'bot' in segment.columns:\n",
    "            bot_pct = (segment.filter(pl.col('bot') == 1).shape[0] / size) * 100\n",
    "            print(f\"{name}: {size:,} users ({size/total*100:.1f}%) - {bot_pct:.1f}% bots\")\n",
    "            \n",
    "            metrics = segment.select([\n",
    "                pl.col('cast_count').mean(),\n",
    "                pl.col('follower_count').mean(),\n",
    "                pl.col('following_count').mean()\n",
    "            ]).to_numpy()[0]\n",
    "            \n",
    "            print(f\"  Avg casts: {metrics[0]:.1f}\")\n",
    "            print(f\"  Avg followers: {metrics[1]:.1f}\")\n",
    "            print(f\"  Avg following: {metrics[2]:.1f}\")\n",
    "        else:\n",
    "            print(f\"{name}: {size:,} users ({size/total*100:.1f}%)\")\n",
    "            \n",
    "    return segments\n",
    "\n",
    "def get_segment_specific_features(segment_name: str) -> List[str]:\n",
    "    \"\"\"Get feature list specific to each behavior segment\"\"\"\n",
    "    base_features = [\n",
    "        'has_ens', 'has_bio', 'has_avatar', 'verification_count',  # verification_count here\n",
    "        'following_count', 'follower_count', 'follower_ratio',\n",
    "        'unique_follower_ratio', 'authenticity_score'\n",
    "    ]\n",
    "    \n",
    "    segment_features = {\n",
    "        'power_users': [\n",
    "            'cast_count', 'total_reactions', 'avg_cast_length',\n",
    "            'reply_count', 'mentions_count', 'engagement_score',\n",
    "            'weekday_diversity', 'hour_diversity', 'rapid_actions',\n",
    "            'avg_hours_between_actions', 'std_hours_between_actions',\n",
    "            'power_user_interaction_ratio', 'influence_score'\n",
    "        ],\n",
    "        'casual_users': [\n",
    "            'cast_count', 'total_reactions', 'engagement_score',\n",
    "            'reply_count', 'rapid_actions', 'avg_hours_between_actions',\n",
    "            'avg_cast_length', 'mentions_count'\n",
    "        ],\n",
    "        'one_time_users': [\n",
    "            'cast_count', 'total_reactions', 'profile_update_consistency',\n",
    "            'follower_growth_rate'\n",
    "        ],\n",
    "        'lurkers': [\n",
    "            'profile_update_consistency', 'network_balance',\n",
    "            'follower_growth_rate',\n",
    "            'profile_completeness'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return base_features + segment_features.get(segment_name, [])\n",
    "\n",
    "class RapidModelEvaluator:\n",
    "    def __init__(self, n_cv_splits: int = 5):\n",
    "        self.n_cv_splits = n_cv_splits\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate_model(self, name: str, model: Any, X: np.ndarray, y: np.ndarray) -> Dict:\n",
    "        \"\"\"Quickly evaluate a model with cross-validation\"\"\"\n",
    "        scoring = {\n",
    "            'precision': make_scorer(lambda y_true, y_pred: \n",
    "                precision_recall_fscore_support(y_true, y_pred)[0].mean()),\n",
    "            'recall': make_scorer(lambda y_true, y_pred: \n",
    "                precision_recall_fscore_support(y_true, y_pred)[1].mean()),\n",
    "            'f1': 'f1'\n",
    "        }\n",
    "        \n",
    "        cv_results = cross_validate(\n",
    "            model, X, y,\n",
    "            cv=self.n_cv_splits,\n",
    "            scoring=scoring,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        self.results[name] = {\n",
    "            'test_scores': {\n",
    "                'precision': cv_results['test_precision'].mean(),\n",
    "                'recall': cv_results['test_recall'].mean(),\n",
    "                'f1': cv_results['test_f1'].mean()\n",
    "            },\n",
    "            'fit_time': cv_results['fit_time'].mean()\n",
    "        }\n",
    "        \n",
    "        return self.results[name]\n",
    "    \n",
    "    def compare_models(self, segment_name: str, models: Dict[str, Any], \n",
    "                      X: np.ndarray, y: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"Compare multiple models quickly\"\"\"\n",
    "        self.results = {}  # Reset results for each segment\n",
    "        for name, model in models.items():\n",
    "            print(f\"Evaluating {name} on {segment_name} segment...\")\n",
    "            self.evaluate_model(f\"{name}\", model, X, y)\n",
    "        \n",
    "        results_df = pd.DataFrame.from_dict(\n",
    "            {k: v['test_scores'] for k, v in self.results.items()}, \n",
    "            orient='index'\n",
    "        )\n",
    "        results_df['fit_time'] = [v['fit_time'] for v in self.results.values()]\n",
    "        \n",
    "        return results_df.sort_values('f1', ascending=False)\n",
    "\n",
    "def prepare_segment_features(segment: pl.DataFrame, segment_name: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare features for a given segment\"\"\"\n",
    "    feature_cols = get_segment_specific_features(segment_name)\n",
    "    valid_features = [col for col in feature_cols if col in segment.columns]\n",
    "    print(f\"\\nUsing {len(valid_features)} features for {segment_name}:\", valid_features)\n",
    "    \n",
    "    X = segment.select(valid_features).fill_null(0).to_numpy()\n",
    "    y = segment['bot'].to_numpy() if 'bot' in segment.columns else None\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y, valid_features  # Return valid_features as well\n",
    "\n",
    "def evaluate_segments(matrix: pl.DataFrame, labels_df: pl.DataFrame) -> Dict:\n",
    "    \"\"\"Evaluate models on each behavioral segment\"\"\"\n",
    "    # Ensure consistent FID type\n",
    "    matrix = matrix.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "    labels_df = labels_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "    \n",
    "    # Join with labels\n",
    "    data = matrix.join(labels_df, on='fid', how='inner')\n",
    "    \n",
    "    # Create segments\n",
    "    segmented_users = segment_users_by_behavior(data)\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = RapidModelEvaluator()\n",
    "    \n",
    "    # Store results\n",
    "    segment_results = {}\n",
    "    \n",
    "    # For each segment\n",
    "    for segment_name, segment_data in segmented_users.items():\n",
    "        print(f\"\\nProcessing {segment_name} segment...\")\n",
    "        \n",
    "        # Prepare features\n",
    "        X, y, valid_features = prepare_segment_features(segment_data, segment_name)\n",
    "        \n",
    "        if y is None or len(np.unique(y)) < 2:\n",
    "            print(f\"Skipping {segment_name} - insufficient labels\")\n",
    "            continue\n",
    "        \n",
    "        # Define models with balanced class weights\n",
    "        models = {\n",
    "            'xgb': xgb.XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                scale_pos_weight=sum(y == 0) / sum(y == 1),\n",
    "                random_state=42\n",
    "            ),\n",
    "            'lgbm': LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ),\n",
    "            'rf': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=4,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ),\n",
    "            'logistic': LogisticRegression(\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Evaluate models\n",
    "        results = evaluator.compare_models(segment_name, models, X, y)\n",
    "        segment_results[segment_name] = results\n",
    "        \n",
    "        print(f\"\\nResults for {segment_name}:\")\n",
    "        print(results)\n",
    "        \n",
    "        # Print feature importance for best model\n",
    "        try:\n",
    "            best_model_name = results.index[0]\n",
    "            best_model = models[best_model_name]\n",
    "            \n",
    "            if hasattr(best_model, 'feature_importances_'):\n",
    "                importances = pd.DataFrame({\n",
    "                    'feature': valid_features,\n",
    "                    'importance': best_model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(f\"\\nTop 10 features for {segment_name}:\")\n",
    "                print(importances.head(10))\n",
    "            else:\n",
    "                print(f\"\\nNo feature importances available for {best_model_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating feature importance: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return segment_results\n",
    "\n",
    "\n",
    "results = evaluate_segments(matrix, labels_df)\n",
    "\n",
    "print(\"\\nBest models per segment:\")\n",
    "for segment_name, result_df in results.items():\n",
    "    best_model = result_df.index[0]\n",
    "    best_f1 = result_df.iloc[0]['f1']\n",
    "    best_precision = result_df.iloc[0]['precision']\n",
    "    best_recall = result_df.iloc[0]['recall']\n",
    "    print(f\"{segment_name}:\")\n",
    "    print(f\"  Best model: {best_model}\")\n",
    "    print(f\"  F1: {best_f1:.3f}\")\n",
    "    print(f\"  Precision: {best_precision:.3f}\")\n",
    "    print(f\"  Recall: {best_recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_full_distribution(matrix: pl.DataFrame, labels_df: pl.DataFrame) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"Analyze distribution in both full and labeled datasets\"\"\"\n",
    "    print(\"=== Full Dataset Distribution ===\")\n",
    "    full_segments = segment_users_by_behavior(matrix)\n",
    "    \n",
    "    print(\"\\n=== Labeled Dataset Distribution ===\")\n",
    "    labeled_data = matrix.join(labels_df, on='fid', how='inner')\n",
    "    labeled_segments = segment_users_by_behavior(labeled_data)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    print(\"\\n=== Label Coverage by Segment ===\")\n",
    "    for segment_name in full_segments.keys():\n",
    "        full_count = len(full_segments[segment_name])\n",
    "        labeled_count = len(labeled_segments.get(segment_name, pl.DataFrame()))\n",
    "        coverage = (labeled_count / full_count * 100) if full_count > 0 else 0\n",
    "        print(f\"{segment_name}: {coverage:.1f}% labeled ({labeled_count}/{full_count})\")\n",
    "        \n",
    "    return full_segments, labeled_segments\n",
    "def get_unlabeled_samples(matrix: pl.DataFrame, \n",
    "                         labels_df: pl.DataFrame, \n",
    "                         samples_per_segment: int = 50,\n",
    "                         use_isolation_forest: bool = True) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Get stratified samples of unlabeled data, optionally using anomaly detection\"\"\"\n",
    "    \n",
    "    # Get unlabeled FIDs\n",
    "    labeled_fids = labels_df['fid'].unique()\n",
    "    unlabeled = matrix.filter(~pl.col('fid').is_in(labeled_fids))\n",
    "    \n",
    "    # Segment unlabeled data\n",
    "    segments = segment_users_by_behavior(unlabeled)\n",
    "    \n",
    "    # Features for anomaly detection\n",
    "    anomaly_features = [\n",
    "        'cast_count', 'follower_count', 'following_count',\n",
    "        'authenticity_score', 'total_reactions', 'rapid_actions',\n",
    "        'avg_hours_between_actions', 'std_hours_between_actions'\n",
    "    ]\n",
    "    \n",
    "    samples = {}\n",
    "    for name, segment in segments.items():\n",
    "        print(f\"\\nProcessing {name} segment ({len(segment)} users)\")\n",
    "        \n",
    "        if len(segment) == 0:\n",
    "            continue\n",
    "            \n",
    "        if use_isolation_forest and len(segment) > samples_per_segment:\n",
    "            # Prepare features for anomaly detection\n",
    "            valid_features = [f for f in anomaly_features if f in segment.columns]\n",
    "            if len(valid_features) > 0:\n",
    "                X = segment.select(valid_features).fill_null(0).to_numpy()\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "                \n",
    "                # Use Isolation Forest to identify anomalies\n",
    "                iso_forest = IsolationForest(\n",
    "                    n_estimators=100,\n",
    "                    contamination=0.1,  # Assume 10% anomalies\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                # Get anomaly scores\n",
    "                scores = iso_forest.fit_predict(X)\n",
    "                anomaly_indices = np.where(scores == -1)[0]\n",
    "                normal_indices = np.where(scores == 1)[0]\n",
    "                \n",
    "                # Sample both anomalies and normal cases\n",
    "                n_anomalies = min(samples_per_segment // 4, len(anomaly_indices))\n",
    "                n_normal = samples_per_segment - n_anomalies\n",
    "                \n",
    "                # Create a filter for selected indices\n",
    "                selected_indices = np.concatenate([\n",
    "                    np.random.choice(anomaly_indices, n_anomalies, replace=False),\n",
    "                    np.random.choice(normal_indices, n_normal, replace=False)\n",
    "                ])\n",
    "                \n",
    "                # Create a row number column and filter by selected indices\n",
    "                samples[name] = (segment\n",
    "                    .with_row_count(\"row_nr\")\n",
    "                    .filter(pl.col(\"row_nr\").is_in(selected_indices))\n",
    "                    .drop(\"row_nr\"))\n",
    "                \n",
    "                print(f\"Selected {n_anomalies} potential anomalies and {n_normal} normal cases\")\n",
    "            else:\n",
    "                # Fallback to random sampling if features not available\n",
    "                samples[name] = segment.sample(n=samples_per_segment, seed=42)\n",
    "        else:\n",
    "            # For small segments, take all samples\n",
    "            n_samples = min(samples_per_segment, len(segment))\n",
    "            samples[name] = segment.sample(n=n_samples, seed=42)\n",
    "        \n",
    "        print(f\"Final sample size: {len(samples[name])}\")\n",
    "        \n",
    "        # Print some statistics about the sample\n",
    "        if 'cast_count' in segment.columns:\n",
    "            stats = samples[name].select([\n",
    "                pl.col('cast_count').mean().alias('avg_casts'),\n",
    "                pl.col('follower_count').mean().alias('avg_followers'),\n",
    "                pl.col('following_count').mean().alias('avg_following')\n",
    "            ])\n",
    "            print(\"Sample statistics:\")\n",
    "            print(f\"  Avg casts: {stats['avg_casts'][0]:.1f}\")\n",
    "            print(f\"  Avg followers: {stats['avg_followers'][0]:.1f}\")\n",
    "            print(f\"  Avg following: {stats['avg_following'][0]:.1f}\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def export_samples_for_labeling(samples: Dict[str, pl.DataFrame], \n",
    "                              output_path: str = \"samples_for_labeling.csv\"):\n",
    "    \"\"\"Export samples for manual labeling\"\"\"\n",
    "    # Combine all samples\n",
    "    all_samples = pl.concat([\n",
    "        segment.with_columns(pl.lit(name).alias('segment'))\n",
    "        for name, segment in samples.items()\n",
    "    ])\n",
    "    \n",
    "    # Select relevant columns for labeling\n",
    "    export_columns = [\n",
    "        'fid', 'segment', 'fname',\n",
    "        'cast_count', 'follower_count', 'following_count',\n",
    "        'total_reactions', 'authenticity_score'\n",
    "    ]\n",
    "    \n",
    "    # Only include columns that exist\n",
    "    valid_columns = [col for col in export_columns if col in all_samples.columns]\n",
    "    \n",
    "    # Export to CSV\n",
    "    all_samples.select(valid_columns).write_csv(output_path)\n",
    "    print(f\"\\nExported {len(all_samples)} samples to {output_path}\")\n",
    "    \n",
    "def suggest_priority_accounts(matrix: pl.DataFrame, \n",
    "                              labels_df: pl.DataFrame,\n",
    "                              n_suggestions: int = 50) -> pl.DataFrame:\n",
    "    \"\"\"Suggest priority accounts for labeling based on influence and uncertainty\"\"\"\n",
    "    \n",
    "    # Get unlabeled accounts\n",
    "    labeled_fids = labels_df['fid'].unique()\n",
    "    unlabeled = matrix.filter(~pl.col('fid').is_in(labeled_fids))\n",
    "    \n",
    "    # Calculate influence score\n",
    "    influence_features = [\n",
    "        'follower_count', 'following_count', 'cast_count',\n",
    "        'total_reactions', 'authenticity_score'\n",
    "    ]\n",
    "    \n",
    "    # Only use available features\n",
    "    valid_features = [f for f in influence_features if f in unlabeled.columns]\n",
    "    \n",
    "    if len(valid_features) > 0:\n",
    "        # Normalize features\n",
    "        normalized = unlabeled.select([\n",
    "            'fid',\n",
    "            *(pl.col(f).fill_null(0) / pl.col(f).fill_null(0).max() for f in valid_features)\n",
    "        ])\n",
    "        \n",
    "        # Simple influence score - average of normalized features\n",
    "        influence_df = normalized.with_columns([\n",
    "            pl.fold(acc=0, function=lambda acc, x: acc + x, exprs=[pl.col(f) for f in valid_features])\n",
    "            .alias('influence_score')\n",
    "        ])\n",
    "        \n",
    "        # Get top influential accounts\n",
    "        suggestions = influence_df.sort('influence_score', descending=True).head(n_suggestions)\n",
    "        \n",
    "        # Select only necessary columns for the join\n",
    "        suggestions = suggestions.select(['fid', 'influence_score'])\n",
    "        \n",
    "        # Join back with original features, including 'fname'\n",
    "        result = suggestions.join(unlabeled, on='fid', how='left', suffix=\"_unlabeled\")\n",
    "        \n",
    "        # Ensure 'fname' is included in the final result\n",
    "        if 'fname' in result.columns:\n",
    "            return result.select(['fid', 'fname', 'influence_score', 'follower_count', 'cast_count', 'total_reactions'])\n",
    "        else:\n",
    "            print(\"Warning: 'fname' column not found in the dataset.\")\n",
    "            return result.select(['fid', 'influence_score', 'follower_count', 'cast_count', 'total_reactions'])\n",
    "    else:\n",
    "        print(\"No valid features found for influence calculation\")\n",
    "        return unlabeled.head(n_suggestions)\n",
    "\n",
    "\n",
    "# Analyze current distribution\n",
    "full_segments, labeled_segments = analyze_full_distribution(matrix, labels_df)\n",
    "\n",
    "# Get samples for each segment\n",
    "samples = get_unlabeled_samples(matrix, labels_df, samples_per_segment=50)\n",
    "\n",
    "# Export samples for labeling\n",
    "export_samples_for_labeling(samples)\n",
    "\n",
    "# Get priority suggestions\n",
    "priority_accounts = suggest_priority_accounts(matrix, labels_df, n_suggestions=50)\n",
    "\n",
    "print(\"\\nTop accounts to consider for labeling:\")\n",
    "print(priority_accounts.select([\n",
    "    'fid', 'fname', 'influence_score', 'follower_count', 'cast_count', 'total_reactions'\n",
    "]).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get indices where bot probability is high\n",
    "bot_mask = y_pred_proba_filtered > 0.9\n",
    "bot_fids = fids_array[bot_mask]\n",
    "\n",
    "# Ensure FIDs are the same type (Int64)\n",
    "high_conf_bots = unlabeled_data_filtered.filter(pl.col('fid').cast(pl.Int64).is_in(bot_fids))\n",
    "bot_profiles = valid_profiles.with_columns(pl.col('fid').cast(pl.Int64)).filter(pl.col('fid').is_in(bot_fids))\n",
    "\n",
    "# Sample 10 random high confidence bots\n",
    "sample_size = min(10, len(bot_profiles))\n",
    "random_indices = np.random.choice(range(len(bot_profiles)), sample_size, replace=False)\n",
    "random_bot_sample = bot_profiles.slice(random_indices[0], sample_size)\n",
    "\n",
    "print(f\"\\nSample of High Confidence Bot Predictions (from {len(bot_fids)} total high-confidence bots):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for row in random_bot_sample.iter_rows():\n",
    "    fname = row[0]  # fname is first column\n",
    "    fid = row[6]    # fid is last column\n",
    "    display_name = row[1]  # display_name is second column\n",
    "    \n",
    "    # Get features for this bot\n",
    "    bot_features = high_conf_bots.filter(pl.col('fid') == fid)\n",
    "    \n",
    "    # Find index in original arrays for probability and SHAP values\n",
    "    fid_idx = np.where(fids_array == fid)[0][0]\n",
    "    bot_prob = y_pred_proba_filtered[fid_idx]\n",
    "    \n",
    "    # Get SHAP values explaining why it's classified as a bot\n",
    "    shap_values = detector.get_feature_explanations('xgb', X_unlabeled_filtered, fid_idx)\n",
    "    \n",
    "    print(f\"\\nUsername: {fname}\")\n",
    "    print(f\"Display Name: {display_name}\")\n",
    "    print(f\"FID: {fid}\")\n",
    "    print(f\"Bot Probability: {bot_prob:.2%}\")\n",
    "    \n",
    "    print(\"\\nTop reasons for bot classification:\")\n",
    "    # Sort by absolute value but only show positive values (contributing to bot classification)\n",
    "    sorted_reasons = sorted(shap_values.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    for feature, impact in sorted_reasons:\n",
    "        if impact > 0:  # Only show features pushing towards bot classification\n",
    "            print(f\"  - {feature}: {impact:.3f}\")\n",
    "    \n",
    "    print(\"\\nKey Behavioral Metrics:\")\n",
    "    metrics_to_check = [\n",
    "        'rapid_actions', \n",
    "        'std_hours_between_actions',\n",
    "        'avg_hours_between_actions',\n",
    "        'following_count',\n",
    "        'follower_count',\n",
    "        'total_activity',\n",
    "        'hour_diversity',\n",
    "        'weekday_diversity'\n",
    "    ]\n",
    "    \n",
    "    for metric in metrics_to_check:\n",
    "        if metric in bot_features.columns:\n",
    "            val = bot_features.select(metric).item()\n",
    "            if isinstance(val, (int, float)):\n",
    "                print(f\"{metric}: {val:.2f}\")\n",
    "            else:\n",
    "                print(f\"{metric}: {val}\")\n",
    "            \n",
    "    print(f\"\\nProfile link: https://warpcast.com/{fname}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nTo verify these accounts, check for:\")\n",
    "print(\"1. Highly regular posting patterns (low std_hours_between_actions)\")\n",
    "print(\"2. Unusually high activity rates (high rapid_actions)\")\n",
    "print(\"3. Unnatural timing patterns (low hour_diversity and weekday_diversity)\")\n",
    "print(\"4. Suspicious follower/following ratios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_features(segment: str) -> List[str]:\n",
    "    base_features = ['has_ens', 'has_bio', 'has_avatar', 'verification_count']\n",
    "    \n",
    "    if segment == 'active':\n",
    "        return base_features + [\n",
    "            'cast_timing_entropy',\n",
    "            'reply_ratio',\n",
    "            'mention_patterns',\n",
    "            'engagement_rate'\n",
    "        ]\n",
    "    elif segment == 'low_activity':\n",
    "        return base_features + [\n",
    "            'network_growth_rate',\n",
    "            'initial_behavior_pattern',\n",
    "            'verification_sequence'\n",
    "        ]\n",
    "    else:  # dormant\n",
    "        return base_features + [\n",
    "            'follower_growth_velocity',\n",
    "            'network_structure',\n",
    "            'profile_completion_sequence'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def segment_users_by_behavior(matrix: pl.DataFrame) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Segment users based on behavioral patterns\"\"\"\n",
    "    segments = {\n",
    "        'power_users': matrix.filter(\n",
    "            (pl.col('cast_count') >= 20) & \n",
    "            (pl.col('reply_count') >= 5)\n",
    "        ),\n",
    "        'casual_users': matrix.filter(\n",
    "            (pl.col('cast_count') >= 5) & \n",
    "            (pl.col('cast_count') < 20)\n",
    "        ),\n",
    "        'one_time_users': matrix.filter(\n",
    "            (pl.col('cast_count') > 0) & \n",
    "            (pl.col('cast_count') < 5)\n",
    "        ),\n",
    "        'lurkers': matrix.filter(pl.col('cast_count') == 0)\n",
    "    }\n",
    "    \n",
    "    total = len(matrix)\n",
    "    print(\"\\nUser Segment Distribution:\")\n",
    "    for name, segment in segments.items():\n",
    "        size = len(segment)\n",
    "        if size > 0 and 'bot' in segment.columns:\n",
    "            bot_pct = (segment.filter(pl.col('bot') == 1).shape[0] / size) * 100\n",
    "            print(f\"{name}: {size:,} users ({size/total*100:.1f}%) - {bot_pct:.1f}% bots\")\n",
    "            \n",
    "            metrics = segment.select([\n",
    "                pl.col('cast_count').mean(),\n",
    "                pl.col('follower_count').mean(),\n",
    "                pl.col('following_count').mean()\n",
    "            ]).to_numpy()[0]\n",
    "            \n",
    "            print(f\"  Avg casts: {metrics[0]:.1f}\")\n",
    "            print(f\"  Avg followers: {metrics[1]:.1f}\")\n",
    "            print(f\"  Avg following: {metrics[2]:.1f}\")\n",
    "        else:\n",
    "            print(f\"{name}: {size:,} users ({size/total*100:.1f}%)\")\n",
    "            \n",
    "    return segments\n",
    "\n",
    "def get_segment_specific_features(segment_name: str) -> List[str]:\n",
    "    \"\"\"Get feature list specific to each behavior segment\"\"\"\n",
    "    base_features = [\n",
    "        'has_ens', 'has_bio', 'has_avatar', 'verification_count',  # verification_count here\n",
    "        'following_count', 'follower_count', 'follower_ratio',\n",
    "        'unique_follower_ratio', 'authenticity_score'\n",
    "    ]\n",
    "    \n",
    "    segment_features = {\n",
    "        'power_users': [\n",
    "            'cast_count', 'total_reactions', 'avg_cast_length',\n",
    "            'reply_count', 'mentions_count', 'engagement_score',\n",
    "            'weekday_diversity', 'hour_diversity', 'rapid_actions',\n",
    "            'avg_hours_between_actions', 'std_hours_between_actions',\n",
    "            'power_user_interaction_ratio', 'influence_score'\n",
    "        ],\n",
    "        'casual_users': [\n",
    "            'cast_count', 'total_reactions', 'engagement_score',\n",
    "            'reply_count', 'rapid_actions', 'avg_hours_between_actions',\n",
    "            'avg_cast_length', 'mentions_count'\n",
    "        ],\n",
    "        'one_time_users': [\n",
    "            'cast_count', 'total_reactions', 'profile_update_consistency',\n",
    "            'follower_growth_rate'\n",
    "        ],\n",
    "        'lurkers': [\n",
    "            'profile_update_consistency', 'network_balance',\n",
    "            'follower_growth_rate',\n",
    "            'profile_completeness'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return base_features + segment_features.get(segment_name, [])\n",
    "\n",
    "class RapidModelEvaluator:\n",
    "    def __init__(self, n_cv_splits: int = 5):\n",
    "        self.n_cv_splits = n_cv_splits\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate_model(self, name: str, model: Any, X: np.ndarray, y: np.ndarray) -> Dict:\n",
    "        \"\"\"Quickly evaluate a model with cross-validation\"\"\"\n",
    "        scoring = {\n",
    "            'precision': make_scorer(lambda y_true, y_pred: \n",
    "                precision_recall_fscore_support(y_true, y_pred)[0].mean()),\n",
    "            'recall': make_scorer(lambda y_true, y_pred: \n",
    "                precision_recall_fscore_support(y_true, y_pred)[1].mean()),\n",
    "            'f1': 'f1'\n",
    "        }\n",
    "        \n",
    "        cv_results = cross_validate(\n",
    "            model, X, y,\n",
    "            cv=self.n_cv_splits,\n",
    "            scoring=scoring,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        self.results[name] = {\n",
    "            'test_scores': {\n",
    "                'precision': cv_results['test_precision'].mean(),\n",
    "                'recall': cv_results['test_recall'].mean(),\n",
    "                'f1': cv_results['test_f1'].mean()\n",
    "            },\n",
    "            'fit_time': cv_results['fit_time'].mean()\n",
    "        }\n",
    "        \n",
    "        return self.results[name]\n",
    "    \n",
    "    def compare_models(self, segment_name: str, models: Dict[str, Any], \n",
    "                      X: np.ndarray, y: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"Compare multiple models quickly\"\"\"\n",
    "        self.results = {}  # Reset results for each segment\n",
    "        for name, model in models.items():\n",
    "            print(f\"Evaluating {name} on {segment_name} segment...\")\n",
    "            self.evaluate_model(f\"{name}\", model, X, y)\n",
    "        \n",
    "        results_df = pd.DataFrame.from_dict(\n",
    "            {k: v['test_scores'] for k, v in self.results.items()}, \n",
    "            orient='index'\n",
    "        )\n",
    "        results_df['fit_time'] = [v['fit_time'] for v in self.results.values()]\n",
    "        \n",
    "        return results_df.sort_values('f1', ascending=False)\n",
    "\n",
    "def prepare_segment_features(segment: pl.DataFrame, segment_name: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare features for a given segment\"\"\"\n",
    "    feature_cols = get_segment_specific_features(segment_name)\n",
    "    valid_features = [col for col in feature_cols if col in segment.columns]\n",
    "    print(f\"\\nUsing {len(valid_features)} features for {segment_name}:\", valid_features)\n",
    "    \n",
    "    X = segment.select(valid_features).fill_null(0).to_numpy()\n",
    "    y = segment['bot'].to_numpy() if 'bot' in segment.columns else None\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y, valid_features  # Return valid_features as well\n",
    "\n",
    "def evaluate_segments(matrix: pl.DataFrame, labels_df: pl.DataFrame) -> Dict:\n",
    "    \"\"\"Evaluate models on each behavioral segment\"\"\"\n",
    "    # Ensure consistent FID type\n",
    "    matrix = matrix.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "    labels_df = labels_df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "    \n",
    "    # Join with labels\n",
    "    data = matrix.join(labels_df, on='fid', how='inner')\n",
    "    \n",
    "    # Create segments\n",
    "    segmented_users = segment_users_by_behavior(data)\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = RapidModelEvaluator()\n",
    "    \n",
    "    # Store results\n",
    "    segment_results = {}\n",
    "    \n",
    "    # For each segment\n",
    "    for segment_name, segment_data in segmented_users.items():\n",
    "        print(f\"\\nProcessing {segment_name} segment...\")\n",
    "        \n",
    "        # Prepare features\n",
    "        X, y, valid_features = prepare_segment_features(segment_data, segment_name)\n",
    "        \n",
    "        if y is None or len(np.unique(y)) < 2:\n",
    "            print(f\"Skipping {segment_name} - insufficient labels\")\n",
    "            continue\n",
    "        \n",
    "        # Define models with balanced class weights\n",
    "        models = {\n",
    "            'xgb': xgb.XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                scale_pos_weight=sum(y == 0) / sum(y == 1),\n",
    "                random_state=42\n",
    "            ),\n",
    "            'lgbm': LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ),\n",
    "            'rf': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=4,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ),\n",
    "            'logistic': LogisticRegression(\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Evaluate models\n",
    "        results = evaluator.compare_models(segment_name, models, X, y)\n",
    "        segment_results[segment_name] = results\n",
    "        \n",
    "        print(f\"\\nResults for {segment_name}:\")\n",
    "        print(results)\n",
    "        \n",
    "        # Print feature importance for best model\n",
    "        try:\n",
    "            best_model_name = results.index[0]\n",
    "            best_model = models[best_model_name]\n",
    "            \n",
    "            if hasattr(best_model, 'feature_importances_'):\n",
    "                importances = pd.DataFrame({\n",
    "                    'feature': valid_features,\n",
    "                    'importance': best_model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(f\"\\nTop 10 features for {segment_name}:\")\n",
    "                print(importances.head(10))\n",
    "            else:\n",
    "                print(f\"\\nNo feature importances available for {best_model_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating feature importance: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return segment_results\n",
    "\n",
    "\n",
    "results = evaluate_segments(matrix, labels_df)\n",
    "\n",
    "print(\"\\nBest models per segment:\")\n",
    "for segment_name, result_df in results.items():\n",
    "    best_model = result_df.index[0]\n",
    "    best_f1 = result_df.iloc[0]['f1']\n",
    "    best_precision = result_df.iloc[0]['precision']\n",
    "    best_recall = result_df.iloc[0]['recall']\n",
    "    print(f\"{segment_name}:\")\n",
    "    print(f\"  Best model: {best_model}\")\n",
    "    print(f\"  F1: {best_f1:.3f}\")\n",
    "    print(f\"  Precision: {best_precision:.3f}\")\n",
    "    print(f\"  Recall: {best_recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_full_distribution(matrix: pl.DataFrame, labels_df: pl.DataFrame) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"Analyze distribution in both full and labeled datasets\"\"\"\n",
    "    print(\"=== Full Dataset Distribution ===\")\n",
    "    full_segments = segment_users_by_behavior(matrix)\n",
    "    \n",
    "    print(\"\\n=== Labeled Dataset Distribution ===\")\n",
    "    labeled_data = matrix.join(labels_df, on='fid', how='inner')\n",
    "    labeled_segments = segment_users_by_behavior(labeled_data)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    print(\"\\n=== Label Coverage by Segment ===\")\n",
    "    for segment_name in full_segments.keys():\n",
    "        full_count = len(full_segments[segment_name])\n",
    "        labeled_count = len(labeled_segments.get(segment_name, pl.DataFrame()))\n",
    "        coverage = (labeled_count / full_count * 100) if full_count > 0 else 0\n",
    "        print(f\"{segment_name}: {coverage:.1f}% labeled ({labeled_count}/{full_count})\")\n",
    "        \n",
    "    return full_segments, labeled_segments\n",
    "def get_unlabeled_samples(matrix: pl.DataFrame, \n",
    "                         labels_df: pl.DataFrame, \n",
    "                         samples_per_segment: int = 50,\n",
    "                         use_isolation_forest: bool = True) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Get stratified samples of unlabeled data, optionally using anomaly detection\"\"\"\n",
    "    \n",
    "    # Get unlabeled FIDs\n",
    "    labeled_fids = labels_df['fid'].unique()\n",
    "    unlabeled = matrix.filter(~pl.col('fid').is_in(labeled_fids))\n",
    "    \n",
    "    # Segment unlabeled data\n",
    "    segments = segment_users_by_behavior(unlabeled)\n",
    "    \n",
    "    # Features for anomaly detection\n",
    "    anomaly_features = [\n",
    "        'cast_count', 'follower_count', 'following_count',\n",
    "        'authenticity_score', 'total_reactions', 'rapid_actions',\n",
    "        'avg_hours_between_actions', 'std_hours_between_actions'\n",
    "    ]\n",
    "    \n",
    "    samples = {}\n",
    "    for name, segment in segments.items():\n",
    "        print(f\"\\nProcessing {name} segment ({len(segment)} users)\")\n",
    "        \n",
    "        if len(segment) == 0:\n",
    "            continue\n",
    "            \n",
    "        if use_isolation_forest and len(segment) > samples_per_segment:\n",
    "            # Prepare features for anomaly detection\n",
    "            valid_features = [f for f in anomaly_features if f in segment.columns]\n",
    "            if len(valid_features) > 0:\n",
    "                X = segment.select(valid_features).fill_null(0).to_numpy()\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "                \n",
    "                # Use Isolation Forest to identify anomalies\n",
    "                iso_forest = IsolationForest(\n",
    "                    n_estimators=100,\n",
    "                    contamination=0.1,  # Assume 10% anomalies\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                # Get anomaly scores\n",
    "                scores = iso_forest.fit_predict(X)\n",
    "                anomaly_indices = np.where(scores == -1)[0]\n",
    "                normal_indices = np.where(scores == 1)[0]\n",
    "                \n",
    "                # Sample both anomalies and normal cases\n",
    "                n_anomalies = min(samples_per_segment // 4, len(anomaly_indices))\n",
    "                n_normal = samples_per_segment - n_anomalies\n",
    "                \n",
    "                # Create a filter for selected indices\n",
    "                selected_indices = np.concatenate([\n",
    "                    np.random.choice(anomaly_indices, n_anomalies, replace=False),\n",
    "                    np.random.choice(normal_indices, n_normal, replace=False)\n",
    "                ])\n",
    "                \n",
    "                # Create a row number column and filter by selected indices\n",
    "                samples[name] = (segment\n",
    "                    .with_row_count(\"row_nr\")\n",
    "                    .filter(pl.col(\"row_nr\").is_in(selected_indices))\n",
    "                    .drop(\"row_nr\"))\n",
    "                \n",
    "                print(f\"Selected {n_anomalies} potential anomalies and {n_normal} normal cases\")\n",
    "            else:\n",
    "                # Fallback to random sampling if features not available\n",
    "                samples[name] = segment.sample(n=samples_per_segment, seed=42)\n",
    "        else:\n",
    "            # For small segments, take all samples\n",
    "            n_samples = min(samples_per_segment, len(segment))\n",
    "            samples[name] = segment.sample(n=n_samples, seed=42)\n",
    "        \n",
    "        print(f\"Final sample size: {len(samples[name])}\")\n",
    "        \n",
    "        # Print some statistics about the sample\n",
    "        if 'cast_count' in segment.columns:\n",
    "            stats = samples[name].select([\n",
    "                pl.col('cast_count').mean().alias('avg_casts'),\n",
    "                pl.col('follower_count').mean().alias('avg_followers'),\n",
    "                pl.col('following_count').mean().alias('avg_following')\n",
    "            ])\n",
    "            print(\"Sample statistics:\")\n",
    "            print(f\"  Avg casts: {stats['avg_casts'][0]:.1f}\")\n",
    "            print(f\"  Avg followers: {stats['avg_followers'][0]:.1f}\")\n",
    "            print(f\"  Avg following: {stats['avg_following'][0]:.1f}\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def export_samples_for_labeling(samples: Dict[str, pl.DataFrame], \n",
    "                              output_path: str = \"samples_for_labeling.csv\"):\n",
    "    \"\"\"Export samples for manual labeling\"\"\"\n",
    "    # Combine all samples\n",
    "    all_samples = pl.concat([\n",
    "        segment.with_columns(pl.lit(name).alias('segment'))\n",
    "        for name, segment in samples.items()\n",
    "    ])\n",
    "    \n",
    "    # Select relevant columns for labeling\n",
    "    export_columns = [\n",
    "        'fid', 'segment', 'fname',\n",
    "        'cast_count', 'follower_count', 'following_count',\n",
    "        'total_reactions', 'authenticity_score'\n",
    "    ]\n",
    "    \n",
    "    # Only include columns that exist\n",
    "    valid_columns = [col for col in export_columns if col in all_samples.columns]\n",
    "    \n",
    "    # Export to CSV\n",
    "    all_samples.select(valid_columns).write_csv(output_path)\n",
    "    print(f\"\\nExported {len(all_samples)} samples to {output_path}\")\n",
    "    \n",
    "def suggest_priority_accounts(matrix: pl.DataFrame, \n",
    "                              labels_df: pl.DataFrame,\n",
    "                              n_suggestions: int = 50) -> pl.DataFrame:\n",
    "    \"\"\"Suggest priority accounts for labeling based on influence and uncertainty\"\"\"\n",
    "    \n",
    "    # Get unlabeled accounts\n",
    "    labeled_fids = labels_df['fid'].unique()\n",
    "    unlabeled = matrix.filter(~pl.col('fid').is_in(labeled_fids))\n",
    "    \n",
    "    # Calculate influence score\n",
    "    influence_features = [\n",
    "        'follower_count', 'following_count', 'cast_count',\n",
    "        'total_reactions', 'authenticity_score'\n",
    "    ]\n",
    "    \n",
    "    # Only use available features\n",
    "    valid_features = [f for f in influence_features if f in unlabeled.columns]\n",
    "    \n",
    "    if len(valid_features) > 0:\n",
    "        # Normalize features\n",
    "        normalized = unlabeled.select([\n",
    "            'fid',\n",
    "            *(pl.col(f).fill_null(0) / pl.col(f).fill_null(0).max() for f in valid_features)\n",
    "        ])\n",
    "        \n",
    "        # Simple influence score - average of normalized features\n",
    "        influence_df = normalized.with_columns([\n",
    "            pl.fold(acc=0, function=lambda acc, x: acc + x, exprs=[pl.col(f) for f in valid_features])\n",
    "            .alias('influence_score')\n",
    "        ])\n",
    "        \n",
    "        # Get top influential accounts\n",
    "        suggestions = influence_df.sort('influence_score', descending=True).head(n_suggestions)\n",
    "        \n",
    "        # Select only necessary columns for the join\n",
    "        suggestions = suggestions.select(['fid', 'influence_score'])\n",
    "        \n",
    "        # Join back with original features, including 'fname'\n",
    "        result = suggestions.join(unlabeled, on='fid', how='left', suffix=\"_unlabeled\")\n",
    "        \n",
    "        # Ensure 'fname' is included in the final result\n",
    "        if 'fname' in result.columns:\n",
    "            return result.select(['fid', 'fname', 'influence_score', 'follower_count', 'cast_count', 'total_reactions'])\n",
    "        else:\n",
    "            print(\"Warning: 'fname' column not found in the dataset.\")\n",
    "            return result.select(['fid', 'influence_score', 'follower_count', 'cast_count', 'total_reactions'])\n",
    "    else:\n",
    "        print(\"No valid features found for influence calculation\")\n",
    "        return unlabeled.head(n_suggestions)\n",
    "\n",
    "\n",
    "# Analyze current distribution\n",
    "full_segments, labeled_segments = analyze_full_distribution(matrix, labels_df)\n",
    "\n",
    "# Get samples for each segment\n",
    "samples = get_unlabeled_samples(matrix, labels_df, samples_per_segment=50)\n",
    "\n",
    "# Export samples for labeling\n",
    "export_samples_for_labeling(samples)\n",
    "\n",
    "# Get priority suggestions\n",
    "priority_accounts = suggest_priority_accounts(matrix, labels_df, n_suggestions=50)\n",
    "\n",
    "print(\"\\nTop accounts to consider for labeling:\")\n",
    "print(priority_accounts.select([\n",
    "    'fid', 'fname', 'influence_score', 'follower_count', 'cast_count', 'total_reactions'\n",
    "]).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "farcaster-social-graph-notebooks-RMjVf8-3-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
