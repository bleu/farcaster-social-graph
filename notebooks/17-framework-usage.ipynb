{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseribeiro/Library/Caches/pypoetry/virtualenvs/farcaster-social-graph-notebooks-RMjVf8-3-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-10 14:19:14,018 - DetectorService - INFO - Loading existing model from checkpoint.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from farcaster_sybil_detection.config.defaults import Config\n",
    "from farcaster_sybil_detection.features.manager import FeatureManager\n",
    "from farcaster_sybil_detection.services.detector import DetectorService\n",
    "\n",
    "pl.Config.set_streaming_chunk_size(1_000_000)\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "\n",
    "\n",
    "# Create configuration\n",
    "config = Config(\n",
    "    data_path=Path(\"data\"),\n",
    "    checkpoint_dir=Path(\"checkpoints\"),\n",
    "    model_dir=Path(\"models\"),\n",
    "    debug_mode=True,\n",
    "    cache_enabled=True\n",
    ")\n",
    "\n",
    "# Initialize Feature Manager\n",
    "feature_manager = FeatureManager(config)\n",
    "\n",
    "# Initialize Detector Service with the Feature Manager\n",
    "detector = DetectorService(config, feature_manager)\n",
    "\n",
    "# Load Labels\n",
    "labels_df = pl.read_csv(\"data/labels.csv\")\n",
    "\n",
    "# Validate labels_df\n",
    "required_columns = {'fid', 'bot'}\n",
    "if not required_columns.issubset(labels_df.columns):\n",
    "    missing = required_columns - set(labels_df.columns)\n",
    "    raise ValueError(f\"Missing required columns in labels.csv: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = detector.trainer.train(labels_df)\n",
    "# print(\"Training Metrics:\")\n",
    "# for metric, value in metrics.items():\n",
    "#     print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 14:19:14,470 - DetectorService - INFO - Making prediction for identifier: vitalik\n",
      "2024-12-10 14:19:14,471 - Predictor - INFO - Predicting for identifier: vitalik\n",
      "2024-12-10 14:19:14,471 - Predictor - INFO - Loading ID mapping from profile data...\n",
      "2024-12-10 14:19:14,472 - DatasetLoader - INFO - Loading profile_with_addresses with columns: ['fid', 'fname']\n",
      "2024-12-10 14:19:14,483 - DatasetLoader - INFO - Filtered dataset: 894048 records, 893130 unique FIDs\n",
      "2024-12-10 14:19:14,489 - DatasetLoader - INFO - Loaded farcaster-profile_with_addresses: 894048 records\n",
      "2024-12-10 14:19:14,491 - FeatureManager - INFO - Starting feature matrix build - Memory usage: 436.38 MB\n",
      "2024-12-10 14:19:14,491 - FeatureManager - INFO - Base FIDs: 1\n",
      "/Users/joseribeiro/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/features/manager.py:299: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  f\"Feature matrix schema: {feature_matrix.schema} ({len(feature_matrix.columns)} columns)\"\n",
      "/Users/joseribeiro/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/features/manager.py:299: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  f\"Feature matrix schema: {feature_matrix.schema} ({len(feature_matrix.columns)} columns)\"\n",
      "2024-12-10 14:19:14,492 - FeatureManager - INFO - Feature matrix schema: Schema({'fid': Int64}) (1 columns)\n",
      "2024-12-10 14:19:14,492 - FeatureManager - INFO - Feature matrix size: naive plan: (run LazyFrame.explain(optimized=True) to see the optimized plan)\n",
      "\n",
      " SELECT [col(\"fid\").count()] FROM\n",
      "  DF [\"fid\"]; PROJECT */1 COLUMNS; SELECTION: None\n",
      "2024-12-10 14:19:14,493 - FeatureManager - INFO - Determining feature build order...\n",
      "2024-12-10 14:19:14,493 - FeatureManager - INFO - Build order determined successfully: ['user_identity', 'temporal_behavior', 'network_analysis', 'content_engagement']\n",
      "2024-12-10 14:19:14,493 - FeatureManager - INFO - Starting user_identity - Memory usage: 436.84 MB\n",
      "2024-12-10 14:19:14,499 - FeatureManager - INFO - Using cached features for user_identity\n",
      "/Users/joseribeiro/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/features/manager.py:114: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  for col in new_features.columns\n",
      "/Users/joseribeiro/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/features/manager.py:115: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  if col not in df.columns and col != \"fid\"\n",
      "2024-12-10 14:19:14,506 - FeatureManager - INFO - Starting temporal_behavior - Memory usage: 436.88 MB\n",
      "2024-12-10 14:19:14,506 - FeatureManager - INFO - Using cached features for temporal_behavior\n",
      "2024-12-10 14:19:14,511 - FeatureManager - INFO - Starting network_analysis - Memory usage: 436.95 MB\n",
      "2024-12-10 14:19:14,512 - FeatureManager - INFO - Using cached features for network_analysis\n",
      "2024-12-10 14:19:14,516 - FeatureManager - INFO - Starting content_engagement - Memory usage: 436.97 MB\n",
      "2024-12-10 14:19:14,518 - FeatureManager - INFO - Using cached features for content_engagement\n",
      "2024-12-10 14:19:14,523 - FeatureManager - INFO - Collecting final feature matrix\n",
      "2024-12-10 14:19:14,526 - FeatureManager - INFO - Feature matrix build completed - Memory usage: 439.28 MB\n",
      "2024-12-10 14:19:14,528 - Predictor - INFO - Generating prediction using shape: (1, 118)\n",
      "┌───────┬─────────┬─────────┬────────────┬───┬─────────────┬─────────────┬────────────┬────────────┐\n",
      "│ fid   ┆ has_ens ┆ has_bio ┆ has_avatar ┆ … ┆ airdrop_ter ┆ parent_fid_ ┆ root_threa ┆ embed_usag │\n",
      "│ ---   ┆ ---     ┆ ---     ┆ ---        ┆   ┆ m_ratio     ┆ diversity   ┆ d_particip ┆ e_ratio    │\n",
      "│ i64   ┆ i64     ┆ i64     ┆ i64        ┆   ┆ ---         ┆ ---         ┆ ation      ┆ ---        │\n",
      "│       ┆         ┆         ┆            ┆   ┆ f64         ┆ u32         ┆ ---        ┆ f64        │\n",
      "│       ┆         ┆         ┆            ┆   ┆             ┆             ┆ f64        ┆            │\n",
      "╞═══════╪═════════╪═════════╪════════════╪═══╪═════════════╪═════════════╪════════════╪════════════╡\n",
      "│ 22032 ┆ 0       ┆ 1       ┆ 1          ┆ … ┆ 0.0         ┆ 4           ┆ 1.0        ┆ 1.0        │\n",
      "└───────┴─────────┴─────────┴────────────┴───┴─────────────┴─────────────┴────────────┴────────────┘\n",
      "2024-12-10 14:19:14,528 - Predictor - INFO - Generating prediction using shape: (1, 118)\n",
      "2024-12-10 14:19:14,528 - Predictor - INFO - Model features: ['has_ens', 'has_bio', 'has_avatar', 'has_display_name', 'profile_completeness', 'display_name_length', 'bio_length', 'fname_random_numbers', 'fname_wallet_pattern', 'fname_excessive_symbols', 'fname_airdrop_terms', 'fname_has_year', 'bio_random_numbers', 'bio_wallet_pattern', 'bio_excessive_symbols', 'bio_airdrop_terms', 'bio_has_year', 'fname_entropy', 'bio_entropy', 'total_verifications', 'eth_verifications', 'verification_consistency', 'platforms_verified', 'verification_span_days', 'avg_hours_between_verifications', 'platform_diversity', 'verification_velocity', 'sequential_verifications', 'verification_gaps', 'storage_units', 'storage_utilization', 'storage_update_frequency', 'storage_growth_rate', 'storage_stability', 'storage_efficiency', 'identity_strength', 'verification_quality', 'profile_authenticity', 'resource_utilization', 'hour_diversity', 'weekday_diversity', 'inactive_periods', 'daily_active_hours', 'posting_frequency', 'response_latency', 'interaction_timing', 'engagement_windows', 'burst_frequency', 'burst_intensity', 'burst_duration', 'inter_burst_interval', 'burst_engagement_ratio', 'burst_impact', 'temporal_consistency', 'rhythm_score', 'variability_index', 'prime_time_ratio', 'off_hours_activity', 'weekend_activity_ratio', 'global_reach', 'activity_entropy', 'temporal_clustering', 'trend_stability', 'adaptation_rate', 'avg_follow_latency_seconds', 'cross_channel_activity', 'multi_channel_ratio', 'follow_ratio', 'network_growth_rate', 'follow_velocity', 'network_age_hours', 'growth_stability', 'follower_growth_rate', 'following_growth_rate', 'follow_reciprocity', 'network_density', 'network_reach', 'network_churn_rate', 'relationship_longevity', 'network_volatility', 'stable_connections', 'network_centrality', 'bridge_score', 'community_embedding', 'network_diversity', 'cast_count', 'reply_count', 'mention_count', 'avg_cast_length', 'link_usage_rate', 'content_type_diversity', 'content_complexity', 'total_reactions', 'like_count', 'recast_count', 'engagement_rate', 'viral_coefficient', 'audience_reach', 'engagement_consistency', 'conversation_depth', 'conversation_initiation_rate', 'thread_participation', 'content_originality', 'vocabulary_richness', 'hashtag_usage', 'casts_with_mentions', 'avg_mentions_per_cast', 'mention_frequency', 'template_usage_ratio', 'urgency_ratio', 'emoji_spam_ratio', 'price_mention_ratio', 'symbol_spam_ratio', 'airdrop_term_ratio', 'parent_fid_diversity', 'root_thread_participation', 'embed_usage_ratio']\n",
      "2024-12-10 14:19:14,529 - Predictor - INFO - Predicting for features: ['has_ens', 'has_bio', 'has_avatar', 'has_display_name', 'profile_completeness', 'display_name_length', 'bio_length', 'fname_random_numbers', 'fname_wallet_pattern', 'fname_excessive_symbols', 'fname_airdrop_terms', 'fname_has_year', 'bio_random_numbers', 'bio_wallet_pattern', 'bio_excessive_symbols', 'bio_airdrop_terms', 'bio_has_year', 'fname_entropy', 'bio_entropy', 'total_verifications', 'eth_verifications', 'verification_consistency', 'platforms_verified', 'verification_span_days', 'avg_hours_between_verifications', 'platform_diversity', 'verification_velocity', 'sequential_verifications', 'verification_gaps', 'storage_units', 'storage_utilization', 'storage_update_frequency', 'storage_growth_rate', 'storage_stability', 'storage_efficiency', 'identity_strength', 'verification_quality', 'profile_authenticity', 'resource_utilization', 'hour_diversity', 'weekday_diversity', 'inactive_periods', 'daily_active_hours', 'posting_frequency', 'response_latency', 'interaction_timing', 'engagement_windows', 'burst_frequency', 'burst_intensity', 'burst_duration', 'inter_burst_interval', 'burst_engagement_ratio', 'burst_impact', 'temporal_consistency', 'rhythm_score', 'variability_index', 'prime_time_ratio', 'off_hours_activity', 'weekend_activity_ratio', 'global_reach', 'activity_entropy', 'temporal_clustering', 'trend_stability', 'adaptation_rate', 'avg_follow_latency_seconds', 'cross_channel_activity', 'multi_channel_ratio', 'follow_ratio', 'network_growth_rate', 'follow_velocity', 'network_age_hours', 'growth_stability', 'follower_growth_rate', 'following_growth_rate', 'follow_reciprocity', 'network_density', 'network_reach', 'network_churn_rate', 'relationship_longevity', 'network_volatility', 'stable_connections', 'network_centrality', 'bridge_score', 'community_embedding', 'network_diversity', 'cast_count', 'reply_count', 'mention_count', 'avg_cast_length', 'link_usage_rate', 'content_type_diversity', 'content_complexity', 'total_reactions', 'like_count', 'recast_count', 'engagement_rate', 'viral_coefficient', 'audience_reach', 'engagement_consistency', 'conversation_depth', 'conversation_initiation_rate', 'thread_participation', 'content_originality', 'vocabulary_richness', 'hashtag_usage', 'casts_with_mentions', 'avg_mentions_per_cast', 'mention_frequency', 'template_usage_ratio', 'urgency_ratio', 'emoji_spam_ratio', 'price_mention_ratio', 'symbol_spam_ratio', 'airdrop_term_ratio', 'parent_fid_diversity', 'root_thread_participation', 'embed_usage_ratio']\n",
      "/Users/joseribeiro/Library/Caches/pypoetry/virtualenvs/farcaster-social-graph-notebooks-RMjVf8-3-py3.12/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/joseribeiro/Library/Caches/pypoetry/virtualenvs/farcaster-social-graph-notebooks-RMjVf8-3-py3.12/lib/python3.12/site-packages/numpy/_core/_methods.py:130: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/Users/joseribeiro/Library/Caches/pypoetry/virtualenvs/farcaster-social-graph-notebooks-RMjVf8-3-py3.12/lib/python3.12/site-packages/numpy/_core/_methods.py:218: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/joseribeiro/Library/Caches/pypoetry/virtualenvs/farcaster-social-graph-notebooks-RMjVf8-3-py3.12/lib/python3.12/site-packages/numpy/_core/_methods.py:175: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/joseribeiro/Library/Caches/pypoetry/virtualenvs/farcaster-social-graph-notebooks-RMjVf8-3-py3.12/lib/python3.12/site-packages/numpy/_core/_methods.py:207: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/Users/joseribeiro/Library/Caches/pypoetry/virtualenvs/farcaster-social-graph-notebooks-RMjVf8-3-py3.12/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/joseribeiro/Library/Caches/pypoetry/virtualenvs/farcaster-social-graph-notebooks-RMjVf8-3-py3.12/lib/python3.12/site-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/joseribeiro/Library/Caches/pypoetry/virtualenvs/farcaster-social-graph-notebooks-RMjVf8-3-py3.12/lib/python3.12/site-packages/numpy/_core/_methods.py:218: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/joseribeiro/Library/Caches/pypoetry/virtualenvs/farcaster-social-graph-notebooks-RMjVf8-3-py3.12/lib/python3.12/site-packages/numpy/_core/_methods.py:175: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/joseribeiro/Library/Caches/pypoetry/virtualenvs/farcaster-social-graph-notebooks-RMjVf8-3-py3.12/lib/python3.12/site-packages/numpy/_core/_methods.py:210: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "2024-12-10 14:19:14,581 - DetectorService - INFO - Prediction completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 118)\n",
      "┌───────┬─────────┬─────────┬────────────┬───┬─────────────┬─────────────┬────────────┬────────────┐\n",
      "│ fid   ┆ has_ens ┆ has_bio ┆ has_avatar ┆ … ┆ airdrop_ter ┆ parent_fid_ ┆ root_threa ┆ embed_usag │\n",
      "│ ---   ┆ ---     ┆ ---     ┆ ---        ┆   ┆ m_ratio     ┆ diversity   ┆ d_particip ┆ e_ratio    │\n",
      "│ i64   ┆ i64     ┆ i64     ┆ i64        ┆   ┆ ---         ┆ ---         ┆ ation      ┆ ---        │\n",
      "│       ┆         ┆         ┆            ┆   ┆ f64         ┆ u32         ┆ ---        ┆ f64        │\n",
      "│       ┆         ┆         ┆            ┆   ┆             ┆             ┆ f64        ┆            │\n",
      "╞═══════╪═════════╪═════════╪════════════╪═══╪═════════════╪═════════════╪════════════╪════════════╡\n",
      "│ 22032 ┆ 0       ┆ 1       ┆ 1          ┆ … ┆ 0.0         ┆ 4           ┆ 1.0        ┆ 1.0        │\n",
      "└───────┴─────────┴─────────┴────────────┴───┴─────────────┴─────────────┴────────────┴────────────┘\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6669671742026618, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6669671742026618\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8834622615880475, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8834622615880475\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6669671742026618, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6669671742026618\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8834622615880475, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8834622615880475\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6669671742026618, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6669671742026618\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8834622615880475, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8834622615880475\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6669671742026618, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6669671742026618\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8834622615880475, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8834622615880475\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6669671742026618, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6669671742026618\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8834622615880475, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8834622615880475\n",
      "\n",
      "Prediction Result:\n",
      "{'prediction': 0, 'probability': nan, 'confidence': 0.5, 'prediction_label': 'human', 'features_used': ['has_ens', 'has_bio', 'has_avatar', 'has_display_name', 'profile_completeness', 'display_name_length', 'bio_length', 'fname_random_numbers', 'fname_wallet_pattern', 'fname_excessive_symbols', 'fname_airdrop_terms', 'fname_has_year', 'bio_random_numbers', 'bio_wallet_pattern', 'bio_excessive_symbols', 'bio_airdrop_terms', 'bio_has_year', 'fname_entropy', 'bio_entropy', 'total_verifications', 'eth_verifications', 'verification_consistency', 'platforms_verified', 'verification_span_days', 'avg_hours_between_verifications', 'platform_diversity', 'verification_velocity', 'sequential_verifications', 'verification_gaps', 'storage_units', 'storage_utilization', 'storage_update_frequency', 'storage_growth_rate', 'storage_stability', 'storage_efficiency', 'identity_strength', 'verification_quality', 'profile_authenticity', 'resource_utilization', 'hour_diversity', 'weekday_diversity', 'inactive_periods', 'daily_active_hours', 'posting_frequency', 'response_latency', 'interaction_timing', 'engagement_windows', 'burst_frequency', 'burst_intensity', 'burst_duration', 'inter_burst_interval', 'burst_engagement_ratio', 'burst_impact', 'temporal_consistency', 'rhythm_score', 'variability_index', 'prime_time_ratio', 'off_hours_activity', 'weekend_activity_ratio', 'global_reach', 'activity_entropy', 'temporal_clustering', 'trend_stability', 'adaptation_rate', 'avg_follow_latency_seconds', 'cross_channel_activity', 'multi_channel_ratio', 'follow_ratio', 'network_growth_rate', 'follow_velocity', 'network_age_hours', 'growth_stability', 'follower_growth_rate', 'following_growth_rate', 'follow_reciprocity', 'network_density', 'network_reach', 'network_churn_rate', 'relationship_longevity', 'network_volatility', 'stable_connections', 'network_centrality', 'bridge_score', 'community_embedding', 'network_diversity', 'cast_count', 'reply_count', 'mention_count', 'avg_cast_length', 'link_usage_rate', 'content_type_diversity', 'content_complexity', 'total_reactions', 'like_count', 'recast_count', 'engagement_rate', 'viral_coefficient', 'audience_reach', 'engagement_consistency', 'conversation_depth', 'conversation_initiation_rate', 'thread_participation', 'content_originality', 'vocabulary_richness', 'hashtag_usage', 'casts_with_mentions', 'avg_mentions_per_cast', 'mention_frequency', 'template_usage_ratio', 'urgency_ratio', 'emoji_spam_ratio', 'price_mention_ratio', 'symbol_spam_ratio', 'airdrop_term_ratio', 'parent_fid_diversity', 'root_thread_participation', 'embed_usage_ratio'], 'fid': 22032, 'fname': 'vitalik', 'status': 'success'}\n"
     ]
    }
   ],
   "source": [
    "# result = detector.predict(identifier='rpunkt')\n",
    "# result = detector.predict(identifier='vitalik')\n",
    "result = detector.predict(identifier='ipungkribo')\n",
    "print(\"\\nPrediction Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def analyze_population_distribution(detector: DetectorService, feature_matrix: pl.DataFrame):\n",
    "    \"\"\"Analyze bot probability distribution across the entire population\"\"\"\n",
    "    print(\"\\nAnalyzing full population distribution...\")\n",
    "    \n",
    "    try:\n",
    "        # Get all features (excluding fid)\n",
    "        feature_cols = [col for col in detector.model.feature_names if col != 'fid']\n",
    "        X = feature_matrix.select(feature_cols).to_numpy()\n",
    "\n",
    "        # Replace inf values with nan\n",
    "        X = np.where(np.isinf(X), np.nan, X)\n",
    "        \n",
    "        # Replace nan with 0 and clip extreme values\n",
    "        X = np.nan_to_num(X, nan=0.0)\n",
    "        X = np.clip(X, -1e9, 1e9)\n",
    "        \n",
    "        # Convert to float32 safely\n",
    "        X = X.astype(np.float32)\n",
    "        \n",
    "        # Get predictions and probabilities\n",
    "        y_prob = detector.model.predict_proba(X)\n",
    "        # Take only the probability for class 1 (bot)\n",
    "        bot_probabilities = y_prob[:, 1]\n",
    "        y_pred = (bot_probabilities >= 0.5).astype(int)\n",
    "\n",
    "        fids = feature_matrix['fid'].to_list()\n",
    "        \n",
    "        # Create results DataFrame - now using only bot probabilities\n",
    "        results_df = pl.DataFrame({\n",
    "            'fid': fids,\n",
    "            'bot_probability': bot_probabilities,\n",
    "            'prediction': y_pred\n",
    "        })\n",
    "        \n",
    "        # Calculate distribution statistics\n",
    "        stats = {\n",
    "            'total_users': len(results_df),\n",
    "            'predicted_bots': (y_pred == 1).sum(),\n",
    "            'predicted_humans': (y_pred == 0).sum(),\n",
    "            'bot_ratio': (y_pred == 1).mean(),\n",
    "            'avg_probability': bot_probabilities.mean(),\n",
    "            'median_probability': np.median(bot_probabilities),\n",
    "            'std_probability': np.std(bot_probabilities)\n",
    "        }\n",
    "        \n",
    "        # Calculate probability buckets\n",
    "        bucket_edges = np.arange(0, 1.1, 0.1)\n",
    "        hist, _ = np.histogram(bot_probabilities, bins=bucket_edges)\n",
    "        bucket_stats = {f\"{bucket_edges[i]:.1f}-{bucket_edges[i+1]:.1f}\": count \n",
    "                       for i, count in enumerate(hist)}\n",
    "        \n",
    "        # Print distribution analysis\n",
    "        print(\"\\nPopulation Distribution Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total Users: {stats['total_users']:,}\")\n",
    "        print(f\"Predicted Bots: {stats['predicted_bots']:,} ({stats['bot_ratio']:.1%})\")\n",
    "        print(f\"Predicted Humans: {stats['predicted_humans']:,} ({1-stats['bot_ratio']:.1%})\")\n",
    "        print(\"\\nProbability Statistics:\")\n",
    "        print(f\"Mean Bot Probability: {stats['avg_probability']:.3f}\")\n",
    "        print(f\"Median Bot Probability: {stats['median_probability']:.3f}\")\n",
    "        print(f\"Std Dev: {stats['std_probability']:.3f}\")\n",
    "        \n",
    "        print(\"\\nProbability Distribution:\")\n",
    "        for bucket, count in bucket_stats.items():\n",
    "            print(f\"{bucket}: {count:,} users ({count/stats['total_users']:.1%})\")\n",
    "        \n",
    "        # Plot distribution\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(bot_probabilities, bins=50, density=True, alpha=0.7)\n",
    "        plt.axvline(x=0.5, color='r', linestyle='--', label='Decision Boundary')\n",
    "        plt.xlabel('Bot Probability')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Distribution of Bot Probabilities')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Save results\n",
    "        results_df = results_df.sort('bot_probability', descending=True)\n",
    "        results_df.write_csv(\"population_analysis.csv\")\n",
    "        print(\"\\nResults saved to population_analysis.csv\")\n",
    "        \n",
    "        return {\n",
    "            'results_df': results_df,\n",
    "            'stats': stats,\n",
    "            'bucket_stats': bucket_stats\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in population analysis: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 14:19:14,594 - DatasetLoader - INFO - Loading profile_with_addresses with columns: ['fid', 'fname']\n",
      "2024-12-10 14:19:14,605 - DatasetLoader - INFO - Filtered dataset: 894048 records, 893130 unique FIDs\n",
      "2024-12-10 14:19:14,611 - DatasetLoader - INFO - Loaded farcaster-profile_with_addresses: 894048 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total population size: 893130 FIDs\n"
     ]
    }
   ],
   "source": [
    "# First, get all available FIDs from the profile dataset\n",
    "profiles_df = feature_manager.data_loader.load_dataset(\n",
    "    'profile_with_addresses', \n",
    "    columns=['fid', 'fname']\n",
    ")\n",
    "all_fids = profiles_df['fid'].unique().sort()\n",
    "print(f\"Total population size: {len(all_fids)} FIDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 14:19:14,622 - FeatureManager - INFO - Starting feature matrix build - Memory usage: 490.78 MB\n",
      "2024-12-10 14:19:14,627 - DatasetLoader - INFO - Loading profile_with_addresses with columns: ['fid']\n",
      "2024-12-10 14:19:14,661 - DatasetLoader - INFO - Filtered dataset: 894048 records, 893130 unique FIDs\n",
      "2024-12-10 14:19:14,672 - FeatureManager - INFO - Base FIDs: 199963\n",
      "/Users/joseribeiro/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/features/manager.py:299: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  f\"Feature matrix schema: {feature_matrix.schema} ({len(feature_matrix.columns)} columns)\"\n",
      "/Users/joseribeiro/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/features/manager.py:299: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  f\"Feature matrix schema: {feature_matrix.schema} ({len(feature_matrix.columns)} columns)\"\n",
      "2024-12-10 14:19:14,673 - FeatureManager - INFO - Feature matrix schema: Schema({'fid': Int64}) (1 columns)\n",
      "2024-12-10 14:19:14,673 - FeatureManager - INFO - Feature matrix size: naive plan: (run LazyFrame.explain(optimized=True) to see the optimized plan)\n",
      "\n",
      " SELECT [col(\"fid\").count()] FROM\n",
      "  UNIQUE[maintain_order: false, keep_strategy: Any] BY None\n",
      "    SLICE[offset: 0, len: 200000]\n",
      "       WITH_COLUMNS:\n",
      "       [col(\"fid\")] \n",
      "         SELECT [col(\"fid\")] FROM\n",
      "           SELECT [col(\"fid\")] FROM\n",
      "            Parquet SCAN [data/farcaster-profile_with_addresses-0-1733162400.parquet]\n",
      "            PROJECT */7 COLUMNS\n",
      "2024-12-10 14:19:14,673 - FeatureManager - INFO - Determining feature build order...\n",
      "2024-12-10 14:19:14,673 - FeatureManager - INFO - Build order determined successfully: ['user_identity', 'temporal_behavior', 'network_analysis', 'content_engagement']\n",
      "2024-12-10 14:19:14,674 - FeatureManager - INFO - Starting user_identity - Memory usage: 527.27 MB\n",
      "2024-12-10 14:19:14,695 - UserIdentityExtractor - INFO - Starting feature extraction: UserIdentityExtractor\n",
      "2024-12-10 14:19:14,696 - UserIdentityExtractor - INFO - Loading dataset 'profile_with_addresses' from source 'farcaster'\n",
      "2024-12-10 14:19:14,696 - UserIdentityExtractor - INFO - Required columns: ['fid', 'fname', 'bio', 'avatar_url', 'verified_addresses', 'display_name']\n",
      "2024-12-10 14:19:14,696 - UserIdentityExtractor - INFO - Filtering for 199963 FIDs\n",
      "2024-12-10 14:19:14,698 - DatasetLoader - INFO - Loading profile_with_addresses with columns: ['fid', 'fname', 'bio', 'avatar_url', 'verified_addresses', 'display_name']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building feature matrix for full population...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 14:19:14,775 - DatasetLoader - INFO - Filtered dataset: 200037 records, 199963 unique FIDs\n",
      "2024-12-10 14:19:14,776 - UserIdentityExtractor - INFO - Loading dataset 'verifications' from source 'farcaster'\n",
      "2024-12-10 14:19:14,776 - UserIdentityExtractor - INFO - Required columns: ['fid', 'claim', 'timestamp', 'deleted_at']\n",
      "2024-12-10 14:19:14,776 - UserIdentityExtractor - INFO - Filtering for 199963 FIDs\n",
      "2024-12-10 14:19:14,778 - DatasetLoader - INFO - Loading verifications with columns: ['fid', 'claim', 'timestamp', 'deleted_at']\n",
      "2024-12-10 14:19:14,851 - DatasetLoader - INFO - Filtered dataset: 13022 records, 10639 unique FIDs\n",
      "2024-12-10 14:19:14,851 - UserIdentityExtractor - INFO - Loading dataset 'account_verifications' from source 'farcaster'\n",
      "2024-12-10 14:19:14,851 - UserIdentityExtractor - INFO - Required columns: ['fid', 'platform', 'verified_at']\n",
      "2024-12-10 14:19:14,851 - UserIdentityExtractor - INFO - Filtering for 199963 FIDs\n",
      "2024-12-10 14:19:14,853 - DatasetLoader - INFO - Loading account_verifications with columns: ['fid', 'platform', 'verified_at']\n",
      "2024-12-10 14:19:14,856 - DatasetLoader - INFO - Filtered dataset: 187 records, 187 unique FIDs\n",
      "2024-12-10 14:19:14,856 - UserIdentityExtractor - INFO - Loading dataset 'storage' from source 'farcaster'\n",
      "2024-12-10 14:19:14,856 - UserIdentityExtractor - INFO - Required columns: ['fid', 'units', 'timestamp', 'deleted_at']\n",
      "2024-12-10 14:19:14,857 - UserIdentityExtractor - INFO - Filtering for 199963 FIDs\n",
      "2024-12-10 14:19:14,859 - DatasetLoader - INFO - Loading storage with columns: ['fid', 'units', 'timestamp', 'deleted_at']\n",
      "2024-12-10 14:19:14,944 - DatasetLoader - INFO - Filtered dataset: 35569 records, 35018 unique FIDs\n",
      "2024-12-10 14:19:15,141 - UserIdentityExtractor - INFO - Extracting user identity features...\n",
      "2024-12-10 14:19:15,142 - UserIdentityExtractor - INFO - Joining features\n",
      "2024-12-10 14:19:15,142 - UserIdentityExtractor - INFO - Joined features\n",
      "2024-12-10 14:19:15,143 - UserIdentityExtractor - INFO - Joining features\n",
      "2024-12-10 14:19:15,143 - UserIdentityExtractor - INFO - Joined features\n",
      "2024-12-10 14:19:15,143 - UserIdentityExtractor - INFO - Joining features\n",
      "2024-12-10 14:19:15,143 - UserIdentityExtractor - INFO - Joined features\n",
      "2024-12-10 14:19:15,143 - UserIdentityExtractor - INFO - Starting post-processing...\n",
      "2024-12-10 14:19:15,144 - UserIdentityExtractor - INFO - Post-processing completed: cast 'fid', ensure uniqueness, fill nulls, and convert to numeric.\n",
      "2024-12-10 14:19:15,144 - UserIdentityExtractor - INFO - Completed feature extraction: UserIdentityExtractor\n",
      "/Users/joseribeiro/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/features/manager.py:114: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  for col in new_features.columns\n",
      "/Users/joseribeiro/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/features/manager.py:115: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  if col not in df.columns and col != \"fid\"\n",
      "2024-12-10 14:19:15,454 - FeatureManager - INFO - Completed user_identity - Memory usage: 927.89 MB\n",
      "2024-12-10 14:19:15,455 - FeatureManager - INFO - Starting temporal_behavior - Memory usage: 927.89 MB\n",
      "2024-12-10 14:19:15,476 - TemporalBehaviorExtractor - INFO - Starting feature extraction: TemporalBehaviorExtractor\n",
      "2024-12-10 14:19:15,476 - TemporalBehaviorExtractor - INFO - Loading dataset 'casts' from source 'farcaster'\n",
      "2024-12-10 14:19:15,477 - TemporalBehaviorExtractor - INFO - Required columns: ['fid', 'timestamp', 'deleted_at', 'text', 'parent_url', 'parent_hash', 'parent_fid', 'embeds', 'mentions', 'root_parent_hash']\n",
      "2024-12-10 14:19:15,477 - TemporalBehaviorExtractor - INFO - Filtering for 199963 FIDs\n",
      "2024-12-10 14:19:15,479 - DatasetLoader - INFO - Loading casts with columns: ['fid', 'timestamp', 'deleted_at', 'text', 'parent_url', 'parent_hash', 'parent_fid', 'embeds', 'mentions', 'root_parent_hash']\n"
     ]
    }
   ],
   "source": [
    "# Build feature matrix for all FIDs\n",
    "print(\"Building feature matrix for full population...\")\n",
    "full_matrix = feature_manager.build_feature_matrix()\n",
    "print(f\"Feature matrix shape: {full_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_matrix.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "population_analysis = analyze_population_distribution(detector, full_matrix)\n",
    "\n",
    "# If you want to examine specific probability ranges:\n",
    "results_df = population_analysis['results_df']\n",
    "\n",
    "# High confidence bots (e.g., >90% probability)\n",
    "high_conf_bots = results_df.filter(pl.col('bot_probability') > 0.8)\n",
    "print(f\"\\nHigh Confidence Bots (>80%): {len(high_conf_bots)}\")\n",
    "\n",
    "# High confidence humans (e.g., <10% probability)\n",
    "high_conf_humans = results_df.filter(pl.col('bot_probability') < 0.2)\n",
    "print(f\"High Confidence Humans (<20%): {len(high_conf_humans)}\")\n",
    "\n",
    "# Uncertain predictions (e.g., 40-60% probability)\n",
    "uncertain = results_df.filter(\n",
    "    (pl.col('bot_probability') >= 0.4) & \n",
    "    (pl.col('bot_probability') <= 0.6)\n",
    ")\n",
    "print(f\"Uncertain Predictions (40-60%): {len(uncertain)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results = results_df.join(profiles_df, on='fid').join(full_matrix, on='fid')\n",
    "full_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get examples of high confidence bots\n",
    "high_conf_bots_examples = profiles_df.filter(pl.col('fid').is_in(high_conf_bots['fid'])).sort('fid')\n",
    "high_conf_bots_examples.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get examples of high confidence humans\n",
    "high_conf_humans_examples = profiles_df.filter(pl.col('fid').is_in(high_conf_humans['fid'])).sort('fid')\n",
    "high_conf_humans_examples.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze a prediction in detail\n",
    "def analyze_prediction(detector, identifier):\n",
    "    result = detector.predict(identifier)\n",
    "    \n",
    "    if result['status'] != 'success':\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nAnalysis for {result['fname']} (FID: {result['fid']})\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Prediction: {result['prediction_label']}\")\n",
    "    print(f\"Probability of being a bot: {result['probability']:.3f}\")\n",
    "    print(f\"Model confidence: {result['confidence']:.3f}\")\n",
    "    \n",
    "    # Get feature importance\n",
    "    features = detector.feature_manager.get_features_for_fid(result['fid'])\n",
    "    feature_importance = detector.model.get_feature_importance()\n",
    "    \n",
    "    print(\"\\nTop contributing features:\")\n",
    "    for feature, importance in sorted(feature_importance.items(), \n",
    "                                    key=lambda x: abs(x[1]), \n",
    "                                    reverse=True)[:10]:\n",
    "        print(f\"{feature}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_model_problems(detector, identifier):\n",
    "    result = detector.predict(identifier)\n",
    "    \n",
    "    problems = []\n",
    "    \n",
    "    # Check probability threshold\n",
    "    if 0.4 <= result['probability'] <= 0.6:\n",
    "        problems.append(\"Uncertain prediction (probability near decision boundary)\")\n",
    "    \n",
    "    # Check confidence\n",
    "    if result['confidence'] < 0.7:\n",
    "        problems.append(\"Low confidence prediction\")\n",
    "    \n",
    "    # Check feature completeness\n",
    "    missing_features = set(detector.model.feature_names) - set(result['features_used'])\n",
    "    if missing_features:\n",
    "        problems.append(f\"Missing features: {missing_features}\")\n",
    "    \n",
    "    # Check for extreme feature values\n",
    "    features = detector.feature_manager.get_features_for_fid(result['fid'])\n",
    "    for col in features.columns:\n",
    "        if col != 'fid':\n",
    "            value = features[col][0]\n",
    "            if value and abs(value) > 1e6:\n",
    "                problems.append(f\"Extreme value in feature {col}: {value}\")\n",
    "    \n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_robustness(detector, test_cases):\n",
    "    results = []\n",
    "    for case in test_cases:\n",
    "        pred = detector.predict(case)\n",
    "        results.append({\n",
    "            'identifier': case,\n",
    "            'prediction': pred['prediction_label'],\n",
    "            'probability': pred['probability'],\n",
    "            'confidence': pred['confidence'],\n",
    "            'problems': identify_model_problems(detector, case)\n",
    "        })\n",
    "    \n",
    "    return pl.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a single prediction\n",
    "analyze_prediction(detector, 'vitalik')\n",
    "\n",
    "# Check for problems\n",
    "problems = identify_model_problems(detector, 'vitalik')\n",
    "if problems:\n",
    "    print(\"\\nPotential problems identified:\")\n",
    "    for problem in problems:\n",
    "        print(f\"- {problem}\")\n",
    "\n",
    "# Evaluate multiple cases\n",
    "test_cases = ['vitalik', 'rpunkt', 'ipungkribo']\n",
    "evaluation = evaluate_model_robustness(detector, test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_high_confidence_predictions(detector, min_confidence=0.9, limit=20):\n",
    "    \"\"\"\n",
    "    Find and analyze predictions with highest confidence scores.\n",
    "    \n",
    "    Args:\n",
    "        detector: DetectorService instance\n",
    "        min_confidence: Minimum confidence threshold (default 0.9)\n",
    "        limit: Number of results to return (default 20)\n",
    "    \"\"\"\n",
    "    # Get all profiles\n",
    "    profiles_df = detector.feature_manager.data_loader.load_dataset(\n",
    "        'profile_with_addresses', \n",
    "        columns=['fid', 'fname']\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    print(f\"Analyzing predictions for {len(profiles_df)} profiles...\")\n",
    "    \n",
    "    # Build feature matrix for all FIDs\n",
    "    feature_matrix = detector.feature_manager.build_feature_matrix()\n",
    "    \n",
    "    # Get predictions for all profiles\n",
    "    feature_cols = [col for col in detector.model.feature_names if col != 'fid']\n",
    "    X = feature_matrix.select(feature_cols).to_numpy()\n",
    "    \n",
    "    # Replace inf/nan values\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    X = np.clip(X, -1e9, 1e9)\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    y_prob = detector.model.predict_proba(X)\n",
    "    y_pred = (y_prob[:, 1] >= 0.5).astype(int)\n",
    "    confidences = detector.model.get_prediction_confidence(X)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pl.DataFrame({\n",
    "        'fid': feature_matrix['fid'],\n",
    "        'probability': y_prob[:, 1],\n",
    "        'prediction': y_pred,\n",
    "        'confidence': confidences\n",
    "    })\n",
    "    \n",
    "    # Join with profiles to get fnames\n",
    "    results_df = results_df.join(profiles_df, on='fid')\n",
    "    \n",
    "    # Join with feature matrix to include all features\n",
    "    results_df = results_df.join(feature_matrix, on='fid')\n",
    "    \n",
    "    # Filter for high confidence predictions\n",
    "    high_conf_df = results_df.filter(pl.col('confidence') >= min_confidence)\n",
    "    \n",
    "    # Sort by confidence\n",
    "    high_conf_df = high_conf_df.sort('confidence', descending=True)\n",
    "    \n",
    "    print(\"\\nHigh Confidence Predictions:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Found {len(high_conf_df)} predictions with confidence >= {min_confidence}\")\n",
    "    \n",
    "    # Analyze top results\n",
    "    print(\"\\nTop High-Confidence Predictions:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'FID':<10} {'Username':<20} {'Prediction':<12} {'Probability':<12} {'Confidence':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for row in high_conf_df.head(limit).iter_rows(named=True):\n",
    "        pred_label = \"Bot\" if row['prediction'] == 1 else \"Human\"\n",
    "        print(f\"{row['fid']:<10} {row['fname']:<20} {pred_label:<12} {row['probability']:.3f}{'':>4} {row['confidence']:.3f}{'':>4}\")\n",
    "    \n",
    "    return high_conf_df\n",
    "\n",
    "def analyze_specific_prediction(detector, fid_or_fname, feature_matrix=None):\n",
    "    \"\"\"\n",
    "    Detailed analysis of a specific prediction with feature importance\n",
    "    \n",
    "    Args:\n",
    "        detector: DetectorService instance\n",
    "        fid_or_fname: FID (int) or fname (str) to analyze\n",
    "        feature_matrix: Optional pre-computed feature matrix\n",
    "    \"\"\"\n",
    "    # Get prediction\n",
    "    result = detector.predict(fid_or_fname)\n",
    "    \n",
    "    if result['status'] != 'success':\n",
    "        print(f\"Error: {result.get('error', 'Unknown error')}\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nDetailed Prediction Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"User: {result['fname']} (FID: {result['fid']})\")\n",
    "    print(f\"Prediction: {result['prediction_label']}\")\n",
    "    print(f\"Probability: {result['probability']:.3f}\")\n",
    "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "    \n",
    "    # Get feature values\n",
    "    if feature_matrix is None:\n",
    "        features = detector.feature_manager.get_features_for_fid(result['fid'])\n",
    "    else:\n",
    "        features = feature_matrix.filter(pl.col('fid') == result['fid'])\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = detector.model.get_feature_importance()\n",
    "    \n",
    "    print(\"\\nTop Contributing Features:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Feature':<30} {'Importance':<15} {'Value':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Sort features by absolute importance\n",
    "    sorted_features = sorted(feature_importance.items(), \n",
    "                           key=lambda x: abs(x[1]), \n",
    "                           reverse=True)\n",
    "    \n",
    "    for feature, importance in sorted_features[:15]:  # Show top 15 features\n",
    "        value = features[feature][0] if feature in features.columns else 'N/A'\n",
    "        print(f\"{feature:<30} {importance:>15.3f} {value:>15.3f}\")\n",
    "    \n",
    "    return result, feature_importance\n",
    "\n",
    "def analyze_confidence_clusters(high_conf_df, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Analyze patterns in high confidence predictions using clustering\n",
    "    \n",
    "    Args:\n",
    "        high_conf_df: DataFrame with high confidence predictions\n",
    "        n_clusters: Number of clusters to analyze\n",
    "    \"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    # Select numeric features for clustering\n",
    "    feature_cols = [col for col in high_conf_df.columns \n",
    "                   if col not in ['fid', 'fname', 'prediction', 'probability', 'confidence']]\n",
    "    \n",
    "    # Prepare data for clustering\n",
    "    # Replace inf values with nan\n",
    "\n",
    "    X = high_conf_df.select(feature_cols).to_numpy()\n",
    "    X = np.where(np.isinf(X), np.nan, X)\n",
    "\n",
    "    # Replace nan with 0 and clip extreme values\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    X = np.clip(X, -1e9, 1e9)\n",
    "\n",
    "    # Convert to float32 safely\n",
    "    X = X.astype(np.float32)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Add cluster assignments to DataFrame\n",
    "    high_conf_df = high_conf_df.with_columns([\n",
    "        pl.Series(name='cluster', values=clusters)\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nCluster Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_df = high_conf_df.filter(pl.col('cluster') == cluster)\n",
    "        print(f\"\\nCluster {cluster}:\")\n",
    "        print(f\"Size: {len(cluster_df)}\")\n",
    "        print(f\"Average confidence: {cluster_df['confidence'].mean():.3f}\")\n",
    "        print(f\"Bot ratio: {(cluster_df['prediction'] == 1).sum() / len(cluster_df):.2%}\")\n",
    "        \n",
    "        # Get top features for this cluster\n",
    "        cluster_center = kmeans.cluster_centers_[cluster]\n",
    "        feature_importance = list(zip(feature_cols, cluster_center))\n",
    "        feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        print(\"\\nTop distinguishing features:\")\n",
    "        for feature, value in feature_importance[:5]:\n",
    "            print(f\"{feature}: {value:.3f}\")\n",
    "    \n",
    "    return high_conf_df\n",
    "\n",
    "# Usage example:\n",
    "high_conf_predictions = analyze_high_confidence_predictions(detector, min_confidence=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_conf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a specific high confidence prediction\n",
    "result, importance = analyze_specific_prediction(detector, high_conf_predictions['fid'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze patterns in high confidence predictions\n",
    "clustered_df = analyze_confidence_clusters(high_conf_predictions, n_clusters=5)\n",
    "clustered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "farcaster-social-graph-notebooks-RMjVf8-3-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
