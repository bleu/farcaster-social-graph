{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseribeiro/Library/Caches/pypoetry/virtualenvs/farcaster-social-graph-notebooks-RMjVf8-3-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-09 09:23:52,124 - DetectorService - INFO - No existing model found. Model will be trained when `train` is called.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from farcaster_sybil_detection.config.defaults import Config\n",
    "from farcaster_sybil_detection.features.manager import FeatureManager\n",
    "from farcaster_sybil_detection.services.detector import DetectorService\n",
    "\n",
    "pl.Config.set_streaming_chunk_size(1_000_000)\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "\n",
    "\n",
    "# Create configuration\n",
    "config = Config(\n",
    "    data_path=Path(\"data\"),\n",
    "    checkpoint_dir=Path(\"checkpoints\"),\n",
    "    model_dir=Path(\"models\"),\n",
    "    debug_mode=True,\n",
    "    cache_enabled=True\n",
    ")\n",
    "\n",
    "# Initialize Feature Manager\n",
    "feature_manager = FeatureManager(config)\n",
    "\n",
    "# Initialize Detector Service with the Feature Manager\n",
    "detector = DetectorService(config, feature_manager)\n",
    "\n",
    "# Load Labels\n",
    "labels_df = pl.read_csv(\"data/labels.csv\")\n",
    "\n",
    "# Validate labels_df\n",
    "required_columns = {'fid', 'bot'}\n",
    "if not required_columns.issubset(labels_df.columns):\n",
    "    missing = required_columns - set(labels_df.columns)\n",
    "    raise ValueError(f\"Missing required columns in labels.csv: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = detector.trainer.train(labels_df)\n",
    "print(\"Training Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 09:23:52,132 - DetectorService - INFO - Making prediction for identifier: vitalik\n",
      "2024-12-09 09:23:52,133 - Predictor - INFO - Predicting for identifier: vitalik\n",
      "2024-12-09 09:23:52,133 - Predictor - INFO - Loading ID mapping from profile data...\n",
      "2024-12-09 09:23:52,134 - DatasetLoader - INFO - Loading profile_with_addresses with columns: ['fid', 'fname']\n",
      "2024-12-09 09:23:52,143 - DatasetLoader - INFO - Filtered dataset: 894048 records, 893130 unique FIDs\n",
      "2024-12-09 09:23:52,148 - DatasetLoader - INFO - Loaded farcaster-profile_with_addresses: 894048 records\n",
      "2024-12-09 09:23:52,150 - FeatureManager - INFO - Starting feature matrix build - Memory usage: 346.83 MB\n",
      "2024-12-09 09:23:52,150 - FeatureManager - INFO - Base FIDs: 1\n",
      "/Users/joseribeiro/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/features/manager.py:380: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  f\"Feature matrix schema: {feature_matrix.schema} ({len(feature_matrix.columns)} columns)\"\n",
      "/Users/joseribeiro/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/features/manager.py:380: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  f\"Feature matrix schema: {feature_matrix.schema} ({len(feature_matrix.columns)} columns)\"\n",
      "2024-12-09 09:23:52,151 - FeatureManager - INFO - Feature matrix schema: Schema({'fid': Int64}) (1 columns)\n",
      "2024-12-09 09:23:52,152 - FeatureManager - INFO - Feature matrix size: naive plan: (run LazyFrame.explain(optimized=True) to see the optimized plan)\n",
      "\n",
      " SELECT [col(\"fid\").count()] FROM\n",
      "  DF [\"fid\"]; PROJECT */1 COLUMNS; SELECTION: None\n",
      "2024-12-09 09:23:52,152 - FeatureManager - INFO - Determining feature build order...\n",
      "2024-12-09 09:23:52,152 - FeatureManager - INFO - Build order determined successfully: ['user_identity', 'network_analysis', 'content_engagement']\n",
      "2024-12-09 09:23:52,152 - FeatureManager - INFO - Starting user_identity - Memory usage: 347.41 MB\n",
      "2024-12-09 09:23:52,153 - FeatureManager - INFO - Using cached features for user_identity\n",
      "/Users/joseribeiro/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/features/manager.py:199: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  for col in new_features.columns\n",
      "/Users/joseribeiro/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/features/manager.py:200: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  if col not in df.columns and col != \"fid\"\n",
      "2024-12-09 09:23:52,154 - FeatureManager - INFO - Starting network_analysis - Memory usage: 347.44 MB\n",
      "2024-12-09 09:23:52,154 - FeatureManager - INFO - Using cached features for network_analysis\n",
      "2024-12-09 09:23:52,155 - FeatureManager - INFO - Starting content_engagement - Memory usage: 347.50 MB\n",
      "2024-12-09 09:23:52,155 - FeatureManager - INFO - Using cached features for content_engagement\n",
      "2024-12-09 09:23:52,155 - FeatureManager - INFO - Collecting final feature matrix\n",
      "2024-12-09 09:23:52,156 - FeatureManager - INFO - Feature matrix build completed - Memory usage: 349.52 MB\n",
      "2024-12-09 09:23:52,157 - Predictor - INFO - Generating prediction using shape: (1, 49)\n",
      "┌───────┬─────────┬─────────┬────────────┬───┬────────────┬─────────────┬─────────────┬────────────┐\n",
      "│ fid   ┆ has_ens ┆ has_bio ┆ has_avatar ┆ … ┆ cast_count ┆ reply_count ┆ mention_cou ┆ avg_cast_l │\n",
      "│ ---   ┆ ---     ┆ ---     ┆ ---        ┆   ┆ ---        ┆ ---         ┆ nt          ┆ ength      │\n",
      "│ i64   ┆ i64     ┆ i64     ┆ i64        ┆   ┆ u32        ┆ i64         ┆ ---         ┆ ---        │\n",
      "│       ┆         ┆         ┆            ┆   ┆            ┆             ┆ i64         ┆ f64        │\n",
      "╞═══════╪═════════╪═════════╪════════════╪═══╪════════════╪═════════════╪═════════════╪════════════╡\n",
      "│ 22032 ┆ 0       ┆ 1       ┆ 1          ┆ … ┆ 12         ┆ 3           ┆ 0           ┆ 26.083333  │\n",
      "└───────┴─────────┴─────────┴────────────┴───┴────────────┴─────────────┴─────────────┴────────────┘\n",
      "2024-12-09 09:23:52,157 - Predictor - INFO - Generating prediction using shape: (1, 49)\n",
      "2024-12-09 09:23:52,158 - Predictor - INFO - Model features: []\n",
      "2024-12-09 09:23:52,158 - Predictor - ERROR - Error generating prediction: Model has no defined feature set. Please ensure model was properly trained and saved with feature names.\n",
      "2024-12-09 09:23:52,158 - Predictor - ERROR - Error in prediction: Model has no defined feature set. Please ensure model was properly trained and saved with feature names.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 49)\n",
      "┌───────┬─────────┬─────────┬────────────┬───┬────────────┬─────────────┬─────────────┬────────────┐\n",
      "│ fid   ┆ has_ens ┆ has_bio ┆ has_avatar ┆ … ┆ cast_count ┆ reply_count ┆ mention_cou ┆ avg_cast_l │\n",
      "│ ---   ┆ ---     ┆ ---     ┆ ---        ┆   ┆ ---        ┆ ---         ┆ nt          ┆ ength      │\n",
      "│ i64   ┆ i64     ┆ i64     ┆ i64        ┆   ┆ u32        ┆ i64         ┆ ---         ┆ ---        │\n",
      "│       ┆         ┆         ┆            ┆   ┆            ┆             ┆ i64         ┆ f64        │\n",
      "╞═══════╪═════════╪═════════╪════════════╪═══╪════════════╪═════════════╪═════════════╪════════════╡\n",
      "│ 22032 ┆ 0       ┆ 1       ┆ 1          ┆ … ┆ 12         ┆ 3           ┆ 0           ┆ 26.083333  │\n",
      "└───────┴─────────┴─────────┴────────────┴───┴────────────┴─────────────┴─────────────┴────────────┘\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model has no defined feature set. Please ensure model was properly trained and saved with feature names.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvitalik\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# result = detector.predict(identifier='ipungkribo')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrediction Result:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/services/detector.py:75\u001b[0m, in \u001b[0;36mDetectorService.predict\u001b[0;34m(self, identifier)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make a prediction for a given FID or fname\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking prediction for identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/services/predictor.py:100\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, identifier)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Generate prediction\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating prediction using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m prediction_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Add identifier information\u001b[39;00m\n\u001b[1;32m    103\u001b[0m prediction_result\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    104\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfid\u001b[39m\u001b[38;5;124m\"\u001b[39m: fid, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfname\u001b[39m\u001b[38;5;124m\"\u001b[39m: fname \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    105\u001b[0m )\n",
      "File \u001b[0;32m~/projects/bleu/op/farcaster-social-graph/farcaster-sybil-detection/farcaster_sybil_detection/services/predictor.py:126\u001b[0m, in \u001b[0;36mPredictor._generate_prediction\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_features:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel has no defined feature set. Please ensure model was properly trained and saved with feature names.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Check feature alignment\u001b[39;00m\n\u001b[1;32m    131\u001b[0m current_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(feature_cols)\n",
      "\u001b[0;31mValueError\u001b[0m: Model has no defined feature set. Please ensure model was properly trained and saved with feature names."
     ]
    }
   ],
   "source": [
    "result = detector.predict(identifier='vitalik')\n",
    "# result = detector.predict(identifier='ipungkribo')\n",
    "print(\"\\nPrediction Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from framework.evaluation.metrics import EvaluationMetrics\n",
    "from framework.evaluation.segmentation import UserSegmentation\n",
    "from framework.evaluation.sampling import LabelingSampler\n",
    "from framework.evaluation.reporting import EvaluationReport\n",
    "\n",
    "def evaluate_model(detector, labels_df: pl.DataFrame, feature_matrix: pl.DataFrame):\n",
    "    \"\"\"Run comprehensive model evaluation\"\"\"\n",
    "    try:\n",
    "        print(\"Starting model evaluation...\")\n",
    "        \n",
    "        # 1. Get predictions for all labeled instances\n",
    "        fids = labels_df['fid'].to_list()\n",
    "        features = detector.feature_manager.get_features_for_fids(fids)\n",
    "        \n",
    "        # Get feature columns (excluding 'fid')\n",
    "        feature_cols = [col for col in features.columns if col != 'fid']\n",
    "        X = features.select(feature_cols).to_numpy()\n",
    "        \n",
    "        # Get predictions and probabilities\n",
    "        y_prob = detector.model.predict_proba(X)\n",
    "        y_pred = (y_prob[:, 1] >= 0.5).astype(int)\n",
    "        y_true = labels_df['bot'].to_numpy()\n",
    "        \n",
    "        print(f\"\\nDataset Statistics:\")\n",
    "        print(f\"Total samples: {len(y_true)}\")\n",
    "        print(f\"Positive samples (bots): {sum(y_true == 1)}\")\n",
    "        print(f\"Negative samples (humans): {sum(y_true == 0)}\")\n",
    "        \n",
    "        # 2. Initialize evaluation components\n",
    "        metrics = EvaluationMetrics()\n",
    "        segmentation = UserSegmentation()\n",
    "        report = EvaluationReport()\n",
    "        \n",
    "        # 3. Compute overall metrics\n",
    "        overall_metrics = metrics.compute_all_metrics(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_prob[:, 1]\n",
    "        )\n",
    "        \n",
    "        # 4. Get user segments and compute segment-specific metrics\n",
    "        segments = segmentation.segment_users(feature_matrix)\n",
    "        segment_results = {}\n",
    "        \n",
    "        for name, segment_df in segments.items():\n",
    "            print(f\"\\nEvaluating {name} segment...\")\n",
    "            # Get segment mask\n",
    "            segment_fids = segment_df['fid'].to_list()\n",
    "            # Create mask by matching FIDs in labels_df\n",
    "            segment_mask = labels_df['fid'].is_in(segment_fids).to_numpy()\n",
    "            \n",
    "            if not any(segment_mask):\n",
    "                print(f\"No labeled data for segment {name}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                segment_metrics = metrics.compute_all_metrics(\n",
    "                    y_true=y_true[segment_mask],\n",
    "                    y_pred=y_pred[segment_mask],\n",
    "                    y_prob=y_prob[segment_mask, 1]\n",
    "                )\n",
    "                \n",
    "                segment_results[name] = {\n",
    "                    'metrics': segment_metrics,\n",
    "                    'size': len(segment_df),\n",
    "                    'bot_ratio': (y_pred[segment_mask] == 1).mean(),\n",
    "                    'sample_size': sum(segment_mask)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing metrics for segment {name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # 5. Generate comprehensive report\n",
    "        report.add_metrics(overall_metrics)\n",
    "        report.add_segment_results(segment_results)\n",
    "        \n",
    "        # 6. Visualize results\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        metrics.plot_confusion_matrix(y_true, y_pred)\n",
    "        report.plot_probability_distribution(y_prob[:, 1])\n",
    "        \n",
    "        # 7. Print detailed report\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\nOverall Metrics:\")\n",
    "        for metric, value in overall_metrics.items():\n",
    "            print(f\"{metric}: {value:.3f}\")\n",
    "            \n",
    "        print(\"\\nSegment Performance:\")\n",
    "        for name, data in segment_results.items():\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"Size: {data['size']} users ({data['sample_size']} labeled)\")\n",
    "            print(f\"Bot Ratio: {data['bot_ratio']:.3f}\")\n",
    "            print(\"Metrics:\")\n",
    "            for metric, value in data['metrics'].items():\n",
    "                print(f\"  {metric}: {value:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'overall_metrics': overall_metrics,\n",
    "            'segment_results': segment_results,\n",
    "            'predictions': {\n",
    "                'y_true': y_true,\n",
    "                'y_pred': y_pred,\n",
    "                'y_prob': y_prob\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluation: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = feature_manager.build_feature_matrix()\n",
    "\n",
    "results = evaluate_model(detector, labels_df, matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_population_distribution(detector: DetectorService, feature_matrix: pl.DataFrame):\n",
    "    \"\"\"Analyze bot probability distribution across the entire population\"\"\"\n",
    "    print(\"\\nAnalyzing full population distribution...\")\n",
    "    \n",
    "    try:\n",
    "        # Get all features (excluding fid)\n",
    "        feature_cols = [col for col in feature_matrix.columns if col != 'fid']\n",
    "        X = feature_matrix.select(feature_cols).to_numpy()\n",
    "        fids = feature_matrix['fid'].to_list()\n",
    "        \n",
    "        # Get predictions for entire population\n",
    "        y_prob = detector.model.predict_proba(X)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(int)\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pl.DataFrame({\n",
    "            'fid': fids,\n",
    "            'bot_probability': y_prob,\n",
    "            'prediction': y_pred\n",
    "        })\n",
    "        \n",
    "        # Calculate distribution statistics\n",
    "        stats = {\n",
    "            'total_users': len(results_df),\n",
    "            'predicted_bots': (y_pred == 1).sum(),\n",
    "            'predicted_humans': (y_pred == 0).sum(),\n",
    "            'bot_ratio': (y_pred == 1).mean(),\n",
    "            'avg_probability': y_prob.mean(),\n",
    "            'median_probability': np.median(y_prob),\n",
    "            'std_probability': np.std(y_prob)\n",
    "        }\n",
    "        \n",
    "        # Calculate probability buckets\n",
    "        bucket_edges = np.arange(0, 1.1, 0.1)\n",
    "        hist, _ = np.histogram(y_prob, bins=bucket_edges)\n",
    "        bucket_stats = {f\"{bucket_edges[i]:.1f}-{bucket_edges[i+1]:.1f}\": count \n",
    "                       for i, count in enumerate(hist)}\n",
    "        \n",
    "        # Print distribution analysis\n",
    "        print(\"\\nPopulation Distribution Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total Users: {stats['total_users']:,}\")\n",
    "        print(f\"Predicted Bots: {stats['predicted_bots']:,} ({stats['bot_ratio']:.1%})\")\n",
    "        print(f\"Predicted Humans: {stats['predicted_humans']:,} ({1-stats['bot_ratio']:.1%})\")\n",
    "        print(\"\\nProbability Statistics:\")\n",
    "        print(f\"Mean Bot Probability: {stats['avg_probability']:.3f}\")\n",
    "        print(f\"Median Bot Probability: {stats['median_probability']:.3f}\")\n",
    "        print(f\"Std Dev: {stats['std_probability']:.3f}\")\n",
    "        \n",
    "        print(\"\\nProbability Distribution:\")\n",
    "        for bucket, count in bucket_stats.items():\n",
    "            print(f\"{bucket}: {count:,} users ({count/stats['total_users']:.1%})\")\n",
    "        \n",
    "        # Plot distribution\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(y_prob, bins=50, density=True, alpha=0.7)\n",
    "        plt.axvline(x=0.5, color='r', linestyle='--', label='Decision Boundary')\n",
    "        plt.xlabel('Bot Probability')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Distribution of Bot Probabilities')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Save results\n",
    "        results_df = results_df.sort('bot_probability', descending=True)\n",
    "        results_df.write_csv(\"population_analysis.csv\")\n",
    "        print(\"\\nResults saved to population_analysis.csv\")\n",
    "        \n",
    "        return {\n",
    "            'results_df': results_df,\n",
    "            'stats': stats,\n",
    "            'bucket_stats': bucket_stats\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in population analysis: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get all available FIDs from the profile dataset\n",
    "profiles_df = feature_manager.data_loader.load_dataset(\n",
    "    'profile_with_addresses', \n",
    "    columns=['fid']\n",
    ")\n",
    "all_fids = profiles_df['fid'].unique().sort()\n",
    "print(f\"Total population size: {len(all_fids)} FIDs\")\n",
    "\n",
    "# Clear any existing base FID filter\n",
    "feature_manager.data_loader.clear_cache()\n",
    "feature_manager.data_loader.set_base_fids(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature matrix for all FIDs\n",
    "print(\"Building feature matrix for full population...\")\n",
    "full_matrix = feature_manager.build_feature_matrix()\n",
    "print(f\"Feature matrix shape: {full_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_matrix.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "population_analysis = analyze_population_distribution(detector, matrix)\n",
    "\n",
    "# If you want to examine specific probability ranges:\n",
    "results_df = population_analysis['results_df']\n",
    "\n",
    "# High confidence bots (e.g., >90% probability)\n",
    "high_conf_bots = results_df.filter(pl.col('bot_probability') > 0.9)\n",
    "print(f\"\\nHigh Confidence Bots (>90%): {len(high_conf_bots)}\")\n",
    "\n",
    "# High confidence humans (e.g., <10% probability)\n",
    "high_conf_humans = results_df.filter(pl.col('bot_probability') < 0.1)\n",
    "print(f\"High Confidence Humans (<10%): {len(high_conf_humans)}\")\n",
    "\n",
    "# Uncertain predictions (e.g., 40-60% probability)\n",
    "uncertain = results_df.filter(\n",
    "    (pl.col('bot_probability') >= 0.4) & \n",
    "    (pl.col('bot_probability') <= 0.6)\n",
    ")\n",
    "print(f\"Uncertain Predictions (40-60%): {len(uncertain)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_manager.data_loader.load_dataset(\n",
    "    'profile_with_addresses', \n",
    "    columns=['fid', 'fname']\n",
    ").filter(\n",
    "    pl.col('fname').is_not_null()\n",
    ").with_columns(\n",
    "    [\n",
    "        pl.col('fid').cast(pl.Int64).alias('fid'),\n",
    "    ]\n",
    ").join(full_matrix.with_columns(\n",
    "    [\n",
    "        pl.col('fid').cast(pl.Int64).alias('fid'),\n",
    "    ]\n",
    "), on='fid').join(\n",
    "    results_df.with_columns([\n",
    "        pl.col('fid').cast(pl.Int64).alias('fid'),\n",
    "    ]), on='fid'\n",
    ").filter(\n",
    "    pl.col('bot_probability') > 0.9\n",
    ").select([\n",
    "    'fid', 'fname', 'bot_probability', 'prediction'\n",
    "]).sort('bot_probability').filter(\n",
    "    pl.col('fname') == 'vitalik'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "farcaster-social-graph-notebooks-RMjVf8-3-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
