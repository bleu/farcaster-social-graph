{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set base FIDs: 8 records\n",
      "Filtering links by 8 base FIDs\n",
      "Filtering links by 8 base FIDs\n",
      "Filtering casts by 8 base FIDs\n",
      "Filtering reactions by 8 base FIDs\n",
      "Filtering verifications by 8 base FIDs\n",
      "Filtering account_verifications by 8 base FIDs\n",
      "Filtering user_data by 8 base FIDs\n",
      "Filtering storage by 8 base FIDs\n",
      "Filtering signers by 8 base FIDs\n",
      "Filtering casts by 8 base FIDs\n",
      "Filtering user_data by 8 base FIDs\n",
      "Filtering verifications by 8 base FIDs\n",
      "Filtering casts by 8 base FIDs\n",
      "Filtering warpcast_power_users by 8 base FIDs\n",
      "Filtering casts by 8 base FIDs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c_/34qp2_2j261cry3ssbl616g00000gn/T/ipykernel_85613/144814217.py:449: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  if col not in df.columns:\n",
      "/var/folders/c_/34qp2_2j261cry3ssbl616g00000gn/T/ipykernel_85613/144814217.py:453: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  if 'first_follow' in df.columns and 'last_follow' in df.columns:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering channel_follows by 8 base FIDs\n",
      "Filtering channel_members by 8 base FIDs\n",
      "Filtering power_users by 8 base FIDs\n",
      "Filtering casts by 8 base FIDs\n",
      "Filtering casts by 8 base FIDs\n",
      "Filtering reactions by 8 base FIDs\n",
      "Collecting final feature matrix...\n",
      "Feature matrix built with shape: (8, 94)\n",
      "Feature matrix built with shape: (8, 94)\n",
      "shape: (5, 94)\n",
      "┌───────┬────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
      "│ fid   ┆ fname  ┆ bio        ┆ avatar_url ┆ … ┆ power_ment ┆ hour_diver ┆ weekday_di ┆ total_acti │\n",
      "│ ---   ┆ ---    ┆ ---        ┆ ---        ┆   ┆ ions_count ┆ sity       ┆ versity    ┆ vities     │\n",
      "│ i64   ┆ str    ┆ str        ┆ str        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
      "│       ┆        ┆            ┆            ┆   ┆ u32        ┆ struct[2]  ┆ struct[2]  ┆ u32        │\n",
      "╞═══════╪════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
      "│ 23587 ┆ suzhen ┆ null       ┆ null       ┆ … ┆ 4          ┆ null       ┆ null       ┆ 4          │\n",
      "│ 22892 ┆ 0x59   ┆ Nelia      ┆ https://i. ┆ … ┆ 5          ┆ null       ┆ null       ┆ 17         │\n",
      "│       ┆        ┆            ┆ imgur.com/ ┆   ┆            ┆            ┆            ┆            │\n",
      "│       ┆        ┆            ┆ D2vZGJq.pn ┆   ┆            ┆            ┆            ┆            │\n",
      "│       ┆        ┆            ┆ …          ┆   ┆            ┆            ┆            ┆            │\n",
      "│ 2004  ┆ palm   ┆ GP at a    ┆ https://lh ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
      "│       ┆        ┆ crypto VC  ┆ 3.googleus ┆   ┆            ┆            ┆            ┆            │\n",
      "│       ┆        ┆ fund and   ┆ ercontent. ┆   ┆            ┆            ┆            ┆            │\n",
      "│       ┆        ┆ ban…       ┆ …          ┆   ┆            ┆            ┆            ┆            │\n",
      "│ 23586 ┆ liuyi  ┆ null       ┆ https://i. ┆ … ┆ 10         ┆ null       ┆ null       ┆ 100        │\n",
      "│       ┆        ┆            ┆ imgur.com/ ┆   ┆            ┆            ┆            ┆            │\n",
      "│       ┆        ┆            ┆ YKw9Aud.pn ┆   ┆            ┆            ┆            ┆            │\n",
      "│       ┆        ┆            ┆ …          ┆   ┆            ┆            ┆            ┆            │\n",
      "│ 23710 ┆ oy     ┆ null       ┆ null       ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
      "└───────┴────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴────────────┘\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# new version\n",
    "import os\n",
    "import polars as pl\n",
    "from typing import List\n",
    "\n",
    "# Configure Polars for memory usage\n",
    "pl.Config.set_streaming_chunk_size(1_000_000)\n",
    "\n",
    "class FeatureSet:\n",
    "    \"\"\"Track feature dependencies and versioning\"\"\"\n",
    "    def __init__(self, name: str, version: str, dependencies: List[str] = None):\n",
    "        self.name = name\n",
    "        self.version = version  # Version of feature calculation logic\n",
    "        self.dependencies = dependencies or []\n",
    "        self.checkpoint_path = None\n",
    "        self.last_modified = None\n",
    "\n",
    "class StreamingDataLoader:\n",
    "    \"\"\"Memory-efficient dataset loader using Polars streaming capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, checkpoint_dir: str, debug_mode: bool = True, sample_size: int = 100):\n",
    "        self.data_path = data_path\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.base_fids = None\n",
    "        self.debug_mode = debug_mode\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def set_base_fids(self, fids: pl.Series):\n",
    "        \"\"\"Set base FIDs for filtering downstream datasets.\"\"\"\n",
    "        self.base_fids = fids.unique()\n",
    "        print(f\"Set base FIDs: {len(self.base_fids)} records\")\n",
    "\n",
    "    def get_checkpoint_fids(self) -> bool:\n",
    "        \"\"\"Get base FIDs from profile checkpoint if it exists\"\"\"\n",
    "        profile_checkpoint = f\"{self.checkpoint_dir}/profile_features.parquet\"\n",
    "        if os.path.exists(profile_checkpoint):\n",
    "            df = pl.read_parquet(profile_checkpoint)\n",
    "            if 'fid' in df.columns:\n",
    "                self.base_fids = df['fid']\n",
    "                print(f\"Loaded base FIDs from checkpoint: {len(self.base_fids)} records\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_dataset(self, name: str, columns: List[str] = None, batch_size: int = 100_000) -> pl.LazyFrame:\n",
    "        \"\"\"Get dataset as a LazyFrame with streaming and optional column selection.\"\"\"\n",
    "        path = f\"{self.data_path}/farcaster-{name}-0-1733162400.parquet\"\n",
    "        \n",
    "        try:\n",
    "            # Start with lazy scanning\n",
    "            scan_query = pl.scan_parquet(\n",
    "                path,\n",
    "                n_rows=batch_size  # Process in batches\n",
    "            )\n",
    "            \n",
    "            # Apply early column selection if specified\n",
    "            if columns:\n",
    "                scan_query = scan_query.select(columns)\n",
    "            \n",
    "            if self.debug_mode:\n",
    "                if self.base_fids is None:\n",
    "                    # Try to get FIDs from checkpoint first\n",
    "                    if not self.get_checkpoint_fids():\n",
    "                        if name == 'profile_with_addresses':\n",
    "                            collected = scan_query.limit(self.sample_size).collect()\n",
    "                            self.base_fids = collected['fid']\n",
    "                            print(f\"Established new base FIDs from {name}: {len(self.base_fids)} records\")\n",
    "                            return collected.lazy()\n",
    "                        else:\n",
    "                            print(f\"Warning: No base FIDs available for {name}\")\n",
    "                            return scan_query.limit(self.sample_size)\n",
    "                else:\n",
    "                    print(f\"Filtering {name} by {len(self.base_fids)} base FIDs\")\n",
    "                    scan_query = scan_query.filter(pl.col('fid').is_in(self.base_fids))\n",
    "                    \n",
    "            # Apply early FID filtering if available\n",
    "            elif self.base_fids is not None:\n",
    "                scan_query = scan_query.filter(pl.col('fid').is_in(self.base_fids))\n",
    "            \n",
    "            return scan_query\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_checkpoint(self, name: str) -> pl.LazyFrame:\n",
    "        \"\"\"Load checkpoint as LazyFrame if it exists.\"\"\"\n",
    "        path = f\"{self.checkpoint_dir}/{name}_features.parquet\"\n",
    "        if os.path.exists(path):\n",
    "            return pl.scan_parquet(path)\n",
    "        return None\n",
    "\n",
    "    def save_checkpoint(self, lf: pl.LazyFrame, name: str):\n",
    "        \"\"\"Save LazyFrame as checkpoint.\"\"\"\n",
    "        path = f\"{self.checkpoint_dir}/{name}_features.parquet\"\n",
    "        # Collect with streaming and save\n",
    "        lf.collect(streaming=True).write_parquet(path)\n",
    "\n",
    "\n",
    "class OptimizedFeatureEngineering:\n",
    "    \"\"\"Example class with memory-efficient feature engineering using streaming.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, checkpoint_dir: str):\n",
    "        self.data_path = data_path\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.loader = StreamingDataLoader(data_path, checkpoint_dir)\n",
    "\n",
    "        self.feature_sets = {   \n",
    "            # Base features\n",
    "            'profile': FeatureSet('profile', '1.0'),\n",
    "            'network': FeatureSet('network', '1.0'),\n",
    "            'temporal': FeatureSet('temporal', '1.0', ['network']),\n",
    "            \n",
    "            # Activity features\n",
    "            'cast': FeatureSet('cast', '1.0'),\n",
    "            'reaction': FeatureSet('reaction', '1.0'),\n",
    "            'channel': FeatureSet('channel', '1.0'),\n",
    "            'verification': FeatureSet('verification', '1.0'),\n",
    "            \n",
    "            # Account features\n",
    "            'user_data': FeatureSet('user_data', '1.0'),\n",
    "            'storage': FeatureSet('storage', '1.0'),\n",
    "            'signers': FeatureSet('signers', '1.0'),\n",
    "            \n",
    "            # Interaction patterns\n",
    "            'engagement': FeatureSet('engagement', '1.0', \n",
    "                ['cast', 'reaction', 'channel']),\n",
    "            'mentions': FeatureSet('mentions', '1.0', \n",
    "                ['cast', 'network']),\n",
    "            'reply_patterns': FeatureSet('reply_patterns', '1.0', \n",
    "                ['cast', 'temporal']),\n",
    "            \n",
    "            # Network quality\n",
    "            'network_quality': FeatureSet('network_quality', '1.0', \n",
    "                ['network', 'engagement']),\n",
    "            'power_user_interaction': FeatureSet('power_user_interaction', '1.0', \n",
    "                ['network', 'temporal']),\n",
    "            'cluster_analysis': FeatureSet('cluster_analysis', '1.0', \n",
    "                ['network', 'engagement']),\n",
    "            \n",
    "            # Behavioral patterns\n",
    "            'activity_patterns': FeatureSet('activity_patterns', '1.0', \n",
    "                ['temporal', 'cast', 'reaction']),\n",
    "            'update_behavior': FeatureSet('update_behavior', '1.0', \n",
    "                ['user_data', 'profile']),\n",
    "            'verification_patterns': FeatureSet('verification_patterns', '1.0', \n",
    "                ['verification', 'temporal']),\n",
    "            \n",
    "            # Meta features\n",
    "            'authenticity': FeatureSet('authenticity', '2.0', [\n",
    "                'profile', 'network', 'channel', 'verification',\n",
    "                'engagement', 'network_quality', 'activity_patterns'\n",
    "            ]),\n",
    "            'influence': FeatureSet('influence', '1.0', [\n",
    "                'network', 'engagement', 'power_user_interaction'\n",
    "            ]),\n",
    "            \n",
    "            # Final derived features\n",
    "            'derived': FeatureSet('derived', '2.0', [\n",
    "                'network', 'temporal', 'authenticity',\n",
    "                'engagement', 'network_quality', 'influence'\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        # Initialize checkpoint tracking\n",
    "        self._init_checkpoints()\n",
    "\n",
    "\n",
    "    def _init_checkpoints(self):\n",
    "        \"\"\"Initialize checkpoint paths and check existing files\"\"\"\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        for name, feature_set in self.feature_sets.items():\n",
    "            path = f\"{self.checkpoint_dir}/{name}_features.parquet\"\n",
    "            feature_set.checkpoint_path = path\n",
    "            \n",
    "            if os.path.exists(path):\n",
    "                feature_set.last_modified = os.path.getmtime(path)\n",
    "\n",
    "    def _needs_rebuild(self, feature_set: FeatureSet) -> bool:\n",
    "        \"\"\"Check if feature set needs to be rebuilt\"\"\"\n",
    "        # Check dependencies too\n",
    "        if not os.path.exists(feature_set.checkpoint_path):\n",
    "            return True\n",
    "        \n",
    "        for dep in feature_set.dependencies:\n",
    "            dep_set = self.feature_sets.get(dep)\n",
    "            if dep_set and dep_set.last_modified > feature_set.last_modified:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _validate_feature_addition(self, original_df: pl.LazyFrame, \n",
    "                                new_df: pl.LazyFrame,\n",
    "                                base_fids: pl.Series,\n",
    "                                feature_name: str) -> pl.LazyFrame:\n",
    "        \"\"\"Validate and fix feature addition results\"\"\"\n",
    "        if new_df is None:\n",
    "            print(f\"Error: {feature_name} returned None\")\n",
    "            return original_df\n",
    "            \n",
    "        # Validate FIDs\n",
    "        if not self._validate_checkpoint_compatibility(new_df, base_fids):\n",
    "            return original_df\n",
    "        \n",
    "        return new_df\n",
    "        \n",
    "    def extract_profile_features(self) -> pl.LazyFrame:\n",
    "        \"\"\"Extract profile features as a LazyFrame.\"\"\"\n",
    "        profiles = self.loader.get_dataset('profile_with_addresses', \n",
    "            ['fid', 'fname', 'bio', 'avatar_url', 'verified_addresses', 'display_name'])\n",
    "        \n",
    "        return (\n",
    "            profiles\n",
    "            .filter(pl.col('fname').is_not_null() & (pl.col('fname') != \"\"))\n",
    "            .with_columns([\n",
    "                pl.col('fid').cast(pl.Int64),\n",
    "                pl.col('fname').str.contains(r'\\.eth$').cast(pl.Int32).alias('has_ens'),\n",
    "                (pl.col('bio').is_not_null() & (pl.col('bio') != \"\")).cast(pl.Int32).alias('has_bio'),\n",
    "                pl.col('avatar_url').is_not_null().cast(pl.Int32).alias('has_avatar'),\n",
    "                pl.when(pl.col('verified_addresses').str.contains(','))\n",
    "                 .then(pl.col('verified_addresses').str.contains(',').cast(pl.Int32) + 1)\n",
    "                 .otherwise(pl.when(pl.col('verified_addresses') != '[]')\n",
    "                             .then(1)\n",
    "                             .otherwise(0))\n",
    "                 .alias('verification_count'),\n",
    "                pl.col('display_name').is_not_null().cast(pl.Int32).alias('has_display_name')\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    def add_network_quality_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Build network quality features with safer dependency handling\"\"\"\n",
    "        # Load power users and calculate metrics\n",
    "        power_users = self.loader.get_dataset('power_users', ['fid'])\n",
    "        power_fids = power_users.select('fid').collect()['fid'].cast(pl.Int64)\n",
    "        \n",
    "        # Calculate power user metrics\n",
    "        casts = self.loader.get_dataset('casts', \n",
    "            ['fid', 'parent_fid', 'mentions', 'deleted_at'])\n",
    "            \n",
    "        power_metrics = (\n",
    "            casts.filter(pl.col('deleted_at').is_null())\n",
    "            .with_columns([\n",
    "                pl.col('parent_fid').cast(pl.Int64).is_in(power_fids)\n",
    "                    .alias('is_power_reply'),\n",
    "                pl.col('mentions').str.contains(power_fids.cast(str).str.concat('|'))\n",
    "                    .alias('has_power_mention')\n",
    "            ])\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.col('is_power_reply').sum().alias('power_reply_count'),\n",
    "                pl.col('has_power_mention').sum().alias('power_mentions_count')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        return base_lf.join(power_metrics, on='fid', how='left')\n",
    "    def add_network_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add network features using streaming joins.\"\"\"\n",
    "        links = self.loader.get_dataset('links', \n",
    "            ['fid', 'target_fid', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        # Following patterns\n",
    "        following = (\n",
    "            links\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.len().alias('following_count'),\n",
    "                pl.n_unique('target_fid').alias('unique_following_count')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Follower patterns\n",
    "        followers = (\n",
    "            links\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .group_by('target_fid')\n",
    "            .agg([\n",
    "                pl.len().alias('follower_count'),\n",
    "                pl.n_unique('fid').alias('unique_follower_count')\n",
    "            ])\n",
    "            .rename({'target_fid': 'fid'})\n",
    "        )\n",
    "        \n",
    "        result = base_lf.join(following, on='fid', how='left')\n",
    "        result = result.join(followers, on='fid', how='left')\n",
    "        \n",
    "        # Derived network metrics\n",
    "        result = result.with_columns([\n",
    "            (pl.col('follower_count') / (pl.col('following_count') + 1)).alias('follower_ratio'),\n",
    "            (pl.col('unique_follower_count') / (pl.col('unique_following_count') + 1))\n",
    "                .alias('unique_follower_ratio')\n",
    "        ])\n",
    "        \n",
    "        return result\n",
    "    def add_temporal_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add temporal features using streaming operations.\"\"\"\n",
    "        links = self.loader.get_dataset('links', \n",
    "            ['fid', 'timestamp', 'deleted_at'])\n",
    "\n",
    "        temporal_features = (\n",
    "            links\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .filter(pl.col('timestamp').is_not_null())\n",
    "            .with_columns(pl.col('timestamp').cast(pl.Datetime))\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.len().alias('total_activity'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().mean().alias('avg_hours_between_actions'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().std().alias('std_hours_between_actions'),\n",
    "                pl.col('timestamp').dt.weekday().std().alias('weekday_variance'),\n",
    "                (pl.col('timestamp').diff().dt.total_hours() < 1).sum().alias('rapid_actions'),\n",
    "                (pl.col('timestamp').diff().dt.total_hours() > 24).sum().alias('long_gaps'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().quantile(0.9).alias('p90_time_between_actions'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().quantile(0.1).alias('p10_time_between_actions')\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        return base_lf.join(temporal_features, on='fid', how='left')\n",
    "\n",
    "    def add_cast_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add cast-related features using streaming.\"\"\"\n",
    "        casts = self.loader.get_dataset('casts', \n",
    "            ['fid', 'text', 'parent_hash', 'mentions', 'deleted_at'])\n",
    "\n",
    "        cast_features = (\n",
    "            casts\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .with_columns([\n",
    "                pl.when(pl.col('text').is_not_null())\n",
    "                    .then(pl.col('text').str.len_chars())\n",
    "                    .otherwise(0)\n",
    "                    .alias('cast_length'),\n",
    "                pl.col('parent_hash').is_not_null().cast(pl.Int32).alias('is_reply'),\n",
    "                pl.when(pl.col('mentions').is_not_null() & \n",
    "                    (pl.col('mentions') != '') & \n",
    "                    (pl.col('mentions') != '[]'))\n",
    "                    .then(1)\n",
    "                    .otherwise(0)\n",
    "                    .alias('has_mentions')\n",
    "            ])\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.len().alias('cast_count'),\n",
    "                pl.col('cast_length').mean().alias('avg_cast_length'),\n",
    "                pl.col('is_reply').sum().alias('reply_count'),\n",
    "                pl.col('has_mentions').sum().alias('mentions_count')\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        return base_lf.join(cast_features, on='fid', how='left')\n",
    "\n",
    "    def add_reaction_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add reaction-related features using streaming.\"\"\"\n",
    "        reactions = self.loader.get_dataset('reactions', \n",
    "            ['fid', 'reaction_type', 'target_fid', 'timestamp', 'deleted_at'])\n",
    "\n",
    "        reaction_features = (\n",
    "            reactions\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .with_columns(pl.col('timestamp').cast(pl.Datetime))\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.len().alias('total_reactions'),\n",
    "                (pl.col('reaction_type') == 1).sum().alias('like_count'),\n",
    "                (pl.col('reaction_type') == 2).sum().alias('recast_count'),\n",
    "                pl.n_unique('target_fid').alias('unique_users_reacted_to'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().mean().alias('avg_hours_between_reactions'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().std().alias('std_hours_between_reactions')\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        return base_lf.join(reaction_features, on='fid', how='left')\n",
    "\n",
    "    def add_verification_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add verification-related features using streaming.\"\"\"\n",
    "        verifications = self.loader.get_dataset('verifications', \n",
    "            ['fid', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        acc_verifications = self.loader.get_dataset('account_verifications', \n",
    "            ['fid', 'platform', 'verified_at'])\n",
    "\n",
    "        # Process on-chain verifications\n",
    "        verif_features = (\n",
    "            verifications\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .with_columns(pl.col('timestamp').cast(pl.Datetime))\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.len().alias('total_verifications'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().mean().alias('avg_hours_between_verifications'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().std().alias('std_hours_between_verifications'),\n",
    "                (pl.col('timestamp').diff().dt.total_hours() < 1).sum().alias('rapid_verifications')\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        # Process platform verifications\n",
    "        platform_features = (\n",
    "            acc_verifications\n",
    "            .with_columns(pl.col('verified_at').cast(pl.Datetime))\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.n_unique('platform').alias('platforms_verified'),\n",
    "                pl.col('verified_at').min().alias('first_platform_verification'),\n",
    "                pl.col('verified_at').max().alias('last_platform_verification')\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        result = base_lf.join(verif_features, on='fid', how='left')\n",
    "        return result.join(platform_features, on='fid', how='left')\n",
    "\n",
    "    def add_authenticity_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Calculate authenticity features using streaming.\"\"\"\n",
    "        return (\n",
    "            base_lf\n",
    "            .with_columns([\n",
    "                # Profile completeness\n",
    "                ((pl.col('has_bio') + \n",
    "                pl.col('has_avatar') + \n",
    "                pl.col('has_ens') + \n",
    "                (pl.col('verification_count') > 0).cast(pl.Int64)) / 4.0\n",
    "                ).alias('profile_completeness'),\n",
    "\n",
    "                # Network balance\n",
    "                (pl.when(pl.col('following_count') + pl.col('follower_count') > 0)\n",
    "                .then(1.0 - (pl.col('following_count') - pl.col('follower_count')).abs() /\n",
    "                    (pl.col('following_count') + pl.col('follower_count')))\n",
    "                .otherwise(0.0)\n",
    "                ).alias('network_balance'),\n",
    "\n",
    "                # Update naturalness\n",
    "                (pl.when(pl.col('total_updates') > 0)\n",
    "                .then(1.0 - pl.col('profile_update_consistency'))\n",
    "                .otherwise(0.0)\n",
    "                ).alias('update_naturalness')\n",
    "            ])\n",
    "            .with_columns([\n",
    "                (pl.col('profile_completeness') * 0.4 +\n",
    "                pl.col('network_balance') * 0.3 +\n",
    "                pl.col('update_naturalness') * 0.3\n",
    "                ).alias('authenticity_score')\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    def add_influence_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Add influence features with proper error handling\"\"\"\n",
    "        try:\n",
    "            # Ensure required columns exist and are properly initialized\n",
    "            required_cols = ['follower_count', 'following_count', 'total_reactions', 'cast_count']\n",
    "            for col in required_cols:\n",
    "                if col not in df.columns:\n",
    "                    df = df.with_columns(pl.lit(0).alias(col))\n",
    "            \n",
    "            # Calculate time span if possible\n",
    "            if 'first_follow' in df.columns and 'last_follow' in df.columns:\n",
    "                df = df.with_columns([\n",
    "                    pl.when(pl.col('last_follow').is_not_null() & pl.col('first_follow').is_not_null())\n",
    "                    .then((pl.col('last_follow') - pl.col('first_follow')).dt.total_hours())\n",
    "                    .otherwise(0)\n",
    "                    .alias('follow_time_span_hours')\n",
    "                ])\n",
    "            else:\n",
    "                df = df.with_columns(pl.lit(0).alias('follow_time_span_hours'))\n",
    "\n",
    "            # Calculate influence metrics safely\n",
    "            df = df.with_columns([\n",
    "                # Normalize influence metrics\n",
    "                ((pl.col('follower_count').fill_null(0) * 0.4 +\n",
    "                pl.col('total_reactions').fill_null(0) * 0.3 +\n",
    "                pl.col('cast_count').fill_null(0) * 0.3) / \n",
    "                (pl.col('following_count').fill_null(0) + 1)\n",
    "                ).alias('influence_score'),\n",
    "                \n",
    "                # Safe engagement rate calculation\n",
    "                (pl.when(pl.col('cast_count') > 0)\n",
    "                .then(pl.col('total_reactions') / pl.col('cast_count'))\n",
    "                .otherwise(0)\n",
    "                ).alias('engagement_rate'),\n",
    "                \n",
    "                # Safe follower growth rate calculation\n",
    "                (pl.when(pl.col('follow_time_span_hours') > 0)\n",
    "                .then(pl.col('follower_count') / pl.col('follow_time_span_hours'))\n",
    "                .otherwise(0)\n",
    "                ).alias('follower_growth_rate')\n",
    "            ])\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in influence features: {str(e)}\")\n",
    "            raise\n",
    "            return df\n",
    "\n",
    "\n",
    "    def add_user_data_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add user data features using streaming.\"\"\"\n",
    "        user_data = self.loader.get_dataset('user_data', \n",
    "            ['fid', 'type', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        update_features = (\n",
    "            user_data\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.len().alias('total_user_data_updates'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().mean()\n",
    "                    .alias('avg_update_interval')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        return base_lf.join(update_features, on='fid', how='left')\n",
    "\n",
    "    def add_storage_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add storage-related features using streaming.\"\"\"\n",
    "        storage = self.loader.get_dataset('storage', \n",
    "            ['fid', 'units', 'deleted_at'])\n",
    "        \n",
    "        storage_features = (\n",
    "            storage\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.col('units').mean().alias('avg_storage_units'),\n",
    "                pl.col('units').max().alias('max_storage_units'),\n",
    "                pl.len().alias('storage_update_count')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        return base_lf.join(storage_features, on='fid', how='left')\n",
    "\n",
    "    def add_signer_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add signer-related features using streaming.\"\"\"\n",
    "        signers = self.loader.get_dataset('signers', \n",
    "            ['fid', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        signer_features = (\n",
    "            signers\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.len().alias('signer_count'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().mean()\n",
    "                    .alias('avg_hours_between_signers'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().std()\n",
    "                    .alias('std_hours_between_signers')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        return base_lf.join(signer_features, on='fid', how='left')\n",
    "\n",
    "    def add_mentions_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add mention pattern features using streaming.\"\"\"\n",
    "        casts = self.loader.get_dataset('casts', \n",
    "            ['fid', 'mentions', 'deleted_at'])\n",
    "        \n",
    "        mention_features = (\n",
    "            casts\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .with_columns([\n",
    "                pl.when(\n",
    "                    pl.col('mentions').is_not_null() & \n",
    "                    (pl.col('mentions') != '') & \n",
    "                    (pl.col('mentions') != '[]')\n",
    "                )\n",
    "                .then(pl.col('mentions').str.json_decode().list.len())\n",
    "                .otherwise(0)\n",
    "                .alias('mention_count')\n",
    "            ])\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.col('mention_count').sum().alias('total_mentions'),\n",
    "                pl.col('mention_count').mean().alias('avg_mentions_per_cast'),\n",
    "                (pl.col('mention_count') > 0).sum().alias('casts_with_mentions')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        return base_lf.join(mention_features, on='fid', how='left')\n",
    "\n",
    "    def add_reply_patterns_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add reply pattern features using streaming.\"\"\"\n",
    "        casts = self.loader.get_dataset('casts', \n",
    "            ['fid', 'parent_hash', 'parent_fid', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        reply_features = (\n",
    "            casts\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .filter(pl.col('parent_hash').is_not_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.len().alias('total_replies'),\n",
    "                pl.n_unique('parent_fid').alias('unique_users_replied_to'),\n",
    "                pl.col('timestamp').diff().dt.total_seconds().mean()\n",
    "                    .alias('avg_seconds_between_replies'),\n",
    "                pl.col('timestamp').diff().dt.total_seconds().std()\n",
    "                    .alias('std_seconds_between_replies')\n",
    "            ])\n",
    "            .with_columns([\n",
    "                (pl.col('unique_users_replied_to') / pl.col('total_replies'))\n",
    "                    .alias('reply_diversity'),\n",
    "                (pl.col('std_seconds_between_replies') / \n",
    "                pl.col('avg_seconds_between_replies'))\n",
    "                    .alias('reply_timing_variability')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        return base_lf.join(reply_features, on='fid', how='left')\n",
    "\n",
    "    def add_power_user_interaction_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add power user interaction features using streaming.\"\"\"\n",
    "        # Load power users\n",
    "        power_users = self.loader.get_dataset('warpcast_power_users', ['fid'])\n",
    "        power_fids = power_users.select('fid').collect()['fid'].cast(pl.Int64)\n",
    "        \n",
    "        # Get interactions\n",
    "        casts = self.loader.get_dataset('casts',\n",
    "            ['fid', 'parent_fid', 'mentions', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        power_cast_features = (\n",
    "            casts\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .with_columns([\n",
    "                pl.col('parent_fid').cast(pl.Int64).is_in(power_fids)\n",
    "                    .alias('is_power_reply'),\n",
    "                pl.col('mentions').str.contains(power_fids.cast(str).str.concat('|'))\n",
    "                    .alias('has_power_mention')\n",
    "            ])\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.col('is_power_reply').sum().alias('power_user_replies'),\n",
    "                pl.col('has_power_mention').sum().alias('power_user_mentions'),\n",
    "                pl.len().alias('total_casts')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        return base_lf.join(power_cast_features, on='fid', how='left')\n",
    "\n",
    "    def add_derived_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add derived/calculated features using streaming.\"\"\"\n",
    "        return (\n",
    "            base_lf\n",
    "            .with_columns([\n",
    "                # Log transformations\n",
    "                pl.col('follower_ratio').fill_null(0).log1p()\n",
    "                    .alias('follower_ratio_log'),\n",
    "                pl.col('unique_follower_ratio').fill_null(0).log1p()\n",
    "                    .alias('unique_follower_ratio_log'),\n",
    "                \n",
    "                # Binary features\n",
    "                (pl.col('follower_count') > pl.col('following_count'))\n",
    "                    .cast(pl.Int32)\n",
    "                    .alias('has_more_followers'),\n",
    "                \n",
    "                # Composite metrics\n",
    "                ((pl.col('following_count') - pl.col('follower_count')).abs() /\n",
    "                (pl.col('following_count') + pl.col('follower_count') + 1)\n",
    "                ).alias('follow_balance_ratio')\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    def add_engagement_metrics(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add engagement metrics using streaming.\"\"\"\n",
    "        return (\n",
    "            base_lf\n",
    "            .with_columns([\n",
    "                # Overall engagement score\n",
    "                ((pl.col('cast_count') + \n",
    "                pl.col('total_reactions') + \n",
    "                pl.col('channel_memberships')) / 3.0\n",
    "                ).alias('engagement_score'),\n",
    "                \n",
    "                # Activity balance\n",
    "                (pl.col('cast_count') / (pl.col('total_reactions') + 1))\n",
    "                    .alias('creation_consumption_ratio'),\n",
    "                \n",
    "                # Interaction diversity\n",
    "                (pl.col('unique_users_reacted_to') / \n",
    "                (pl.col('total_reactions') + 1))\n",
    "                    .alias('interaction_diversity')\n",
    "            ])\n",
    "        )\n",
    "    def add_activity_patterns(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add activity patterns with fully safe calculations\"\"\"\n",
    "        # Get activity data\n",
    "        casts = self.loader.get_dataset('casts', ['fid', 'timestamp', 'deleted_at'])\n",
    "        reactions = self.loader.get_dataset('reactions', ['fid', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        # Combine valid activities\n",
    "        activities = pl.concat([\n",
    "            casts.filter(pl.col('deleted_at').is_null())\n",
    "                .select(['fid', 'timestamp']),\n",
    "            reactions.filter(pl.col('deleted_at').is_null())\n",
    "                .select(['fid', 'timestamp'])\n",
    "        ])\n",
    "        \n",
    "        activity_features = (activities\n",
    "            .with_columns([\n",
    "                pl.col('timestamp').cast(pl.Datetime).dt.hour().alias('hour'),\n",
    "                pl.col('timestamp').cast(pl.Datetime).dt.weekday().alias('weekday')\n",
    "            ])\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.col('hour').value_counts().std().alias('hour_diversity'),\n",
    "                pl.col('weekday').value_counts().std().alias('weekday_diversity'),\n",
    "                pl.len().alias('total_activities')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        return base_lf.join(activity_features, on='fid', how='left')\n",
    "    def add_update_behavior(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add update behavior features with comprehensive null handling\"\"\"\n",
    "        user_data = self.loader.get_dataset('user_data', \n",
    "            ['fid', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        valid_updates = (user_data\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .filter(pl.col('timestamp').is_not_null())\n",
    "            .with_columns([pl.col('timestamp').cast(pl.Datetime)]))\n",
    "        \n",
    "        update_metrics = (valid_updates\n",
    "            .sort(['fid', 'timestamp'])\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.len().alias('total_updates'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().mean().alias('avg_update_interval'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().std().alias('update_time_std')\n",
    "            ])\n",
    "            .with_columns([\n",
    "                pl.when(pl.col('avg_update_interval') > 0)\n",
    "                    .then(pl.col('update_time_std') / pl.col('avg_update_interval'))\n",
    "                    .otherwise(0.0)\n",
    "                    .alias('profile_update_consistency')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        return base_lf.join(update_metrics, on='fid', how='left')\n",
    "    def add_channel_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add channel features using streaming operations\"\"\"\n",
    "        # Process channel follows\n",
    "        channel_follows = self.loader.get_dataset('channel_follows', \n",
    "            ['fid', 'channel_id', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        follow_features = (\n",
    "            channel_follows\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.n_unique('channel_id').alias('unique_channels_followed'),\n",
    "                (pl.col('timestamp').diff().dt.total_seconds() < 60)\n",
    "                    .sum().alias('rapid_channel_follows'),\n",
    "                pl.col('timestamp').dt.hour().value_counts().std()\n",
    "                    .alias('channel_follow_hour_std')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Process memberships\n",
    "        channel_members = self.loader.get_dataset('channel_members', \n",
    "            ['fid', 'channel_id', 'deleted_at'])\n",
    "        \n",
    "        member_features = (\n",
    "            channel_members\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.len().alias('channel_memberships'),\n",
    "                pl.n_unique('channel_id').alias('unique_channel_memberships')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        result = base_lf.join(follow_features, on='fid', how='left')\n",
    "        return result.join(member_features, on='fid', how='left')\n",
    "    def add_verification_patterns_features(self, base_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"Add verification patterns using streaming\"\"\"\n",
    "        verifications = self.loader.get_dataset('verifications', \n",
    "            ['fid', 'timestamp', 'deleted_at'])\n",
    "        \n",
    "        # On-chain verification patterns\n",
    "        verif_patterns = (\n",
    "            verifications\n",
    "            .filter(pl.col('deleted_at').is_null())\n",
    "            .with_columns(pl.col('timestamp').cast(pl.Datetime))\n",
    "            .group_by('fid')\n",
    "            .agg([\n",
    "                pl.col('timestamp').diff().dt.total_hours().mean()\n",
    "                    .alias('avg_hours_between_verifications'),\n",
    "                pl.col('timestamp').diff().dt.total_hours().std()\n",
    "                    .alias('std_hours_between_verifications'),\n",
    "                (pl.col('timestamp').diff().dt.total_hours() < 1)\n",
    "                    .sum().alias('rapid_verifications')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        return base_lf.join(verif_patterns, on='fid', how='left')\n",
    "    def build_feature_matrix(self) -> pl.DataFrame:\n",
    "        \"\"\"Build complete feature matrix with all optimizations.\"\"\"\n",
    "        try:\n",
    "            # Start with profile features\n",
    "            feature_lf = self.loader.load_checkpoint('profile')\n",
    "            if feature_lf is None:\n",
    "                print(\"Building profile features...\")\n",
    "                feature_lf = self.extract_profile_features()\n",
    "                self.loader.save_checkpoint(feature_lf, 'profile')\n",
    "                \n",
    "            # Get base FIDs\n",
    "            base_fids = feature_lf.select('fid').collect(streaming=True)['fid']\n",
    "            self.loader.set_base_fids(base_fids)\n",
    "            \n",
    "            # Build features in optimal order with dependencies\n",
    "            feature_lf = (\n",
    "                feature_lf\n",
    "                .pipe(self.add_network_features)\n",
    "                .pipe(self.add_temporal_features)\n",
    "                .pipe(self.add_cast_features)\n",
    "                .pipe(self.add_reaction_features)\n",
    "                .pipe(self.add_verification_features)\n",
    "                .pipe(self.add_user_data_features)\n",
    "                .pipe(self.add_storage_features)\n",
    "                .pipe(self.add_signer_features)\n",
    "                .pipe(self.add_mentions_features)\n",
    "                .pipe(self.add_update_behavior)\n",
    "                .pipe(self.add_verification_patterns_features)\n",
    "                .pipe(self.add_reply_patterns_features)\n",
    "                .pipe(self.add_power_user_interaction_features)\n",
    "                .pipe(self.add_influence_features)\n",
    "                .pipe(self.add_channel_features)\n",
    "                .pipe(self.add_engagement_metrics)\n",
    "                .pipe(self.add_authenticity_features)\n",
    "                .pipe(self.add_derived_features)\n",
    "                .pipe(self.add_network_quality_features)\n",
    "                .pipe(self.add_activity_patterns)\n",
    "            )\n",
    "            \n",
    "            # Collect with streaming\n",
    "            print(\"Collecting final feature matrix...\")\n",
    "            result = feature_lf.collect(streaming=True)\n",
    "            \n",
    "            # Clean and validate\n",
    "            result = self._validate_and_clean_features(result)\n",
    "            \n",
    "            print(f\"Feature matrix built with shape: {result.shape}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error building feature matrix: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _validate_and_clean_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Validate and clean feature matrix.\"\"\"\n",
    "        # Ensure FID column is correct type\n",
    "        df = df.with_columns(pl.col('fid').cast(pl.Int64))\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df.unique(subset=['fid'])\n",
    "        \n",
    "        # Fill nulls in numeric columns\n",
    "        numeric_cols = [\n",
    "            c for c in df.columns \n",
    "            if c != 'fid' and df[c].dtype in [pl.Int64, pl.Float64]\n",
    "        ]\n",
    "        \n",
    "        df = df.with_columns([\n",
    "            pl.col(c).fill_null(0).cast(pl.Float64) \n",
    "            for c in numeric_cols\n",
    "        ])\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "data_path = \"data\"\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "feature_eng = OptimizedFeatureEngineering(data_path, checkpoint_dir)\n",
    "feature_matrix = feature_eng.build_feature_matrix()\n",
    "print(\"Feature matrix built with shape:\", feature_matrix.shape)\n",
    "print(feature_matrix.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import shap\n",
    "from scipy import stats\n",
    "\n",
    "class StreamingSybilDetector:\n",
    "    \"\"\"Memory-efficient Sybil detection with streaming predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_engineering: 'OptimizedFeatureEngineering'):\n",
    "        self.feature_engineering = feature_engineering\n",
    "        self.model = None\n",
    "        self.base_models = {}\n",
    "        self.feature_names = None\n",
    "        self.shap_explainers = {}\n",
    "        self.shap_values = {}\n",
    "        self.feature_importance = {}\n",
    "        \n",
    "    def prepare_streaming_features(self, df: pl.LazyFrame, required_cols: List[str] = None) -> pl.LazyFrame:\n",
    "        \"\"\"Prepare features using streaming operations\"\"\"\n",
    "        if required_cols is None:\n",
    "            required_cols = [\n",
    "                'has_ens', 'has_bio', 'has_avatar', 'verification_count',\n",
    "                'following_count', 'follower_count', 'follower_ratio',\n",
    "                # Add other base feature columns as needed\n",
    "            ]\n",
    "        \n",
    "        # Ensure all required columns exist\n",
    "        available_cols = df.columns\n",
    "        valid_cols = [col for col in required_cols if col in available_cols]\n",
    "        \n",
    "        if not valid_cols:\n",
    "            raise ValueError(\"No valid feature columns found\")\n",
    "            \n",
    "        return df.select(valid_cols)\n",
    "        \n",
    "    def train(self, train_lf: pl.LazyFrame, labels: np.ndarray, feature_names: List[str]):\n",
    "        \"\"\"Train models using streaming batches\"\"\"\n",
    "        self.feature_names = feature_names\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Define base models\n",
    "        base_models = {\n",
    "            'xgb': xgb.XGBClassifier(eval_metric='auc', \n",
    "                                   use_label_encoder=False, \n",
    "                                   random_state=42),\n",
    "            'rf': RandomForestClassifier(n_jobs=-1, \n",
    "                                       random_state=42, \n",
    "                                       class_weight='balanced'),\n",
    "            'lgbm': LGBMClassifier(n_jobs=-1, \n",
    "                                  random_state=42, \n",
    "                                  class_weight='balanced')\n",
    "        }\n",
    "        \n",
    "        print(\"Starting model training...\")\n",
    "        for name, model in base_models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            \n",
    "            def objective(trial):\n",
    "                # Model-specific parameter tuning\n",
    "                if name == 'xgb':\n",
    "                    params = {\n",
    "                        'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "                        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "                        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "                    }\n",
    "                elif name == 'rf':\n",
    "                    params = {\n",
    "                        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                        'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "                        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10)\n",
    "                    }\n",
    "                else:  # lgbm\n",
    "                    params = {\n",
    "                        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                        'num_leaves': trial.suggest_int('num_leaves', 20, 100)\n",
    "                    }\n",
    "                \n",
    "                model.set_params(**params)\n",
    "                scores = []\n",
    "                \n",
    "                # Stream data in batches for cross-validation\n",
    "                BATCH_SIZE = 10000\n",
    "                for train_idx, val_idx in cv.split(range(len(labels)), labels):\n",
    "                    train_data = (train_lf\n",
    "                        .collect(streaming=True)\n",
    "                        .select(feature_names)\n",
    "                        .iter_slices(n_rows=BATCH_SIZE))\n",
    "                    \n",
    "                    X_train = []\n",
    "                    for batch in train_data:\n",
    "                        if len(batch) > 0:\n",
    "                            X_train.append(batch.to_numpy())\n",
    "                    X_train = np.vstack(X_train)\n",
    "                    \n",
    "                    # Train and evaluate\n",
    "                    model.fit(X_train[train_idx], labels[train_idx])\n",
    "                    val_pred = model.predict_proba(X_train[val_idx])[:, 1]\n",
    "                    score = roc_auc_score(labels[val_idx], val_pred)\n",
    "                    scores.append(score)\n",
    "                \n",
    "                return np.mean(scores)\n",
    "            \n",
    "            # Optimize hyperparameters\n",
    "            study.optimize(objective, n_trials=30, timeout=600)\n",
    "            print(f\"Best parameters for {name}: {study.best_params}\")\n",
    "            \n",
    "            # Train final model with best parameters\n",
    "            best_model = type(model)(**study.best_params)\n",
    "            \n",
    "            # Stream full training data\n",
    "            X_train = []\n",
    "            for batch in (train_lf\n",
    "                .collect(streaming=True)\n",
    "                .select(feature_names)\n",
    "                .iter_slices(n_rows=10000)):\n",
    "                if len(batch) > 0:\n",
    "                    X_train.append(batch.to_numpy())\n",
    "            X_train = np.vstack(X_train)\n",
    "            \n",
    "            best_model.fit(X_train, labels)\n",
    "            \n",
    "            # Generate SHAP values for feature importance\n",
    "            try:\n",
    "                explainer = shap.TreeExplainer(best_model)\n",
    "                shap_vals = explainer.shap_values(X_train[:1000])  # Sample for memory efficiency\n",
    "                self.shap_values[name] = shap_vals\n",
    "                self.shap_explainers[name] = explainer\n",
    "                print(f\"SHAP values generated for {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating SHAP values for {name}: {str(e)}\")\n",
    "            \n",
    "            # Calibrate probabilities\n",
    "            calibrated_model = CalibratedClassifierCV(best_model, cv=5, n_jobs=-1)\n",
    "            calibrated_model.fit(X_train, labels)\n",
    "            self.base_models[name] = calibrated_model\n",
    "        \n",
    "        # Create final ensemble\n",
    "        self.model = VotingClassifier(\n",
    "            estimators=[(name, model) for name, model in self.base_models.items()],\n",
    "            voting='soft'\n",
    "        )\n",
    "        self.model.fit(X_train, labels)\n",
    "        \n",
    "        # Calculate feature importance\n",
    "        self._calculate_feature_importance()\n",
    "        print(\"Training complete!\")\n",
    "        \n",
    "    def predict_streaming(self, test_lf: pl.LazyFrame, batch_size: int = 10000) -> pl.LazyFrame:\n",
    "        \"\"\"Generate predictions using streaming\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained\")\n",
    "            \n",
    "        predictions = []\n",
    "        total_batches = 0\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch in (test_lf\n",
    "            .collect(streaming=True)\n",
    "            .iter_slices(n_rows=batch_size)):\n",
    "            \n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get predictions from all base models\n",
    "            batch_predictions = []\n",
    "            batch_features = batch.select(self.feature_names).to_numpy()\n",
    "            \n",
    "            for name, model in self.base_models.items():\n",
    "                pred_proba = model.predict_proba(batch_features)[:, 1]\n",
    "                batch_predictions.append(pred_proba)\n",
    "            \n",
    "            # Calculate ensemble statistics\n",
    "            batch_predictions = np.array(batch_predictions)\n",
    "            mean_probs = batch_predictions.mean(axis=0)\n",
    "            std_probs = batch_predictions.std(axis=0)\n",
    "            \n",
    "            # Calculate confidence intervals\n",
    "            conf_intervals = stats.norm.interval(0.95, loc=mean_probs, scale=std_probs)\n",
    "            \n",
    "            # Create results DataFrame\n",
    "            batch_results = pl.DataFrame({\n",
    "                'fid': batch['fid'],\n",
    "                'bot_probability': mean_probs,\n",
    "                'prediction_uncertainty': std_probs,\n",
    "                'confidence_lower': conf_intervals[0],\n",
    "                'confidence_upper': conf_intervals[1],\n",
    "                'is_bot': mean_probs >= 0.5\n",
    "            })\n",
    "            \n",
    "            predictions.append(batch_results)\n",
    "            total_batches += 1\n",
    "            \n",
    "            if total_batches % 10 == 0:\n",
    "                print(f\"Processed {total_batches} batches...\")\n",
    "        \n",
    "        return pl.concat(predictions)\n",
    "    \n",
    "    def _calculate_feature_importance(self):\n",
    "        \"\"\"Calculate feature importance across all models\"\"\"\n",
    "        importance_dict = {}\n",
    "        \n",
    "        for name, model in self.base_models.items():\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances = model.feature_importances_\n",
    "            elif hasattr(model, 'base_estimator_'):\n",
    "                importances = model.base_estimator_.feature_importances_\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            for feat, imp in zip(self.feature_names, importances):\n",
    "                if feat not in importance_dict:\n",
    "                    importance_dict[feat] = []\n",
    "                importance_dict[feat].append(imp)\n",
    "        \n",
    "        # Average importance across models\n",
    "        self.feature_importance = {\n",
    "            feat: np.mean(scores) \n",
    "            for feat, scores in importance_dict.items()\n",
    "        }\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save model with streaming support\"\"\"\n",
    "        model_data = {\n",
    "            'base_models': self.base_models,\n",
    "            'ensemble_model': self.model,\n",
    "            'feature_names': self.feature_names,\n",
    "            'feature_importance': self.feature_importance\n",
    "        }\n",
    "        joblib.dump(model_data, path)\n",
    "        \n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"Load model with streaming support\"\"\"\n",
    "        model_data = joblib.load(path)\n",
    "        self.base_models = model_data['base_models']\n",
    "        self.model = model_data['ensemble_model']\n",
    "        self.feature_names = model_data['feature_names']\n",
    "        self.feature_importance = model_data['feature_importance']\n",
    "\n",
    "\n",
    "\n",
    "# Initialize components\n",
    "feature_eng = OptimizedFeatureEngineering(\"data\", \"checkpoints\")\n",
    "detector = StreamingSybilDetector(feature_eng)\n",
    "\n",
    "# Build features using streaming\n",
    "feature_matrix = feature_eng.build_feature_matrix()\n",
    "\n",
    "# Load labels efficiently\n",
    "labels_lf = pl.scan_csv('data/labels.csv')\n",
    "\n",
    "# Join with streaming\n",
    "data = pl.concat([\n",
    "    feature_matrix.lazy(),\n",
    "    labels_lf.select(['fid', 'bot'])\n",
    "]).collect(streaming=True)\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_idx, test_idx = train_test_split(\n",
    "    range(len(data)), \n",
    "    test_size=0.2, \n",
    "    stratify=data['bot'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model with streaming\n",
    "train_data = data[train_idx].lazy()\n",
    "test_data = data[test_idx].lazy()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = [c for c in data.columns if c not in ['fid', 'bot']]\n",
    "\n",
    "# Train and evaluate\n",
    "detector.train(train_data, data['bot'][train_idx].to_numpy(), feature_names)\n",
    "predictions = detector.predict_streaming(test_data)\n",
    "\n",
    "# Save model\n",
    "detector.save_model(\"checkpoints/sybil_detector.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "farcaster-social-graph-notebooks-RMjVf8-3-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
